{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import random\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "import pickle\n",
    "\n",
    "train_example_names = [fn.split('.')[0] for fn in os.listdir('data/train')]\n",
    "test_example_names = [fn.split('.')[0] for fn in os.listdir('data/test')]\n",
    "\n",
    "metadata = pd.read_csv('data/train.csv')\n",
    "docIdx = train_example_names.copy()\n",
    "\n",
    "connection_tokens = {'s', 'of', 'and', 'in', 'on', 'for', 'from', 'the', 'act', 'coast', 'future', 'system', 'per'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291984\n",
      "60188\n"
     ]
    }
   ],
   "source": [
    "def text_cleaning(text):\n",
    "    text = re.sub('[^A-Za-z]+', ' ', str(text)).strip() # remove unnecessary literals\n",
    "\n",
    "    # remove extra spaces\n",
    "    text = re.sub(\"\\s+\",\" \", text)\n",
    "\n",
    "    return text.lower().strip()\n",
    "\n",
    "def is_name_ok(text):\n",
    "    if len([c for c in text if c.isalnum()]) < 4:\n",
    "        return False\n",
    "    \n",
    "    tokens = [t for t in text.split(' ') if len(t) > 3]\n",
    "    tokens = [t for t in tokens if not t in connection_tokens]\n",
    "    if len(tokens) < 3:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "    \n",
    "with open('data/all_preds_selected.txt', 'r') as f:\n",
    "    selected_pred_labels = f.readlines()\n",
    "    selected_pred_labels = [l.strip() for l in selected_pred_labels]\n",
    "\n",
    "existing_labels = [text_cleaning(x) for x in metadata['dataset_label']] +\\\n",
    "                  [text_cleaning(x) for x in metadata['dataset_title']] +\\\n",
    "                  [text_cleaning(x) for x in metadata['cleaned_label']] +\\\n",
    "                  [text_cleaning(x) for x in selected_pred_labels]\n",
    "\n",
    "to_remove = [\n",
    "    'frequently asked questions', 'total maximum daily load tmd', 'health care facilities',\n",
    "    'traumatic brain injury', 'north pacific high', 'droplet number concentration', 'great slave lake',\n",
    "    'census block groups'\n",
    "]\n",
    "\n",
    "df = pd.read_csv(r'C:\\projects\\personal\\kaggle\\kaggle_coleridge_initiative\\string_search\\data\\gov_data.csv')\n",
    "print(len(df))\n",
    "\n",
    "\n",
    "df['title'] = df.title.apply(text_cleaning)\n",
    "titles = list(df.title.unique())\n",
    "titles = [t for t in titles if not t in to_remove]\n",
    "df = pd.DataFrame({'title': titles})\n",
    "df = df.loc[df.title.apply(is_name_ok)]\n",
    "df = pd.concat([df, pd.DataFrame({'title': existing_labels})], ignore_index= True).reset_index(drop = True)\n",
    "titles = list(df.title.unique())\n",
    "df = pd.DataFrame({'title': titles})\n",
    "df['title'] = df.title.apply(text_cleaning)\n",
    "\n",
    "# Sort labels by length in ascending order\n",
    "existing_labels = sorted(list(df.title.values), key = len, reverse = True)\n",
    "existing_labels = [l for l in existing_labels if len(l.split(' ')) < 10]\n",
    "del df\n",
    "\n",
    "print(len(existing_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load, Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos size: 69267\n",
      "neg size: 902401\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f'data/bert_ner_sentences/pos.pkl', 'rb') as f:\n",
    "    pos_sentences_raw = pickle.load(f)\n",
    "\n",
    "with open(f'data/bert_ner_sentences/neg.pkl', 'rb') as f:\n",
    "    neg_sentences_raw = pickle.load(f)\n",
    "\n",
    "pos_sentences = [text_cleaning(s) for s in pos_sentences_raw]\n",
    "neg_sentences = [text_cleaning(s) for s in neg_sentences_raw]\n",
    "\n",
    "print(f'pos size: {len(pos_sentences)}')\n",
    "print(f'neg size: {len(neg_sentences)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove labels from positive sentences to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 69267/69267 [13:50<00:00, 83.36it/s]\n"
     ]
    }
   ],
   "source": [
    "pos_sentences_with_label = pos_sentences\n",
    "pos_sentences = []\n",
    "\n",
    "for s in tqdm(pos_sentences_with_label):\n",
    "    for l in existing_labels:\n",
    "        if l in s:\n",
    "            s = s.replace(l, '')\n",
    "\n",
    "    pos_sentences.append(s)\n",
    "\n",
    "pos_sentences = [re.sub(\"\\s+\",\" \", s) for s in pos_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take first n tokens from each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCE_TOKEN_SIZE = 25\n",
    "\n",
    "def shorten_sentence(text):\n",
    "    tokens = text.split(' ')\n",
    "    return ' '.join(tokens[:SENTENCE_TOKEN_SIZE])\n",
    "\n",
    "pos_sentences = [shorten_sentence(s) for s in pos_sentences]\n",
    "neg_sentences = [shorten_sentence(s) for s in neg_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.permutation(len(neg_sentences))\n",
    "neg_sentences = [neg_sentences[i] for i in idx]\n",
    "\n",
    "neg_sentences = neg_sentences[:500000]\n",
    "#pos_sentences = pos_sentences[:20000]\n",
    "\n",
    "sentences = pos_sentences + neg_sentences\n",
    "labels = np.zeros(len(sentences))\n",
    "labels[:len(pos_sentences)] = 1\n",
    "\n",
    "assert len(sentences) == len(labels)\n",
    "\n",
    "idx = np.random.permutation(len(sentences))\n",
    "sentences = [sentences[i] for i in idx]\n",
    "labels = [labels[i] for i in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building sklearn text classifier...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import csv\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "import statistics as stats\n",
    "from bert_sklearn import BertClassifier\n",
    "\n",
    "model = BertClassifier(bert_model='scibert-scivocab-uncased',\n",
    "                        validation_fraction= 0.1,\n",
    "                        max_seq_length=25,\n",
    "                        train_batch_size=4,\n",
    "                        warmup_proportion=0.1,\n",
    "                        gradient_accumulation_steps=3,\n",
    "                        epochs = 1\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading scibert-scivocab-uncased model...\n",
      "Defaulting to linear classifier/regressor\n",
      "Loading Pytorch checkpoint\n",
      "train data size: 512341, validation data size: 56926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training  :   0%|                                                  | 1/512341 [00:13<1935:19:20, 13.60s/it, loss=0.948]C:\\Users\\ozano\\.conda\\envs\\torch\\lib\\site-packages\\bert_sklearn\\model\\pytorch_pretrained\\optimization.py:275: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:1005.)\n",
      "  next_m.mul_(beta1).add_(1 - beta1, grad)\n",
      "Training  : 100%|███████████████████████████████████████████████| 512341/512341 [15:54:41<00:00,  8.94it/s, loss=0.444]\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████| 7116/7116 [03:48<00:00, 31.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train loss: 0.4445, Val loss: 0.3759, Val accy: 87.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertClassifier(bert_model='scibert-scivocab-uncased', do_lower_case=True,\n",
       "               epochs=1, gradient_accumulation_steps=3,\n",
       "               label_list=array([0., 1.]), max_seq_length=25,\n",
       "               train_batch_size=4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(pd.Series(sentences), pd.Series(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to disk\n",
    "savefile='data/sklearn_bert_classification.bin'\n",
    "model.save(savefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|████████████████████████████████████████████████████████████████████████| 1/1 [00:08<00:00,  8.23s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.91655505, 0.08344493],\n",
       "       [0.91656065, 0.08343935],\n",
       "       [0.91655666, 0.08344332]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(pd.Series(sentences[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
