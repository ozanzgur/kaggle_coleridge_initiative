{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import random\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "import pickle\n",
    "\n",
    "train_example_names = [fn.split('.')[0] for fn in os.listdir('data/train')]\n",
    "test_example_names = [fn.split('.')[0] for fn in os.listdir('data/test')]\n",
    "\n",
    "metadata = pd.read_csv('data/train.csv')\n",
    "docIdx = train_example_names.copy()\n",
    "\n",
    "connection_tokens = {'s', 'of', 'and', 'in', 'on', 'for', 'from', 'the', 'act', 'coast', 'future', 'system', 'per'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291984\n",
      "60187\n"
     ]
    }
   ],
   "source": [
    "def text_cleaning(text):\n",
    "    text = re.sub('[^A-Za-z]+', ' ', str(text)).strip() # remove unnecessary literals\n",
    "\n",
    "    # remove extra spaces\n",
    "    text = re.sub(\"\\s+\",\" \", text)\n",
    "\n",
    "    return text.lower().strip()\n",
    "\n",
    "def is_name_ok(text):\n",
    "    if len([c for c in text if c.isalnum()]) < 4:\n",
    "        return False\n",
    "    \n",
    "    tokens = [t for t in text.split(' ') if len(t) > 3]\n",
    "    tokens = [t for t in tokens if not t in connection_tokens]\n",
    "    if len(tokens) < 3:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "    \n",
    "with open('data/all_preds_selected.txt', 'r') as f:\n",
    "    selected_pred_labels = f.readlines()\n",
    "    selected_pred_labels = [l.strip() for l in selected_pred_labels]\n",
    "\n",
    "existing_labels = [text_cleaning(x) for x in metadata['dataset_label']] +\\\n",
    "                  [text_cleaning(x) for x in metadata['dataset_title']] +\\\n",
    "                  [text_cleaning(x) for x in metadata['cleaned_label']] +\\\n",
    "                  [text_cleaning(x) for x in selected_pred_labels]\n",
    "\n",
    "to_remove = [\n",
    "    'frequently asked questions', 'total maximum daily load tmd', 'health care facilities',\n",
    "    'traumatic brain injury', 'north pacific high', 'droplet number concentration', 'great slave lake',\n",
    "    'census block groups'\n",
    "]\n",
    "\n",
    "df = pd.read_csv(r'C:\\projects\\personal\\kaggle\\kaggle_coleridge_initiative\\string_search\\data\\gov_data.csv')\n",
    "print(len(df))\n",
    "\n",
    "\n",
    "df['title'] = df.title.apply(text_cleaning)\n",
    "titles = list(df.title.unique())\n",
    "titles = [t for t in titles if not t in to_remove]\n",
    "df = pd.DataFrame({'title': titles})\n",
    "df = df.loc[df.title.apply(is_name_ok)]\n",
    "df = pd.concat([df, pd.DataFrame({'title': existing_labels})], ignore_index= True).reset_index(drop = True)\n",
    "titles = list(df.title.unique())\n",
    "df = pd.DataFrame({'title': titles})\n",
    "df['title'] = df.title.apply(text_cleaning)\n",
    "\n",
    "# Sort labels by length in ascending order\n",
    "existing_labels = sorted(list(df.title.values), key = len, reverse = True)\n",
    "existing_labels = [l for l in existing_labels if len(l.split(' ')) < 10]\n",
    "del df\n",
    "\n",
    "existing_labels.remove('adni')\n",
    "print(len(existing_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load, Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos size: 69267\n",
      "neg size: 902401\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f'data/bert_ner_sentences/pos.pkl', 'rb') as f:\n",
    "    pos_sentences_raw = pickle.load(f)\n",
    "\n",
    "with open(f'data/bert_ner_sentences/neg.pkl', 'rb') as f:\n",
    "    neg_sentences_raw = pickle.load(f)\n",
    "\n",
    "pos_sentences = [text_cleaning(s) for s in pos_sentences_raw]\n",
    "neg_sentences = [text_cleaning(s) for s in neg_sentences_raw]\n",
    "\n",
    "print(f'pos size: {len(pos_sentences)}')\n",
    "print(f'neg size: {len(neg_sentences)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove labels from positive sentences to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 69267/69267 [12:42<00:00, 90.84it/s]\n"
     ]
    }
   ],
   "source": [
    "pos_sentences_with_label = pos_sentences\n",
    "pos_sentences = []\n",
    "\n",
    "for s in tqdm(pos_sentences_with_label):\n",
    "    for l in existing_labels:\n",
    "        if l in s:\n",
    "            s = s.replace(l, '')\n",
    "\n",
    "    pos_sentences.append(s)\n",
    "\n",
    "pos_sentences = [re.sub(\"\\s+\",\" \", s) for s in pos_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take first n tokens from each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sentences_raw = pos_sentences_with_label.copy()\n",
    "neg_sentences_raw = neg_sentences.copy()\n",
    "\n",
    "SENTENCE_TOKEN_SIZE = 25\n",
    "\n",
    "def shorten_sentence(text):\n",
    "    tokens = text.split(' ')\n",
    "    return ' '.join(tokens[:SENTENCE_TOKEN_SIZE])\n",
    "\n",
    "pos_sentences = [shorten_sentence(s) for s in pos_sentences]\n",
    "neg_sentences = [shorten_sentence(s) for s in neg_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69267\n",
      "902401\n"
     ]
    }
   ],
   "source": [
    "print(len(pos_sentences))\n",
    "print(len(neg_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.permutation(len(neg_sentences))\n",
    "neg_sentences = [neg_sentences[i] for i in idx]\n",
    "neg_sentences_raw = [neg_sentences_raw[i] for i in idx]\n",
    "neg_sentences = neg_sentences[:75000]\n",
    "#pos_sentences = pos_sentences[:20000]\n",
    "\n",
    "sentences = pos_sentences + neg_sentences\n",
    "labels = np.zeros(len(sentences))\n",
    "labels[:len(pos_sentences)] = 1\n",
    "\n",
    "assert len(sentences) == len(labels)\n",
    "\n",
    "idx = np.random.permutation(len(sentences))\n",
    "sentences = [sentences[i] for i in idx]\n",
    "labels = [labels[i] for i in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building sklearn text classifier...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import csv\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "import statistics as stats\n",
    "from bert_sklearn import BertClassifier\n",
    "\n",
    "model = BertClassifier(bert_model='scibert-scivocab-uncased',\n",
    "                        validation_fraction= 0.1,\n",
    "                        max_seq_length=25,\n",
    "                        train_batch_size=4,\n",
    "                        warmup_proportion=0.1,\n",
    "                        gradient_accumulation_steps=3,\n",
    "                        epochs = 1\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading scibert-scivocab-uncased model...\n",
      "Defaulting to linear classifier/regressor\n",
      "Loading Pytorch checkpoint\n",
      "train data size: 129841, validation data size: 14426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training  : 100%|████████████████████████████████████████████████| 129841/129841 [3:27:59<00:00, 10.40it/s, loss=0.475]\n",
      "Validating: 100%|██████████████████████████████████████████████████████████████████| 1804/1804 [00:54<00:00, 32.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train loss: 0.4754, Val loss: 0.5159, Val accy: 85.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertClassifier(bert_model='scibert-scivocab-uncased', do_lower_case=True,\n",
       "               epochs=1, gradient_accumulation_steps=3,\n",
       "               label_list=array([0., 1.]), max_seq_length=25,\n",
       "               train_batch_size=4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(pd.Series(sentences), pd.Series(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to disk\n",
    "savefile='data/sklearn_bert_classification2.bin'\n",
    "model.save(savefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|████████████████████████████████████████████████████████████████████████| 1/1 [00:12<00:00, 12.73s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[9.6569099e-03, 9.9034309e-01],\n",
       "       [9.7400814e-01, 2.5991896e-02],\n",
       "       [2.0728298e-04, 9.9979275e-01]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(pd.Series(sentences[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from data/sklearn_bert_classification2.bin...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/15/2021 13:59:22 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to linear classifier/regressor\n",
      "Building sklearn text classifier...\n"
     ]
    }
   ],
   "source": [
    "from bert_sklearn import load_model\n",
    "bert_model = load_model(r'data/sklearn_bert_classification2.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f'data/bert_ner_sentences/pos.pkl', 'rb') as f:\n",
    "    pos_sentences_raw = pickle.load(f)\n",
    "\n",
    "with open(f'data/bert_ner_sentences/neg.pkl', 'rb') as f:\n",
    "    neg_sentences_raw = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "902401"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neg_sentences_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69267\n",
      "902401\n"
     ]
    }
   ],
   "source": [
    "print(len(pos_sentences))\n",
    "print(len(neg_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████████████████████████████████████████████████████████████| 8659/8659 [03:34<00:00, 40.34it/s]\n",
      "Predicting: 100%|██████████████████████████████████████████████████████████████| 112801/112801 [57:39<00:00, 32.61it/s]\n"
     ]
    }
   ],
   "source": [
    "pos_pred = bert_model.predict_proba(pd.Series(pos_sentences))\n",
    "neg_pred = bert_model.predict_proba(pd.Series(neg_sentences))\n",
    "pos_pred = pos_pred[:, 1] > 0.5\n",
    "neg_pred = neg_pred[:, 1] > 0.5\n",
    "\n",
    "pred_sentences = []\n",
    "pred_sentences += [pos_sentences_raw[i] for i in np.argwhere(pos_pred).squeeze()]\n",
    "pred_sentences += [neg_sentences_raw[i] for i in np.argwhere(neg_pred).squeeze()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119897"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54449"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_pred.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65448"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_pred.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7860741767363968\n",
      "0.07252651537398562\n"
     ]
    }
   ],
   "source": [
    "print(pos_pred.mean())\n",
    "print(neg_pred.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction size: 119897\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f'data/classifier_output/pos_classified.pkl', 'wb') as f:\n",
    "    pickle.dump(pred_sentences, f)\n",
    "\n",
    "print(f'prediction size: {len(pred_sentences)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
