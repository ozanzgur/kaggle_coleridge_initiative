{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd01b4c7016e99d31c2e7c892573dc93dbd4548eb0a0f5dca22fbf3a690830b4e66",
   "display_name": "Python 3.8.8 64-bit ('torch': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import random\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "import pickle\n",
    "\n",
    "train_example_names = [fn.split('.')[0] for fn in os.listdir('data/train')]\n",
    "test_example_names = [fn.split('.')[0] for fn in os.listdir('data/test')]\n",
    "\n",
    "metadata = pd.read_csv('data/train.csv')\n",
    "docIdx = train_example_names.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading model from data/sklearn_bert.bin...\n",
      "06/02/2021 03:11:29 - INFO - bert_sklearn.model.pytorch_pretrained.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 31090\n",
      "}\n",
      "\n",
      "Defaulting to linear classifier/regressor\n",
      "Building sklearn token classifier...\n"
     ]
    }
   ],
   "source": [
    "from bert_sklearn import load_model\n",
    "from bert_sklearn import BertTokenClassifier \n",
    "\n",
    "bert_model = load_model(r'data/sklearn_bert.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pos size: 32235\nneg size: 1032513\npos label size: 32235\nneg label size: 1032513\nall size: 1064748\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f'data/bert_ner_data/pos.pkl', 'rb') as f:\n",
    "    pos_sentences_processed = pickle.load(f)\n",
    "\n",
    "with open(f'data/bert_ner_data/neg.pkl', 'rb') as f:\n",
    "    neg_sentences_processed = pickle.load(f)\n",
    "\n",
    "with open(f'data/bert_ner_data/pos_labels.pkl', 'rb') as f:\n",
    "    pos_labels = pickle.load(f)\n",
    "\n",
    "with open(f'data/bert_ner_data/neg_labels.pkl', 'rb') as f:\n",
    "    neg_labels = pickle.load(f)\n",
    "\n",
    "print(f'pos size: {len(pos_sentences_processed)}')\n",
    "print(f'neg size: {len(neg_sentences_processed)}')\n",
    "print(f'pos label size: {len(pos_labels)}')\n",
    "print(f'neg label size: {len(neg_labels)}')\n",
    "\n",
    "all_sentences = pos_sentences_processed + neg_sentences_processed\n",
    "del pos_sentences_processed\n",
    "del neg_sentences_processed\n",
    "\n",
    "print(f'all size: {len(all_sentences)}')"
   ]
  },
  {
   "source": [
    "## Load sent classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading vectorizer...\nLoading sentence classifier...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "print('Loading vectorizer...') # TfidfVectorizer\n",
    "with open(f'data/tfidf_vectorizer.pkl', 'rb') as f:\n",
    "    vectorizer = pickle.load(f)\n",
    "    \n",
    "print('Loading sentence classifier...')\n",
    "sklearn_model = joblib.load('pipeline_model.joblib')"
   ]
  },
  {
   "source": [
    "## Utils"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning_sent_classifier(text):\n",
    "    text = re.sub('[^A-Za-z0-9]+', ' ', str(text)).strip() # remove unnecessary literals\n",
    "\n",
    "    text = re.sub(r'\\[[0-9]+]', ' specialreference ', text)\n",
    "\n",
    "    # Remove years\n",
    "    text = re.sub(r'(19|20)[0-9][0-9]', ' specialyear ', text)\n",
    "\n",
    "    # remove other digits\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "\n",
    "    # remove extra spaces\n",
    "    text = re.sub(\"\\s+\",\" \", text)\n",
    "\n",
    "    # Remove websites\n",
    "    text = ' '.join(['specialwebsite' if 'http' in t or 'www' in t else t for t in text.split(' ') ])\n",
    "\n",
    "    return text.lower()\n",
    "\n",
    "def process_doc_sent_classifier(sentences):\n",
    "    clean_sentences = [text_cleaning_sent_classifier(s).lower() for s in sentences]\n",
    "    X = vectorizer.transform(clean_sentences)\n",
    "    return X"
   ]
  },
  {
   "source": [
    "## Select Sentences"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 107/107 [00:53<00:00,  2.01it/s]\n"
     ]
    }
   ],
   "source": [
    "sel_sentences = []\n",
    "\n",
    "batch_size = 10000\n",
    "n_batches = len(all_sentences) // batch_size\n",
    "if len(all_sentences) % batch_size != 0: n_batches += 1\n",
    "\n",
    "for i in tqdm(range(n_batches), total = n_batches):\n",
    "    i_start = batch_size * i\n",
    "    i_end = min(len(all_sentences), batch_size * (i + 1))\n",
    "\n",
    "    sen_batch = all_sentences[i_start:i_end]\n",
    "\n",
    "    # Classification\n",
    "    doc_class_X = process_doc_sent_classifier(sen_batch)\n",
    "    doc_class_pred = sklearn_model.predict_proba(doc_class_X)\n",
    "    doc_class_pos_pred_idx = np.argwhere(doc_class_pred[:, 1] > 0.02)[:, 0]\n",
    "\n",
    "    # Select sentences\n",
    "    sel_sentences.extend([sen_batch[i] for i in doc_class_pos_pred_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "197914"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "len(sel_sentences)"
   ]
  },
  {
   "source": [
    "## Save"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'data/bert_ner_data/selected.pkl', 'wb') as f:\n",
    "    pickle.dump(sel_sentences, f)"
   ]
  },
  {
   "source": [
    "## Load"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "selected size: 197914\n"
     ]
    }
   ],
   "source": [
    "with open(f'data/bert_ner_data/selected.pkl', 'rb') as f:\n",
    "    sel_sentences = pickle.load(f)\n",
    "\n",
    "print(f'selected size: {len(sel_sentences)}')"
   ]
  },
  {
   "source": [
    "## Predict"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\ozano\\.conda\\envs\\torch\\lib\\site-packages\\bert_sklearn\\utils.py:26: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(X)\n",
      "Predicting: 100%|██████████| 24740/24740 [1:02:50<00:00,  6.56it/s]\n"
     ]
    }
   ],
   "source": [
    "preds = bert_model.predict(sel_sentences)"
   ]
  },
  {
   "source": [
    "## Save Predictions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'data/bert_ner_data/semi_preds.pkl', 'wb') as f:\n",
    "    pickle.dump(preds, f)"
   ]
  },
  {
   "source": [
    "## Load Predictions with data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'data/bert_ner_data/selected.pkl', 'rb') as f:\n",
    "    sel_sentences = pickle.load(f)\n",
    "\n",
    "print(f'selected size: {len(sel_sentences)}')\n",
    "\n",
    "with open(f'data/bert_ner_data/semi_preds.pkl', 'rb') as f:\n",
    "    preds = pickle.load(f)\n",
    "\n",
    "print(f'selected size: {len(preds)}')"
   ]
  },
  {
   "source": [
    "## Train New NER Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Building sklearn token classifier...\n"
     ]
    }
   ],
   "source": [
    "model = BertTokenClassifier(bert_model='scibert-scivocab-uncased',\n",
    "                             max_seq_length=150, \n",
    "                             epochs=3,\n",
    "                             #gradient accumulation\n",
    "                             gradient_accumulation_steps=4,\n",
    "                             learning_rate=3e-5,\n",
    "                             train_batch_size=8,#batch size for training\n",
    "                             eval_batch_size=8, #batch size for evaluation\n",
    "                             validation_fraction=0.15, \n",
    "                             #ignore the tokens with label ‘O’                      \n",
    "                             ignore_label=['O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['in',\n",
       " 'fact',\n",
       " ',',\n",
       " 'organizations',\n",
       " 'are',\n",
       " 'now',\n",
       " 'identifying',\n",
       " 'digital',\n",
       " 'skills',\n",
       " 'or',\n",
       " 'computer',\n",
       " 'literacy',\n",
       " 'as',\n",
       " 'one',\n",
       " 'of',\n",
       " 'their',\n",
       " 'core',\n",
       " 'values',\n",
       " 'for',\n",
       " 'employability',\n",
       " '(',\n",
       " 'such',\n",
       " 'as',\n",
       " 'the',\n",
       " 'us',\n",
       " 'department',\n",
       " 'of',\n",
       " 'education',\n",
       " ',',\n",
       " 'the',\n",
       " 'us',\n",
       " 'department',\n",
       " 'of',\n",
       " 'commerce',\n",
       " ',',\n",
       " 'the',\n",
       " 'oecd',\n",
       " 'program',\n",
       " 'for',\n",
       " 'the',\n",
       " 'international',\n",
       " 'assessment',\n",
       " 'of',\n",
       " 'adult',\n",
       " 'competencies',\n",
       " 'and',\n",
       " 'the',\n",
       " 'european',\n",
       " 'commission',\n",
       " ')',\n",
       " '.',\n",
       " '']"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "sel_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [[e if not e is None else 'O' for e in p] for p in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\ozano\\.conda\\envs\\torch\\lib\\site-packages\\bert_sklearn\\utils.py:26: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(X)\n",
      "Loading scibert-scivocab-uncased model...\n",
      "Defaulting to linear classifier/regressor\n",
      "Loading Pytorch checkpoint\n",
      "train data size: 168227, validation data size: 29687\n",
      "Training  :   0%|          | 2/84114 [00:24<230:50:49,  9.88s/it, loss=0.257]C:\\Users\\ozano\\.conda\\envs\\torch\\lib\\site-packages\\bert_sklearn\\model\\pytorch_pretrained\\optimization.py:275: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:1005.)\n",
      "  next_m.mul_(beta1).add_(1 - beta1, grad)\n",
      "Training  : 100%|██████████| 84114/84114 [3:14:17<00:00,  7.22it/s, loss=0.00299]\n",
      "Validating: 100%|██████████| 3711/3711 [08:59<00:00,  6.88it/s]Epoch 1, Train loss: 0.0030, Val loss: 0.0009, Val accy: 99.89%, f1: 97.75\n",
      "\n",
      "Training  : 100%|██████████| 84114/84114 [2:59:47<00:00,  7.80it/s, loss=0.00044]\n",
      "Validating: 100%|██████████| 3711/3711 [09:14<00:00,  6.70it/s]Epoch 2, Train loss: 0.0004, Val loss: 0.0003, Val accy: 99.96%, f1: 99.21\n",
      "\n",
      "Training  :  15%|█▌        | 12866/84114 [29:43<2:21:49,  8.37it/s, loss=0.000183]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x00000263A8822A60>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ozano\\.conda\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1324, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"C:\\Users\\ozano\\.conda\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1297, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"C:\\Users\\ozano\\.conda\\envs\\torch\\lib\\multiprocessing\\process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"C:\\Users\\ozano\\.conda\\envs\\torch\\lib\\multiprocessing\\popen_spawn_win32.py\", line 108, in wait\n",
      "    res = _winapi.WaitForSingleObject(int(self._handle), msecs)\n",
      "KeyboardInterrupt: \n",
      "Training  :  15%|█▌        | 12867/84114 [29:45<2:44:46,  7.21it/s, loss=0.000183]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-74e2a38cea80>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msel_sentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\torch\\lib\\site-packages\\bert_sklearn\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, load_at_start)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m         \u001b[1;31m# finetune model!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 374\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfinetune\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtexts_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtexts_b\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\torch\\lib\\site-packages\\bert_sklearn\\finetune.py\u001b[0m in \u001b[0;36mfinetune\u001b[1;34m(model, X1, X2, y, config)\u001b[0m\n\u001b[0;32m    134\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mgrad_accum_steps\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m                 \u001b[0mupdate_learning_rate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_schedule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m                 \u001b[0mglobal_step\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\torch\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\torch\\lib\\site-packages\\bert_sklearn\\model\\pytorch_pretrained\\optimization.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    269\u001b[0m                 \u001b[1;31m# Add grad clipping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'max_grad_norm'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 271\u001b[1;33m                     \u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'max_grad_norm'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    272\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m                 \u001b[1;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\torch\\lib\\site-packages\\torch\\nn\\utils\\clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[1;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mclip_coef\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtotal_norm\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1e-6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mclip_coef\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclip_coef\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(sel_sentences, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to disk\n",
    "savefile='data/sklearn_bert_semi_supervised.bin'\n",
    "model.save(savefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}