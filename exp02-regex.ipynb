{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd01b4c7016e99d31c2e7c892573dc93dbd4548eb0a0f5dca22fbf3a690830b4e66",
   "display_name": "Python 3.8.8 64-bit ('torch': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_example_paths = glob.glob('data/train/*.json')\n",
    "test_example_paths = glob.glob('data/test/*.json')\n",
    "\n",
    "train_example_names = [fn.split('.')[0] for fn in os.listdir('data/train')]\n",
    "test_example_names = [fn.split('.')[0] for fn in os.listdir('data/test')]\n",
    "\n",
    "metadata = pd.read_csv('data/train.csv')\n",
    "metadata_train = metadata.loc[metadata.Id.isin(train_example_names)]\n",
    "metadata_test = metadata.loc[metadata.Id.isin(test_example_names)]\n",
    "\n",
    "metadata = pd.read_csv('data/train.csv')\n",
    "metadata_train = metadata.loc[metadata.Id.isin(train_example_names)]\n",
    "metadata_test = metadata.loc[metadata.Id.isin(test_example_names)]\n",
    "\n",
    "_RE_COMBINE_WHITESPACE = re.compile(r\"\\s+\")\n",
    "def make_single_whitespace(text):\n",
    "    return _RE_COMBINE_WHITESPACE.sub(\" \", text).strip()\n",
    "\n",
    "def remove_punc(txt):\n",
    "    return re.sub('[^A-Za-z0-9]+', ' ', str(txt))\n",
    "\n",
    "def load_train_example_by_name(name):\n",
    "    doc_path = os.path.join('data/train', name + '.json')\n",
    "    with open(doc_path) as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def get_doc_id(doc_path):\n",
    "    return os.path.split(train_example_names[0])[-1].split('.')[0]\n",
    "\n",
    "def clean_text(txt):\n",
    "    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower())\n",
    "\n",
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "# Load dataset names\n",
    "df = pd.read_csv('C:\\projects\\personal\\kaggle\\kaggle_coleridge_initiative\\data\\data_set_26897.csv')\n",
    "us_dataset_names = list(df.title.values)\n",
    "us_dataset_names = [make_single_whitespace(remove_punc(n)).lower() for n in us_dataset_names]\n",
    "\n",
    "labels = list(metadata.cleaned_label.unique())\n",
    "labels = sorted(labels, key = len, reverse = True)\n",
    "labels = [l.strip() for l in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_bigrams(label):\n",
    "    tokens = label.split(' ')\n",
    "    return [f'{t1} {t2}' for t1, t2 in zip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "for t in ['and', 'was', 'in']:\n",
    "    stop_words.remove(t)\n",
    "\n",
    "banned_kw = [\n",
    "    'STEM', 'FDA', 'SSH', 'FSIZE', 'PET', 'NCATE', 'TESOL', 'AVHRR-OI',\n",
    "    'ICT',\n",
    "    'AAEA',\n",
    "    'BMI', 'ADGC', 'CDRSUM', 'NASS',\n",
    "    'MMSE', 'CDR', 'SPSS', 'LCRP', 'DML', 'ITU', 'DRI', 'CIPSEA', 'IEP', 'NCES', 'BCG', 'HLM', 'MLLW', 'FDG', 'MRMC'\n",
    "]\n",
    "\n",
    "banned_values = [\n",
    "    'laboratory', 'body mass index', 'admission test', 'neural networks', 'accuracy of', 'chain reaction', 'adversarial network',\n",
    "    'state exam', 'reform act', 'least', 'labeling', 'principal components analysis', 'independent components analysis', 'markov chain', 'monte carlo',\n",
    "    'bayesian information', 'family wise error', 'posterior anterior', 'Bidirectional Encoder', 'Morphometry', 'Integral', 'T2*weighted', 'T2-weighted',\n",
    "    'T2weighted', 'T1*weighted', 'T1-weighted', 'T1weighted', 'EMCI', 'Learning Test', 'Gradepoint average', 'doctor of', 'masters of',\n",
    "    'Expected Family Contribution', 'life in', 'Long Short Term', 'Long ShortTerm', 'LSTM', 'lipoprotein', 'Support Vector Machine', 'User Interface',\n",
    "    'National Institute of', 'glucose', 'Research Division', '%', 'Heating Weeks', 'Public Management', 'Theory', 'Middle East respiratory',\n",
    "    'Discriminant Analysis', 'boltzmann', 'Disease Control and Prevention', 'polymorphism', 'positron emission tomography', 'dorsolateral', 'Data Analysis System',\n",
    "    'Analysis Kit', 'Google', 'Principal Analysis', 'Cognitive Impairment', 'Analysis of Variance'\n",
    "    ]\n",
    "\n",
    "banned_after_tokens = stop_words\n",
    "\n",
    "banned_values = [b.lower() for b in banned_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_from_abbr_in_parantheses(match, doc_text):\n",
    "    try:\n",
    "        match_i = doc_text.index(f'({match})')\n",
    "    except:\n",
    "        try:\n",
    "            match_i = doc_text.index(f'({match};')\n",
    "        except:\n",
    "            match_i = doc_text.index(f'({match}')\n",
    "\n",
    "    match_nopunc = remove_punc(match).replace(' ', '')\n",
    "    n_tokens = len(match_nopunc.replace(' ', ''))\n",
    "\n",
    "    slice_start_i = max(match_i - 200, 0)\n",
    "    slice_end_i = min(match_i + len(match) + 40, len(doc_text))\n",
    "    doc_slice = doc_text[slice_start_i: slice_end_i]\n",
    "    \n",
    "    # Remove dates\n",
    "    doc_slice = re.sub(r'(19|20)[0-9][0-9]', ' ', doc_slice)\n",
    "\n",
    "    # Remove parantheses\n",
    "    doc_slice = doc_slice.replace('(', ' ').replace(')', ' ').replace(';', ' ').replace('-', '').replace('\\n', ' ')\n",
    "\n",
    "    tokens = doc_slice.split(' ')\n",
    "\n",
    "    tokens = [t for t in tokens if len(t) > 0]\n",
    "    match_token_i = tokens.index(match_nopunc)\n",
    "\n",
    "    if match_token_i - n_tokens <= 0:\n",
    "        return [], [], \"\"\n",
    "\n",
    "    start_i = match_token_i-n_tokens\n",
    "    end_i = match_token_i\n",
    "\n",
    "    # If a prev token started with uppercase, use it\n",
    "    try:\n",
    "        if start_i > 1:\n",
    "            if tokens[start_i-2][0].isupper():\n",
    "                start_i -= 2\n",
    "            else:\n",
    "                if tokens[start_i-1][0].isupper():\n",
    "                    start_i -= 1\n",
    "\n",
    "        word_tokens = tokens[start_i:end_i]\n",
    "\n",
    "        # Drop number token if it is coming first\n",
    "        if word_tokens[0].isdigit():\n",
    "            word_tokens = word_tokens[1:]\n",
    "\n",
    "        # Remove 2 lowercase tokens from start\n",
    "        for _ in range(2):\n",
    "            if word_tokens[0][0].islower():\n",
    "                word_tokens = word_tokens[1:]\n",
    "                start_i += 1\n",
    "\n",
    "    except IndexError:\n",
    "        print(f'IndexError for {match}')\n",
    "        return [], [], \"\"\n",
    "\n",
    "    after_token = \"\"\n",
    "    if len(tokens) > match_token_i + 1:\n",
    "        after_token = tokens[match_token_i + 1]\n",
    "\n",
    "    before_tokens = tokens[max(start_i - 3, 0) : start_i]\n",
    "\n",
    "    return before_tokens, word_tokens, after_token\n",
    "\n",
    "\n",
    "def tokens_are_dataset_name(tokens):\n",
    "    if len(tokens) == 0:\n",
    "        return False\n",
    "    \n",
    "    long_tokens = [t for t in tokens if len(t) > 3]\n",
    "    lowercase_count = len([t for t in long_tokens if t[0].islower()])\n",
    "    uppercase_count = len([t for t in long_tokens if t[0].isupper()])\n",
    "\n",
    "    return lowercase_count < 4 and uppercase_count > 0\n",
    "\n",
    "def after_token_ok(after_token):\n",
    "    if after_token == \"\":\n",
    "        return True\n",
    "\n",
    "    if after_token.lower() in banned_after_tokens:\n",
    "        return False\n",
    "\n",
    "    # Probably plural\n",
    "    if after_token not in ['was', 'has', 'is', 'this'] and after_token[-1].lower() == 's':\n",
    "        return False\n",
    "\n",
    "    # Probably a link\n",
    "    if 'http' in after_token:\n",
    "        return False\n",
    "\n",
    "    # A reference. Datasets don't get referenced like that\n",
    "    if '[' in after_token and ']' in after_token:\n",
    "        return False\n",
    "\n",
    "    # Probably a link\n",
    "    if '/' in after_token:\n",
    "        return False\n",
    "\n",
    "    if 'cell' in after_token:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def before_tokens_ok(before_tokens):\n",
    "    if len(before_tokens) == 0:\n",
    "        return True\n",
    "\n",
    "    if 'by' in before_tokens:\n",
    "        return False\n",
    "\n",
    "    if 'adjusted' in before_tokens:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "#get_words_from_abbr_in_parantheses('BDNF', doc_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "doc_id = train_example_names[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0007f880-0a9b-492d-9a58-76eb0b0e0bd7\n['program for the international assessment of adult competencies']\n{'DML', 'SPSS', 'ITU', 'DRI', 'LCRP'}\n"
     ]
    }
   ],
   "source": [
    "doc_json = load_train_example_by_name(doc_id)\n",
    "\n",
    "doc_labels = list(metadata_train.loc[metadata_train.Id == doc_id, 'dataset_label'].values)\n",
    "doc_labels = [make_single_whitespace(remove_punc(l.strip())).lower() for l in doc_labels]\n",
    "doc_text = ' '.join([s['text'] for s in doc_json])\n",
    "print(doc_id)\n",
    "print(doc_labels)\n",
    "\n",
    "re_find_par = r'\\(([A-Z]{2,}-?[A-Z]{1,}?[a-z]?)[\\);]'\n",
    "matches = set(re.findall(re_find_par, doc_text))\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ratio has data: 0.0\nall has data: 2\nall: 20\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'section_title': 'Introduction',\n",
       "  'text': 'The spur of ICT (Information and Communication Technologies) innovations in the twenty-first century has massively disrupted economies and business models (Christensen, 2013; Tohmatsu, 2012) . Millions of jobs face a high probability of being replaced because computers and the internet are reshaping the labor market (Oliver, 2015) . In the framework of globalization, digital skills are now considered preliminary for securing professional employment across the globe (Pirzada & Khan, 2013) . Consequently, many employers across a wide range of sectors are increasingly viewing ICT skills as an important component of employability (Belt & Richardson, 2005; Johnson & Burden, 2003) . The omnipresence of computing has made digital literacy increasingly critical to success in any occupation (Murray & Pérez, 2014) . In fact, organizations are now identifying digital skills or computer literacy as one of their core values for employability (such as the US Department of Education, the US Department of commerce, the OECD Program for the International Assessment of Adult Competencies and the European Commission).\\nIn particular, in developing countries, digital skills are estimated to reduce poverty levels and increase employment rates (Bennett, Maton, & Kervin, 2008) . ICT competence can bring significant benefits to marginalized groups, allowing these groups to participate more fully in society as it improves employment opportunities, overcomes isolation, builds confidence and leads to further learning (Bunker, 2010) . It\"s not a surprise that Digital Literacy has been deemed an essential life skill (Europea, 2008) . Individuals\" digital engagement has direct implications on their academic performance, labor market success and entrepreneurship uptakes. Those who function better in the digital realm and participate more fully in digitally mediated life enjoy advantages over their digitally disadvantaged counterparts (Robinson et al., 2015) . ICT have become central to every economy and to people\"s quality of life in every society (Olatoye, 2011) .\\nNevertheless, a new form of inequality is emerging, adding to all the existing forms of discrimination (Hilbert, 2011) . Concerns are being raised that the digital divide is leaving behind those most in need of assistance (Greig, 2004) . It is feared that as the role of ICT continues to expand, the exclusion experienced by disadvantaged groups may be accentuated and reinforced, rather than mitigated (Corrigan & Joyce, 2000) . Discussions about digital divides typically refer to socio-economic disparities in accessibility and usage of ICT. The use of such technologies result in several beneficial outcomes while the non-use of such technologies excludes people from full participation in contemporary society (Helsper, Deursen, & Eynon, 2015) . Many studies have established that basic access to digital resources and the skills to effectively use them still escape many economically disadvantaged or traditionally underrepresented portions of the population (Witte & Mannon, 2010) . Thus, digital inequalities can reinforce and exacerbate existing social inequalities (DiMaggio & Garip, 2012) . Further interventions are needed to ensure that the unqualified, the low skilled, the long-term unemployed and those on low incomes are enabled to reap the benefits of new services and opportunities for job seeking through ICT (Lindsay, 2005) .'}]"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "l_search = doc_labels[0]\n",
    "secs_with_label = [section for section in doc_json if l_search in clean_text(section['text'])]\n",
    "\n",
    "has_kw = 0\n",
    "has_kw_all = 0\n",
    "for sec in secs_with_label:\n",
    "    if 'data' in clean_text(sec['text']):\n",
    "        has_kw += 1\n",
    "\n",
    "for sec in doc_json:\n",
    "    if 'data' in clean_text(sec['text']):\n",
    "        has_kw_all += 1\n",
    "\n",
    "print(f'ratio has data: {has_kw / len(secs_with_label)}')\n",
    "print(f'all has data: {has_kw_all}')\n",
    "print(f'all: {len(doc_json)}')\n",
    "secs_with_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "len(secs_with_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'section_title': 'Introduction',\n",
       " 'text': \"International studies reveal that South African learners still show poorer performances in maths than most of their peers worldwide (Reddy et al. 2016) . International comparative studies usually use scales with a fixed mean at 500 points keeping a standard deviation of 100 points to measure learners' competencies. Across the years, South African learners scored an average dramatically below 400 points (Hanushek & Woessmann 2015; Spaull 2013) . Thus, they score more than one standard deviation below the worldwide mean; this equals the lack of more than 2 years of schooling (Hanushek & Woessmann 2015) . Maths competencies are related to the economic development of the country. Better maths competencies across the population of a nation sustainably lead to higher economic growth (Hanushek & Woessmann 2015) . Although the economy of the country increased, the below-average performance of South African learners is stable, not only across the recent Trends in International Mathematics and Science Study (TIMSS) and Program for International Student Assessment (PISA) studies, but it can also be tracked back for the last 50 years (Hanushek & Woessmann 2015) .\\nPoor mathematical knowledge implies enormous individual disadvantages for learners. They earn less, are more often unemployed and have fewer chances to work in the field of their choice (Parsons & Brynner 2005) . Moreover, the educational status of parents affects the educational potential of children. Thus, poor maths performance is likely to replicate in the following generation and it is hard to break the cycle.\\nHowever, research suggests that additional schooling time does not affect learning outcomes positively. It is not only the time learners spend in the school that determines their progress, but Background: Several studies have shown the influence of mathematical knowledge on both individual opportunities and chances for a self-determined and prosperous life as well as the welfare of nations. Against this background, the contents of maths education in the foundation phase as well as the way in which it is conveyed gain importance. While competence-oriented approaches (e.g. the Curriculum Assessment Policy Statements [CAPS] ) state learning goals that all learners should achieve, developmental approaches (e.g. developmental models) describe typical learning trajectories of learners. As both approaches are quite separated, there is a need for bridging the gap between them. also the knowledge they obtain during this time (Hanushek & Woessmann 2015) .\\nSouth African policy is aware that poor maths knowledge of learners leads to severe individual and economic problems. As a reaction to the maths performance misery amongst others, Grade-R was established in 1998 and efforts enhanced within the last years (Van Rensburg 2015) . Grade-R implies both more total learning time and an earlier school start with the intention to increase pupils' knowledge and performance. In particular, Grade-R was supposed to improve learners' school readiness at their entrance into Grade 1 (Van Rensburg 2015) . The term 'school readiness' refers to the experiences and knowledge children gain before they enter school, which are necessary for successful in-school learning. This is of particular interest for maths learning as the acquisition of mathematical competencies is a complex learning process that sets in long before formal schooling (e.g. Carey 2009; Dehaene 2011). As not all children learn at the same pace -because of individual learning capacities and opportunities -their mathematical prerequisites differ both in quantity and quality (Aunola et al. 2004) . It is important to note that the prior knowledge that learners have when they enter school is a good predictor for later learning success (Aunio & Niemvierta 2010) .\\nRegrettably, empirical findings underpin that South Africa's Grade-R has only little effect on learners' school readiness. In particular regarding maths, Grade-R does not substantially improve learners' competencies or school readiness (Reddy et al. 2016; Van der Berg et al. 2013) . Regarding school readiness, Van Rensburg (2015) recently revealed that about half of the South African preschoolers are not school ready even after introduction of Grade-R. The sample included schools from all socio-economic backgrounds and even in the richest quintile, 40% of the students lacked important cognitive prerequisites for formal schooling.\\nThe main reason for the failure of the current Grade-R is seen in the insufficient professional education of the majority of the Grade-R teachers regarding content and pedagogical content knowledge (Van Rensburg 2015; Venkat & Spaull 2015) . We argue that there is no adequate curriculum yet for Grade-R that meets teachers' skills and learners' development by now.\\nAs Grade-R in South Africa is not yet able to promote learners' early numerical knowledge, the question how this can be done remains urgent (Long & Dunne 2014) . Promoting pupils' school readiness involves the contents and their structure (i.e. the curriculum) as well as the expertise and proficiency of the teachers, who convey the contents. This article aims at presenting a comprehensive approach towards an option of better maths education in South Africa. The result of these efforts is a training programme named Meerkat Maths. With the training programme Meerkat Maths, we aim to make research results applicable for in school teaching. To provide a comprehensive training programme, three questions have to be answered: which contents should be addressed by the training, how should it be structured and how should the training be realised?\"}"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "secs_with_label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['the most recent collisions.',\n",
       " 'We used the 2006 Coastal Change Analysis Program for San Clemente Island, CA and western Maine to describe the land cover and land use (National Oceanic and Atmospheric Administration Coastal Services Center 2012).',\n",
       " 'We used the 2001 National Land Cover Database for Onondaga County, NY (Homer et al.',\n",
       " '2007 ).',\n",
       " 'Land-cover and land-use maps were base']"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "sent_tokenize('the most recent collisions.\\nWe used the 2006 Coastal Change Analysis Program for San Clemente Island, CA and western Maine to describe the land cover and land use (National Oceanic and Atmospheric Administration Coastal Services Center 2012). We used the 2001 National Land Cover Database for Onondaga County, NY (Homer et al. 2007 ). Land-cover and land-use maps were base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DEFTs : Tokens ['the', 'design', 'effects'] do not make a dataset name.\nDEFTs : Aftertoken for was in banlist.\nPSEFIRTY : Tokens ['consider', 'type', 'of', 'institution', 'first', 'attended'] do not make a dataset name.\nPSEFIRDA : Tokens ['education', 'enrollment', 'date', 'for', 'valid', 'institutions'] do not make a dataset name.\nBPSLNKWT : Tokens ['This', 'disturbance', 'term', 'inflated', 'the', 'weight'] do not make a dataset name.\nBPSLNKWT : Aftertoken so was in banlist.\nDEFT : Tokens ['dependent', 'variable'] do not make a dataset name.\nSES : Tokens ['status'] do not make a dataset name.\nSES : Aftertoken are was in banlist.\nBYSES : Tokens ['1.', 'Low', 'SES'] do not make a dataset name.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'NEB': ('Beginning Postsecondary Longitudinal Study', 'Data'),\n",
       " 'PSE': ('Postsecondary Education', 'enrollment.'),\n",
       " 'BYFCOMP': ('Low SES BYSES 2. Single parent family', '3.'),\n",
       " 'ANOVA': ('Analysis of Variance', 'was'),\n",
       " 'GED': ('General Educational Development', ','),\n",
       " 'DAS': ('Data File Data Analysis System', 'see'),\n",
       " 'IPEDS': ('Integrated Postsecondary Education Data System', 'data')}"
      ]
     },
     "metadata": {},
     "execution_count": 599
    }
   ],
   "source": [
    "selected_mathces = {}\n",
    "for m in matches:\n",
    "    try:\n",
    "        before_tokens, found_tokens, after_token = get_words_from_abbr_in_parantheses(m, doc_text)\n",
    "    except Exception as e:\n",
    "        print(f'Exception for {m}')\n",
    "        raise e\n",
    "\n",
    "    cond1 = tokens_are_dataset_name(found_tokens)\n",
    "    cond2 = after_token_ok(after_token)\n",
    "    cond3 = before_tokens_ok(before_tokens)\n",
    "\n",
    "    if not cond1:\n",
    "        print(f'{m} : Tokens {found_tokens} do not make a dataset name.')\n",
    "\n",
    "    if not cond2:\n",
    "        print(f'{m} : Aftertoken {after_token} was in banlist.')\n",
    "\n",
    "    if not cond3:\n",
    "        print(f'{m} : Beforetokens {before_tokens} were in banlist.')\n",
    "\n",
    "    if cond1 and cond2 and cond3:\n",
    "        selected_mathces[m] = (' '.join(found_tokens), after_token)\n",
    "\n",
    "selected_mathces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'NEB': ('Beginning Postsecondary Longitudinal Study', 'Data'),\n",
       " 'PSE': ('Postsecondary Education', 'enrollment.'),\n",
       " 'BYFCOMP': ('Low SES BYSES 2. Single parent family', '3.'),\n",
       " 'ANOVA': ('Analysis of Variance', 'was'),\n",
       " 'GED': ('General Educational Development', ','),\n",
       " 'DAS': ('Data File Data Analysis System', 'see'),\n",
       " 'IPEDS': ('Integrated Postsecondary Education Data System', 'data')}"
      ]
     },
     "metadata": {},
     "execution_count": 600
    }
   ],
   "source": [
    "# Drop by keyword\n",
    "matches_not_banned = {m: v for m, v in selected_mathces.items() if m not in banned_kw}\n",
    "matches_not_banned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'NEB': ('Beginning Postsecondary Longitudinal Study', 'Data'),\n",
       " 'PSE': ('Postsecondary Education', 'enrollment.'),\n",
       " 'BYFCOMP': ('Low SES BYSES 2. Single parent family', '3.'),\n",
       " 'GED': ('General Educational Development', ','),\n",
       " 'IPEDS': ('Integrated Postsecondary Education Data System', 'data')}"
      ]
     },
     "metadata": {},
     "execution_count": 601
    }
   ],
   "source": [
    "# Drop by text\n",
    "matches_not_banned = {m: v for m, v in matches_not_banned.items() if not any([b for b in banned_values if b in v[0].lower()])}\n",
    "matches_not_banned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'ECLS-K', 'IEP', 'IRT', 'MD', 'NCES', 'SEM', 'SES'}"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": []
  },
  {
   "source": [
    "clues:\n",
    "- between parantheses\n",
    "- starts with such as\n",
    "- has abbreviation between parantheses\n",
    "- starts with capital letters or all capital letters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "How to process:\n",
    "\n",
    "Split into sentences\n",
    "\n",
    "Keep uppercase letters.\n",
    "\n",
    "remove []\n",
    "\n",
    "keep ()\n",
    "\n",
    "Capital letter words followed by (abbreviation)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ]
}