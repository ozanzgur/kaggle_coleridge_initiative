{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd01b4c7016e99d31c2e7c892573dc93dbd4548eb0a0f5dca22fbf3a690830b4e66",
   "display_name": "Python 3.8.8 64-bit ('torch': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_example_paths = glob.glob('data/train/*.json')\n",
    "test_example_paths = glob.glob('data/test/*.json')\n",
    "\n",
    "train_example_names = [fn.split('.')[0] for fn in os.listdir('data/train')]\n",
    "test_example_names = [fn.split('.')[0] for fn in os.listdir('data/test')]\n",
    "\n",
    "metadata = pd.read_csv('data/train.csv')\n",
    "metadata_train = metadata.loc[metadata.Id.isin(train_example_names)]\n",
    "metadata_test = metadata.loc[metadata.Id.isin(test_example_names)]\n",
    "\n",
    "metadata = pd.read_csv('data/train.csv')\n",
    "metadata_train = metadata.loc[metadata.Id.isin(train_example_names)]\n",
    "metadata_test = metadata.loc[metadata.Id.isin(test_example_names)]\n",
    "\n",
    "_RE_COMBINE_WHITESPACE = re.compile(r\"\\s+\")\n",
    "def make_single_whitespace(text):\n",
    "    return _RE_COMBINE_WHITESPACE.sub(\" \", text).strip()\n",
    "\n",
    "def remove_punc(txt):\n",
    "    return re.sub('[^A-Za-z0-9]+', ' ', str(txt))\n",
    "\n",
    "def load_train_example_by_name(name):\n",
    "    doc_path = os.path.join('data/train', name + '.json')\n",
    "    with open(doc_path) as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def get_doc_id(doc_path):\n",
    "    return os.path.split(train_example_names[0])[-1].split('.')[0]\n",
    "\n",
    "def clean_text(txt):\n",
    "    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower())\n",
    "\n",
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "# Load dataset names\n",
    "df = pd.read_csv('C:\\projects\\personal\\kaggle\\kaggle_coleridge_initiative\\data\\data_set_26897.csv')\n",
    "us_dataset_names = list(df.title.values)\n",
    "us_dataset_names = [make_single_whitespace(remove_punc(n)).lower() for n in us_dataset_names]\n",
    "\n",
    "labels = list(metadata.cleaned_label.unique())\n",
    "labels = sorted(labels, key = len, reverse = True)\n",
    "labels = [l.strip() for l in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_bigrams(label):\n",
    "    tokens = label.split(' ')\n",
    "    return [f'{t1} {t2}' for t1, t2 in zip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "for t in ['and', 'was', 'in']:\n",
    "    stop_words.remove(t)\n",
    "\n",
    "banned_kw = [\n",
    "    'STEM', 'FDA', 'SSH', 'FSIZE', 'PET', 'NCATE', 'TESOL', 'AVHRR-OI',\n",
    "    'ICT',\n",
    "    'AAEA',\n",
    "    'BMI', 'ADGC', 'CDRSUM', 'NASS',\n",
    "    'MMSE', 'CDR', 'SPSS', 'LCRP', 'DML', 'ITU', 'DRI', 'CIPSEA', 'IEP', 'NCES', 'BCG', 'HLM', 'MLLW', 'FDG', 'MRMC'\n",
    "]\n",
    "\n",
    "banned_values = [\n",
    "    'laboratory', 'body mass index', 'admission test', 'neural networks', 'accuracy of', 'chain reaction', 'adversarial network',\n",
    "    'state exam', 'reform act', 'least', 'labeling', 'principal components analysis', 'independent components analysis', 'markov chain', 'monte carlo',\n",
    "    'bayesian information', 'family wise error', 'posterior anterior', 'Bidirectional Encoder', 'Morphometry', 'Integral', 'T2*weighted', 'T2-weighted',\n",
    "    'T2weighted', 'T1*weighted', 'T1-weighted', 'T1weighted', 'EMCI', 'Learning Test', 'Gradepoint average', 'doctor of', 'masters of',\n",
    "    'Expected Family Contribution', 'life in', 'Long Short Term', 'Long ShortTerm', 'LSTM', 'lipoprotein', 'Support Vector Machine', 'User Interface',\n",
    "    'National Institute of', 'glucose', 'Research Division', '%', 'Heating Weeks', 'Public Management', 'Theory', 'Middle East respiratory',\n",
    "    'Discriminant Analysis', 'boltzmann', 'Disease Control and Prevention', 'polymorphism', 'positron emission tomography', 'dorsolateral', 'Data Analysis System',\n",
    "    'Analysis Kit', 'Google', 'Principal Analysis', 'Cognitive Impairment', 'Analysis of Variance'\n",
    "    ]\n",
    "\n",
    "banned_after_tokens = stop_words\n",
    "\n",
    "banned_values = [b.lower() for b in banned_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_from_abbr_in_parantheses(match, doc_text):\n",
    "    try:\n",
    "        match_i = doc_text.index(f'({match})')\n",
    "    except:\n",
    "        try:\n",
    "            match_i = doc_text.index(f'({match};')\n",
    "        except:\n",
    "            match_i = doc_text.index(f'({match}')\n",
    "\n",
    "    match_nopunc = remove_punc(match).replace(' ', '')\n",
    "    n_tokens = len(match_nopunc.replace(' ', ''))\n",
    "\n",
    "    slice_start_i = max(match_i - 200, 0)\n",
    "    slice_end_i = min(match_i + len(match) + 40, len(doc_text))\n",
    "    doc_slice = doc_text[slice_start_i: slice_end_i]\n",
    "    \n",
    "    # Remove dates\n",
    "    doc_slice = re.sub(r'(19|20)[0-9][0-9]', ' ', doc_slice)\n",
    "\n",
    "    # Remove parantheses\n",
    "    doc_slice = doc_slice.replace('(', ' ').replace(')', ' ').replace(';', ' ').replace('-', '').replace('\\n', ' ')\n",
    "\n",
    "    tokens = doc_slice.split(' ')\n",
    "\n",
    "    tokens = [t for t in tokens if len(t) > 0]\n",
    "    match_token_i = tokens.index(match_nopunc)\n",
    "\n",
    "    if match_token_i - n_tokens <= 0:\n",
    "        return [], [], \"\"\n",
    "\n",
    "    start_i = match_token_i-n_tokens\n",
    "    end_i = match_token_i\n",
    "\n",
    "    # If a prev token started with uppercase, use it\n",
    "    try:\n",
    "        if start_i > 1:\n",
    "            if tokens[start_i-2][0].isupper():\n",
    "                start_i -= 2\n",
    "            else:\n",
    "                if tokens[start_i-1][0].isupper():\n",
    "                    start_i -= 1\n",
    "\n",
    "        word_tokens = tokens[start_i:end_i]\n",
    "\n",
    "        # Drop number token if it is coming first\n",
    "        if word_tokens[0].isdigit():\n",
    "            word_tokens = word_tokens[1:]\n",
    "\n",
    "        # Remove 2 lowercase tokens from start\n",
    "        for _ in range(2):\n",
    "            if word_tokens[0][0].islower():\n",
    "                word_tokens = word_tokens[1:]\n",
    "                start_i += 1\n",
    "\n",
    "    except IndexError:\n",
    "        print(f'IndexError for {match}')\n",
    "        return [], [], \"\"\n",
    "\n",
    "    after_token = \"\"\n",
    "    if len(tokens) > match_token_i + 1:\n",
    "        after_token = tokens[match_token_i + 1]\n",
    "\n",
    "    before_tokens = tokens[max(start_i - 3, 0) : start_i]\n",
    "\n",
    "    return before_tokens, word_tokens, after_token\n",
    "\n",
    "\n",
    "def tokens_are_dataset_name(tokens):\n",
    "    if len(tokens) == 0:\n",
    "        return False\n",
    "    \n",
    "    long_tokens = [t for t in tokens if len(t) > 3]\n",
    "    lowercase_count = len([t for t in long_tokens if t[0].islower()])\n",
    "    uppercase_count = len([t for t in long_tokens if t[0].isupper()])\n",
    "\n",
    "    return lowercase_count < 4 and uppercase_count > 0\n",
    "\n",
    "def after_token_ok(after_token):\n",
    "    if after_token == \"\":\n",
    "        return True\n",
    "\n",
    "    if after_token.lower() in banned_after_tokens:\n",
    "        return False\n",
    "\n",
    "    # Probably plural\n",
    "    if after_token not in ['was', 'has', 'is', 'this'] and after_token[-1].lower() == 's':\n",
    "        return False\n",
    "\n",
    "    # Probably a link\n",
    "    if 'http' in after_token:\n",
    "        return False\n",
    "\n",
    "    # A reference. Datasets don't get referenced like that\n",
    "    if '[' in after_token and ']' in after_token:\n",
    "        return False\n",
    "\n",
    "    # Probably a link\n",
    "    if '/' in after_token:\n",
    "        return False\n",
    "\n",
    "    if 'cell' in after_token:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def before_tokens_ok(before_tokens):\n",
    "    if len(before_tokens) == 0:\n",
    "        return True\n",
    "\n",
    "    if 'by' in before_tokens:\n",
    "        return False\n",
    "\n",
    "    if 'adjusted' in before_tokens:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "#get_words_from_abbr_in_parantheses('BDNF', doc_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "doc_id = train_example_names[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "7dd73c7e-96f5-4978-942d-24235bc65758\n['survey of industrial research and development', 'national science foundation survey of industrial research and development']\n{'PCAST', 'GPT'}\n"
     ]
    }
   ],
   "source": [
    "doc_json = load_train_example_by_name(doc_id)\n",
    "\n",
    "doc_labels = list(metadata_train.loc[metadata_train.Id == doc_id, 'dataset_label'].values)\n",
    "doc_labels = [make_single_whitespace(remove_punc(l.strip())).lower() for l in doc_labels]\n",
    "doc_text = ' '.join([s['text'] for s in doc_json])\n",
    "print(doc_id)\n",
    "print(doc_labels)\n",
    "\n",
    "re_find_par = r'\\(([A-Z]{2,}-?[A-Z]{1,}?[a-z]?)[\\);]'\n",
    "matches = set(re.findall(re_find_par, doc_text))\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ratio has data: 1.0\nall has data: 7\nall: 10\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'section_title': 'Crowding out across sectors',\n",
       "  'text': 'We begin by looking at the effect of energy R&D spending on overall R&D activity at the sectoral level. Here, we are interested in whether R&D flows across sectors when energy R&D levels change, so that there is a net draw on R&D away from specific sectors. For this, we use R&D data from the National Science Foundation Survey of Industrial Research and Development. One limitation to these data is that industry-level energy 1 Throughout the paper, the term \"crowding out\" is not meant to have normative implications, but simply to refer to a case where energy R&D replaces other types of energy R&D. While we explore whether this occurs due to financial constraints or changes in research composition due to profit-maximizing behavior, for expositional purposes we refer to both types of changes as crowding out. 2 In a related vein, several authors have asked whether increases in defense R&D spending crowds out other R&D. Examples include Morales-Ramos (2002), Buck et al. (1993), and Mueller and Atesoglu (1993). 3 Recent papers on R&D financing constraints include Brown et al. (2009), who find financial constraints for young, but not mature, firms, Hall et al. (1999), who find stronger financial constraints in the U.S. than in Japan and France; Harhoff (1998), who finds weaker constraints in Germany, with smaller firms most affected; Bloch (2005), who finds financial constraints for smaller firms in Denmark, and Bougheas, Görg, & Strobl (2003), who find evidence of financial constraints in Irish firms. Hall (2002) provides a review of this literature on firm-level financing of R&D. R&D data are not available for all industries. Although published energy R&D data include industry-level breakdowns, there are many missing observations in these data. This is particularly true in industries that do not perform much energy R&D. Thus, we are only able to include economywide energy R&D financed and performed by industry. This allows us to focus on crowding out across sectors. That is, as the total level of energy R&D spending goes up, do R&D resources flow from non-energy sectors to those doing more energy R&D? Fig. 1 shows the trends in this variable, for both company-financed and federal-government financed energy R&D performed by industry. Defining IRD i,t as company-financed R&D performed in industry i at time t, ERD t as the level of total company-financed energy R&D spending in year t, FEDRD t as federally-funded R&D performed by industry, 4 and Y i,t as value added in industry i at time t, we propose the following relationship: All variables are in levels, rather than logs, to test whether a dollar of energy R&D crowds out a dollar of overall R&D. 5 We include the lagged dependent variable to allow for gradual adjustment of R&D in response to changing conditions. Allowing further for gradual adjustment, we estimate an alternative model using lagged values of value added and federal R&D. 6 Standard errors are corrected for heteroskedasticity using robust standard errors. Because we are using panel data, lagged industry-performed R&D may be endogenous. In short time series panels, lagged values of the dependent variable are likely correlated with the fixed effect. Thus, we use an instrumental variables approach, using one-and two-year lagged values of the other independent variables as instruments. Dynamic panel bias becomes insignificant as the number of time periods increases (Roodman, 2009). As such, we estimate alternative models treating lagged industryperformed R&D as either endogenous or exogenous. Our strategy for identifying crowding out across industries is to see how changes in the overall level of energy R&D spending affect company R&D performance at the industry-level. However, because we have total industry-level R&D on the left-hand side, but economy-wide energy R&D on the right-hand side, our dependent variable includes energy R&D financed by that industry. This limits the industries to which we can apply the model, since in some industries the dependent variable will include energy R&D for that industry. For an industry that does not perform energy R&D, such as the food industry, this is not a problem. A coefficient of 0 on the energy R&D variable in these industries suggests that R&D in that industry is not affected by increases in overall energy R&D, so that crowding out is not a problem. For these industries, we would be concerned about crowding out only if the coefficient on energy R&D was negative-implying that an economy-wide increase in energy R&D took R&D resources away from this industry. However, it is unclear how to interpret the energy R&D coefficient for industries that perform a significant amount of energy R&D. In these industries, as energy R&D increases, their overall R&D spending should increase, as some of the economy-wide energy R&D occurs within the industry. However, without knowing how much of the economy-wide energy R&D occurs within each industry, we do not know the magnitude by which overall R&D should increase. As a result, in this section we focus only on industries performing little energy R&D. We begin by looking at the percentage of R&D devoted to energy R&D in years for which data are available, and include only industries which spent, on average, less than 5% of overall R&D on energy R&D. 7 Because of data availability, we look at two samples. From 1983 to 1997, we have data for 16 industries, of which 11 spend less than 5% of their R&D budget on energy. However, this does not allow us to look at crowding out during the energy crisis of the 1970s, when energy R&D levels were highest. Thus, we also look at a subsample of 7 industries that have data available from 1973 to 1997. Of these, only 3 spend less than 5% of their R&D budget on energy. Tables 1 and 2 list the industries in the larger and smaller samples, respectively, along with descriptive data on the key industry variables. The tables also show the average percentage of R&D devoted to energy within each sector, for the years in which industrylevel detail is available. In each table, the industries are sorted by this percentage. Finally, note that the last two lines of each table present summary statistics for all industries with more than or less than five percent of R&D devoted to energy. In both samples, energy intensive industries are larger. These include industries such as petroleum refining 4 Unfortunately, there are several missing observations in the industry-level data for federally-funded R&D performed by industry, so that we can only include aggregate levels of government-funded R&D. 5 At the request of one reviewer, we have also run the model in log-log form. The main results do not change and are available from the authors by request. 6 In both cases, we use current energy R&D to test for crowding out, as this allows us to test for potential supply-side constraints. Such constraints suggest that the total supply of R&D is inelastic, so that contemporaneous increases in one type of R&D result in a loss of other types of R&D. 7 For sensitivity analysis, we also obtained results using cutoffs of 1% and 10% for energy-R&D intensive industries. These are available from the authors upon request. and transportation equipment. In the following section, we use firm level data to focus on activity within one energy intensive sectors. Table 3 presents our results. The top section shows results for the longer sample, which begins in 1974 but only includes three industries. The first two columns treat lagged industry R&D as endogenous, while the second two do not. Note that the results are generally similar whether the lagged value is endogenous or exogenous. When endogenous, the fit of the first stage is somewhat poor, with an F-stat slightly less than the \"rule of thumb\" of 10, suggesting weak instruments (Baum et al., 2007;Staiger and Stock, 1997). Moreover, the p-value for Hansen\\'s J-statistic suggests the instruments are endogenous when using lagged values of all control variables. Given this, we get a slightly better fit of the model when lagged R&D is exogenous. Moreover, the key results don\\'t change, so we consider this to be our preferred specification. In all specifications, the insignificant coefficient on economy-wide private energy R&D spending suggests that increases in energy R&D elsewhere in the economy have no Table 3 Industry-level crowding out results. 3 Industries: 1974Industries: -1998 Industry R&D endogenous Industry R&D exogenous  : 1984-1998 Industry R&D endogenous Industry R&D exogenous  effect on overall R&D spending in these specific industries. Thus, we find that increases in energy R&D spending do not appear to crowd out R&D in unrelated sectors. Results for other variables are generally as expected. The model suggests that R&D adjusts gradually, with the coefficient on lagged industry R&D between 0.75 and 0.85. Industry value added has a small impact on overall R&D, and is significant only when using current value added as the control. When significant, the coefficient on value added of about 0.03, which seems reasonable given that just over two percent of GDP is devoted to R&D. Finally, the coefficient on federal R&D is near 0 for all cases, suggesting little crowding out of overall private R&D spending from overall levels of government R&D. The bottom panel of Table 3 presents results for the shorter time series but with 11 industries. The results are similar here, but the fit of the model is poorer. Once again, the instruments perform poorly, so we focus on the results with the lagged dependent variable exogenous. We again see evidence of gradual adjustment, and a positive coefficient on value added. More importantly, there is once again no evidence of crowding out. The results of these regressions suggest that, during past spikes in energy research activity, research funding did not flow from non-energy R&D sectors to sectors performing R&D. We thus turn our attention to within sector R&D activity, to ascertain whether increases in energy R&D lead to reductions in other types of R&D within the same industry.'}]"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "l_search = doc_labels[0]\n",
    "secs_with_label = [section for section in doc_json if l_search in clean_text(section['text'])]\n",
    "\n",
    "has_kw = 0\n",
    "has_kw_all = 0\n",
    "for sec in secs_with_label:\n",
    "    if 'data' in clean_text(sec['text']):\n",
    "        has_kw += 1\n",
    "\n",
    "for sec in doc_json:\n",
    "    if 'data' in clean_text(sec['text']):\n",
    "        has_kw_all += 1\n",
    "\n",
    "print(f'ratio has data: {has_kw / len(secs_with_label)}')\n",
    "print(f'all has data: {has_kw_all}')\n",
    "print(f'all: {len(doc_json)}')\n",
    "secs_with_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "len(secs_with_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'section_title': 'Introduction',\n",
       " 'text': \"International studies reveal that South African learners still show poorer performances in maths than most of their peers worldwide (Reddy et al. 2016) . International comparative studies usually use scales with a fixed mean at 500 points keeping a standard deviation of 100 points to measure learners' competencies. Across the years, South African learners scored an average dramatically below 400 points (Hanushek & Woessmann 2015; Spaull 2013) . Thus, they score more than one standard deviation below the worldwide mean; this equals the lack of more than 2 years of schooling (Hanushek & Woessmann 2015) . Maths competencies are related to the economic development of the country. Better maths competencies across the population of a nation sustainably lead to higher economic growth (Hanushek & Woessmann 2015) . Although the economy of the country increased, the below-average performance of South African learners is stable, not only across the recent Trends in International Mathematics and Science Study (TIMSS) and Program for International Student Assessment (PISA) studies, but it can also be tracked back for the last 50 years (Hanushek & Woessmann 2015) .\\nPoor mathematical knowledge implies enormous individual disadvantages for learners. They earn less, are more often unemployed and have fewer chances to work in the field of their choice (Parsons & Brynner 2005) . Moreover, the educational status of parents affects the educational potential of children. Thus, poor maths performance is likely to replicate in the following generation and it is hard to break the cycle.\\nHowever, research suggests that additional schooling time does not affect learning outcomes positively. It is not only the time learners spend in the school that determines their progress, but Background: Several studies have shown the influence of mathematical knowledge on both individual opportunities and chances for a self-determined and prosperous life as well as the welfare of nations. Against this background, the contents of maths education in the foundation phase as well as the way in which it is conveyed gain importance. While competence-oriented approaches (e.g. the Curriculum Assessment Policy Statements [CAPS] ) state learning goals that all learners should achieve, developmental approaches (e.g. developmental models) describe typical learning trajectories of learners. As both approaches are quite separated, there is a need for bridging the gap between them. also the knowledge they obtain during this time (Hanushek & Woessmann 2015) .\\nSouth African policy is aware that poor maths knowledge of learners leads to severe individual and economic problems. As a reaction to the maths performance misery amongst others, Grade-R was established in 1998 and efforts enhanced within the last years (Van Rensburg 2015) . Grade-R implies both more total learning time and an earlier school start with the intention to increase pupils' knowledge and performance. In particular, Grade-R was supposed to improve learners' school readiness at their entrance into Grade 1 (Van Rensburg 2015) . The term 'school readiness' refers to the experiences and knowledge children gain before they enter school, which are necessary for successful in-school learning. This is of particular interest for maths learning as the acquisition of mathematical competencies is a complex learning process that sets in long before formal schooling (e.g. Carey 2009; Dehaene 2011). As not all children learn at the same pace -because of individual learning capacities and opportunities -their mathematical prerequisites differ both in quantity and quality (Aunola et al. 2004) . It is important to note that the prior knowledge that learners have when they enter school is a good predictor for later learning success (Aunio & Niemvierta 2010) .\\nRegrettably, empirical findings underpin that South Africa's Grade-R has only little effect on learners' school readiness. In particular regarding maths, Grade-R does not substantially improve learners' competencies or school readiness (Reddy et al. 2016; Van der Berg et al. 2013) . Regarding school readiness, Van Rensburg (2015) recently revealed that about half of the South African preschoolers are not school ready even after introduction of Grade-R. The sample included schools from all socio-economic backgrounds and even in the richest quintile, 40% of the students lacked important cognitive prerequisites for formal schooling.\\nThe main reason for the failure of the current Grade-R is seen in the insufficient professional education of the majority of the Grade-R teachers regarding content and pedagogical content knowledge (Van Rensburg 2015; Venkat & Spaull 2015) . We argue that there is no adequate curriculum yet for Grade-R that meets teachers' skills and learners' development by now.\\nAs Grade-R in South Africa is not yet able to promote learners' early numerical knowledge, the question how this can be done remains urgent (Long & Dunne 2014) . Promoting pupils' school readiness involves the contents and their structure (i.e. the curriculum) as well as the expertise and proficiency of the teachers, who convey the contents. This article aims at presenting a comprehensive approach towards an option of better maths education in South Africa. The result of these efforts is a training programme named Meerkat Maths. With the training programme Meerkat Maths, we aim to make research results applicable for in school teaching. To provide a comprehensive training programme, three questions have to be answered: which contents should be addressed by the training, how should it be structured and how should the training be realised?\"}"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "secs_with_label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['the most recent collisions.',\n",
       " 'We used the 2006 Coastal Change Analysis Program for San Clemente Island, CA and western Maine to describe the land cover and land use (National Oceanic and Atmospheric Administration Coastal Services Center 2012).',\n",
       " 'We used the 2001 National Land Cover Database for Onondaga County, NY (Homer et al.',\n",
       " '2007 ).',\n",
       " 'Land-cover and land-use maps were base']"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "sent_tokenize('the most recent collisions.\\nWe used the 2006 Coastal Change Analysis Program for San Clemente Island, CA and western Maine to describe the land cover and land use (National Oceanic and Atmospheric Administration Coastal Services Center 2012). We used the 2001 National Land Cover Database for Onondaga County, NY (Homer et al. 2007 ). Land-cover and land-use maps were base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DEFTs : Tokens ['the', 'design', 'effects'] do not make a dataset name.\nDEFTs : Aftertoken for was in banlist.\nPSEFIRTY : Tokens ['consider', 'type', 'of', 'institution', 'first', 'attended'] do not make a dataset name.\nPSEFIRDA : Tokens ['education', 'enrollment', 'date', 'for', 'valid', 'institutions'] do not make a dataset name.\nBPSLNKWT : Tokens ['This', 'disturbance', 'term', 'inflated', 'the', 'weight'] do not make a dataset name.\nBPSLNKWT : Aftertoken so was in banlist.\nDEFT : Tokens ['dependent', 'variable'] do not make a dataset name.\nSES : Tokens ['status'] do not make a dataset name.\nSES : Aftertoken are was in banlist.\nBYSES : Tokens ['1.', 'Low', 'SES'] do not make a dataset name.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'NEB': ('Beginning Postsecondary Longitudinal Study', 'Data'),\n",
       " 'PSE': ('Postsecondary Education', 'enrollment.'),\n",
       " 'BYFCOMP': ('Low SES BYSES 2. Single parent family', '3.'),\n",
       " 'ANOVA': ('Analysis of Variance', 'was'),\n",
       " 'GED': ('General Educational Development', ','),\n",
       " 'DAS': ('Data File Data Analysis System', 'see'),\n",
       " 'IPEDS': ('Integrated Postsecondary Education Data System', 'data')}"
      ]
     },
     "metadata": {},
     "execution_count": 599
    }
   ],
   "source": [
    "selected_mathces = {}\n",
    "for m in matches:\n",
    "    try:\n",
    "        before_tokens, found_tokens, after_token = get_words_from_abbr_in_parantheses(m, doc_text)\n",
    "    except Exception as e:\n",
    "        print(f'Exception for {m}')\n",
    "        raise e\n",
    "\n",
    "    cond1 = tokens_are_dataset_name(found_tokens)\n",
    "    cond2 = after_token_ok(after_token)\n",
    "    cond3 = before_tokens_ok(before_tokens)\n",
    "\n",
    "    if not cond1:\n",
    "        print(f'{m} : Tokens {found_tokens} do not make a dataset name.')\n",
    "\n",
    "    if not cond2:\n",
    "        print(f'{m} : Aftertoken {after_token} was in banlist.')\n",
    "\n",
    "    if not cond3:\n",
    "        print(f'{m} : Beforetokens {before_tokens} were in banlist.')\n",
    "\n",
    "    if cond1 and cond2 and cond3:\n",
    "        selected_mathces[m] = (' '.join(found_tokens), after_token)\n",
    "\n",
    "selected_mathces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'NEB': ('Beginning Postsecondary Longitudinal Study', 'Data'),\n",
       " 'PSE': ('Postsecondary Education', 'enrollment.'),\n",
       " 'BYFCOMP': ('Low SES BYSES 2. Single parent family', '3.'),\n",
       " 'ANOVA': ('Analysis of Variance', 'was'),\n",
       " 'GED': ('General Educational Development', ','),\n",
       " 'DAS': ('Data File Data Analysis System', 'see'),\n",
       " 'IPEDS': ('Integrated Postsecondary Education Data System', 'data')}"
      ]
     },
     "metadata": {},
     "execution_count": 600
    }
   ],
   "source": [
    "# Drop by keyword\n",
    "matches_not_banned = {m: v for m, v in selected_mathces.items() if m not in banned_kw}\n",
    "matches_not_banned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'NEB': ('Beginning Postsecondary Longitudinal Study', 'Data'),\n",
       " 'PSE': ('Postsecondary Education', 'enrollment.'),\n",
       " 'BYFCOMP': ('Low SES BYSES 2. Single parent family', '3.'),\n",
       " 'GED': ('General Educational Development', ','),\n",
       " 'IPEDS': ('Integrated Postsecondary Education Data System', 'data')}"
      ]
     },
     "metadata": {},
     "execution_count": 601
    }
   ],
   "source": [
    "# Drop by text\n",
    "matches_not_banned = {m: v for m, v in matches_not_banned.items() if not any([b for b in banned_values if b in v[0].lower()])}\n",
    "matches_not_banned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'ECLS-K', 'IEP', 'IRT', 'MD', 'NCES', 'SEM', 'SES'}"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": []
  },
  {
   "source": [
    "clues:\n",
    "- between parantheses\n",
    "- starts with such as\n",
    "- has abbreviation between parantheses\n",
    "- starts with capital letters or all capital letters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "How to process:\n",
    "\n",
    "Split into sentences\n",
    "\n",
    "Keep uppercase letters.\n",
    "\n",
    "remove []\n",
    "\n",
    "keep ()\n",
    "\n",
    "Capital letter words followed by (abbreviation)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ]
}