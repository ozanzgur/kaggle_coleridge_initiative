{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dont forget to add all functions and connection_tokens to kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2020/07/transfer-learning-for-nlp-fine-tuning-bert-for-text-classification/\n",
    "### Model from:\n",
    "\n",
    "https://github.com/allenai/scibert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import random\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "import pickle\n",
    "\n",
    "train_example_names = [fn.split('.')[0] for fn in os.listdir('data/train')]\n",
    "test_example_names = [fn.split('.')[0] for fn in os.listdir('data/test')]\n",
    "\n",
    "metadata = pd.read_csv('data/train.csv')\n",
    "docIdx = train_example_names.copy()\n",
    "\n",
    "connection_tokens = {'s', 'of', 'and', 'in', 'on', 'for', 'from', 'the', 'act', 'coast', 'future', 'system', 'per', \"'\", ','}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Name Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "387\n"
     ]
    }
   ],
   "source": [
    "def text_cleaning(text):\n",
    "    text = re.sub('[^A-Za-z]+', ' ', str(text)).strip() # remove unnecessary literals\n",
    "\n",
    "    # remove extra spaces\n",
    "    text = re.sub(\"\\s+\",\" \", text)\n",
    "\n",
    "    return text.lower().strip()\n",
    "\n",
    "def is_name_ok(text):\n",
    "    if len([c for c in text if c.isalnum()]) < 4:\n",
    "        return False\n",
    "    \n",
    "    tokens = [t for t in text.split(' ') if len(t) > 3]\n",
    "    tokens = [t for t in tokens if not t in connection_tokens]\n",
    "    if len(tokens) < 3:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "with open('data/all_preds_selected.csv', 'r') as f:\n",
    "    selected_pred_labels = f.readlines()\n",
    "    selected_pred_labels = [l.strip() for l in selected_pred_labels]\n",
    "\n",
    "existing_labels = [text_cleaning(x) for x in metadata['dataset_label']] +\\\n",
    "                  [text_cleaning(x) for x in metadata['dataset_title']] +\\\n",
    "                  [text_cleaning(x) for x in metadata['cleaned_label']] +\\\n",
    "                  [text_cleaning(x) for x in selected_pred_labels]\n",
    "\n",
    "\"\"\"to_remove = [\n",
    "    'frequently asked questions', 'total maximum daily load tmd', 'health care facilities',\n",
    "    'traumatic brain injury', 'north pacific high', 'droplet number concentration', 'great slave lake',\n",
    "    'census block groups'\n",
    "]\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"df = pd.read_csv(r'C:\\projects\\personal\\kaggle\\kaggle_coleridge_initiative\\string_search\\data\\gov_data.csv')\n",
    "print(len(df))\n",
    "\n",
    "\n",
    "df['title'] = df.title.apply(text_cleaning)\n",
    "titles = list(df.title.unique())\n",
    "titles = [t for t in titles if not t in to_remove]\n",
    "df = pd.DataFrame({'title': titles})\n",
    "df = df.loc[df.title.apply(is_name_ok)]\n",
    "df = pd.concat([df, pd.DataFrame({'title': existing_labels})], ignore_index= True).reset_index(drop = True)\n",
    "titles = list(df.title.unique())\n",
    "df = pd.DataFrame({'title': titles})\n",
    "df['title'] = df.title.apply(text_cleaning)\"\"\"\n",
    "\n",
    "# Sort labels by length in ascending order\n",
    "#existing_labels = sorted(list(df.title.values), key = len, reverse = True)\n",
    "\n",
    "existing_labels = list(set(existing_labels))\n",
    "existing_labels = sorted(existing_labels, key = len, reverse = True)\n",
    "existing_labels = [l for l in existing_labels if len(l.split(' ')) < 15]\n",
    "#del df\n",
    "#existing_labels.remove('adni')\n",
    "\n",
    "print(len(existing_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataframe for tokens and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_example_by_name(name):\n",
    "    doc_path = os.path.join('data/train', name + '.json')\n",
    "    with open(doc_path) as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def load_test_example_by_name(name):\n",
    "    doc_path = os.path.join('data/test', name + '.json')\n",
    "    with open(doc_path) as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "\n",
    "match_puncs_re = r\"([.,!?()\\-;\\[\\]+\\\\\\/@:<>#_{}&%'*=\" + r'\"' + r\"|])\"\n",
    "match_puncs_re = re.compile(match_puncs_re)\n",
    "\n",
    "def text_cleaning_upper(text):\n",
    "    text = re.sub('[^A-Za-z]+', ' ', str(text)).strip() # remove unnecessary literals\n",
    "\n",
    "    # remove extra spaces\n",
    "    text = re.sub(\"\\s+\",\" \", text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "def has_connected_uppercase(tokens):\n",
    "    if len(tokens) < 5:\n",
    "        return False\n",
    "\n",
    "    group_len = 0\n",
    "    n_long_tokens = 0\n",
    "    for token in tokens:\n",
    "        token_lower = token.lower()\n",
    "        if token[0].isupper():\n",
    "            if token_lower not in connection_tokens:\n",
    "                if len(token) > 2:\n",
    "                    n_long_tokens += 1\n",
    "\n",
    "                group_len += 1\n",
    "                if group_len > 2 and n_long_tokens > 0:\n",
    "                    return True\n",
    "\n",
    "        else:\n",
    "            if token_lower not in connection_tokens:\n",
    "                group_len = 0\n",
    "                n_long_tokens = 0\n",
    "\n",
    "    return False\n",
    "\n",
    "def sent_has_acronym(tokens):\n",
    "    # Acronym check\n",
    "    for token in tokens:\n",
    "        if len(token) > 3 and token.isupper():\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def sent_is_candidate(clean_sentence):\n",
    "    tokens = clean_sentence.split(' ')\n",
    "    \n",
    "    if sent_has_acronym(tokens):\n",
    "        return True\n",
    "    else:\n",
    "        return has_connected_uppercase(tokens)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"pos_sentences = []\\nneg_sentences = []\\ndocs_no_pos = []\\ntotal_sentences = 0\\n\\n\\n\\ndef process_doc(doc_id):\\n    global total_sentences\\n    doc_json = load_train_example_by_name(doc_id)\\n    doc_text = ' '.join([sec['text'] for sec in doc_json])\\n    doc_has_pos = False\\n\\n    # Tokenize sentencewise\\n    sentences = sent_tokenize(doc_text)\\n    total_sentences += len(sentences)\\n\\n    for sentence in sentences:\\n        clean_sentence = text_cleaning_upper(sentence)\\n        is_candidate = sent_is_candidate(clean_sentence)\\n\\n        has_label = False\\n        if is_candidate:\\n            clean_sentence_lower = clean_sentence.lower()\\n            for clean_label in existing_labels:\\n                if clean_label in clean_sentence_lower:\\n                    has_label = True\\n                    break\\n        \\n        # Store sentence in list if candidate\\n        # Non-candidate sentences are discarded\\n        if has_label:\\n            pos_sentences.append(sentence)\\n            doc_has_pos = True\\n        elif is_candidate:\\n            neg_sentences.append(sentence)\\n\\n    if not doc_has_pos:\\n        docs_no_pos.append(doc_id)\\n\\n#process_doc('0026563b-d5b3-417d-bd25-7656b97a044f')\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"pos_sentences = []\n",
    "neg_sentences = []\n",
    "docs_no_pos = []\n",
    "total_sentences = 0\n",
    "\n",
    "\n",
    "\n",
    "def process_doc(doc_id):\n",
    "    global total_sentences\n",
    "    doc_json = load_train_example_by_name(doc_id)\n",
    "    doc_text = ' '.join([sec['text'] for sec in doc_json])\n",
    "    doc_has_pos = False\n",
    "\n",
    "    # Tokenize sentencewise\n",
    "    sentences = sent_tokenize(doc_text)\n",
    "    total_sentences += len(sentences)\n",
    "\n",
    "    for sentence in sentences:\n",
    "        clean_sentence = text_cleaning_upper(sentence)\n",
    "        is_candidate = sent_is_candidate(clean_sentence)\n",
    "\n",
    "        has_label = False\n",
    "        if is_candidate:\n",
    "            clean_sentence_lower = clean_sentence.lower()\n",
    "            for clean_label in existing_labels:\n",
    "                if clean_label in clean_sentence_lower:\n",
    "                    has_label = True\n",
    "                    break\n",
    "        \n",
    "        # Store sentence in list if candidate\n",
    "        # Non-candidate sentences are discarded\n",
    "        if has_label:\n",
    "            pos_sentences.append(sentence)\n",
    "            doc_has_pos = True\n",
    "        elif is_candidate:\n",
    "            neg_sentences.append(sentence)\n",
    "\n",
    "    if not doc_has_pos:\n",
    "        docs_no_pos.append(doc_id)\n",
    "\n",
    "#process_doc('0026563b-d5b3-417d-bd25-7656b97a044f')\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and Save Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import pickle\\nassert len(docIdx) > 0\\n\\npos_sentences = []\\nneg_sentences = []\\ndocs_no_pos = []\\ntotal_sentences = 0\\n\\npbar = tqdm(docIdx)\\nfor doc_id in pbar:\\n    process_doc(doc_id)\\n    pbar.set_description(        f'pos_size: {len(pos_sentences)}, neg_size: {len(neg_sentences)}, no pos label doc: {len(docs_no_pos)}, n_sentences: {total_sentences}')\\n\\nwith open(f'data/bert_ner_sentences/pos.pkl', 'wb') as f:\\n    pickle.dump(pos_sentences, f)\\n\\nwith open(f'data/bert_ner_sentences/neg.pkl', 'wb') as f:\\n    pickle.dump(neg_sentences, f)\\n\\nprint(f'pos size: {len(pos_sentences)}')\\nprint(f'neg size: {len(neg_sentences)}')\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import pickle\n",
    "assert len(docIdx) > 0\n",
    "\n",
    "pos_sentences = []\n",
    "neg_sentences = []\n",
    "docs_no_pos = []\n",
    "total_sentences = 0\n",
    "\n",
    "pbar = tqdm(docIdx)\n",
    "for doc_id in pbar:\n",
    "    process_doc(doc_id)\n",
    "    pbar.set_description(\\\n",
    "        f'pos_size: {len(pos_sentences)}, neg_size: {len(neg_sentences)}, no pos label doc: {len(docs_no_pos)}, n_sentences: {total_sentences}')\n",
    "\n",
    "with open(f'data/bert_ner_sentences/pos.pkl', 'wb') as f:\n",
    "    pickle.dump(pos_sentences, f)\n",
    "\n",
    "with open(f'data/bert_ner_sentences/neg.pkl', 'wb') as f:\n",
    "    pickle.dump(neg_sentences, f)\n",
    "\n",
    "print(f'pos size: {len(pos_sentences)}')\n",
    "print(f'neg size: {len(neg_sentences)}')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metadata.loc[metadata.Id == docs_no_pos[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import pickle\n",
    "\n",
    "with open(f'data/classifier_output/pos_classified.pkl', 'rb') as f:\n",
    "    pos_sentences = pickle.load(f)\n",
    "\n",
    "\"\"\"with open(f'data/bert_ner_sentences/neg.pkl', 'rb') as f:\n",
    "    neg_sentences = pickle.load(f)\n",
    "\"\"\"\n",
    "print(f'pos size: {len(pos_sentences)}')\n",
    "#print(f'neg size: {len(neg_sentences)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f'data/selected_sentences/pos.pkl', 'rb') as f:\n",
    "    pos_sentences_raw = pickle.load(f)\n",
    "\n",
    "with open(f'data/selected_sentences/neg.pkl', 'rb') as f:\n",
    "    neg_sentences_raw = pickle.load(f)\n",
    "    \n",
    "    \n",
    "pos_sentences = pos_sentences_raw + neg_sentences_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In fact, organizations are now identifying digital skills or computer literacy as one of their core values for employability (such as the US Department of Education, the US Department of commerce, the OECD Program for the International Assessment of Adult Competencies and the European Commission).',\n",
       " 'International studies on student achievement, such as Trends in International Mathematics and Science Study (TIMMS) and the Programme for International Student Assessment (PISA) from past several years have documented a narrowing gap in gender differences in science and mathematics achievement (Else-Quest, Hyde, & Linn, 2010; Martin, Mullis, Foy, & Hooper, 2016; OECD, 2016) .',\n",
       " '1 manages access to results of the Agricultural Resources Management Survey (ARMS), a fundamental source of information on agricultural practices, farm businesses and farm household financials.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_sentences[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def process_neg_sentence(sentence):\\n    global n_broken_sent\\n    \\n    bert_sentence = text_cleaning_upper(sentence)\\n    label_sentence = bert_sentence.lower()\\n\\n    if is_text_broken(label_sentence.split(' ')): # Can't use bert cleaning for this, because all punc.s are padded with spaces\\n        n_broken_sent += 1\\n        return\\n\\n    bert_tokens = bert_sentence.split(' ')\\n    \\n    ### STEP 1: Split into fixed sized sentences ###\\n    for small_sentence_tokens in split_to_smaller_sent(bert_tokens, s_size = 64, overlap_size = 20):\\n        small_sent_targets = ['O' for _ in range(len(bert_tokens))]\\n\\n        neg_sentences_processed.append([convert_tokens(t) for t in small_sentence_tokens])\\n        neg_labels.append(small_sent_targets)\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_sentences_processed = []\n",
    "neg_sentences_processed = []\n",
    "pos_labels = []\n",
    "neg_labels = []\n",
    "\n",
    "n_broken_sent = 0\n",
    "n_pos_no_label = 0\n",
    "\n",
    "def text_cleaning_for_bert(text):\n",
    "    # Keeps puncs, pads them with whitespaces\n",
    "    text = text.replace('^', ' ')\n",
    "    text = unidecode.unidecode(text)\n",
    "    \n",
    "    text = re.sub(r'\\[[0-9]+]', ' SpecialReference ', text)\n",
    "    \n",
    "    # Remove years\n",
    "    text = re.sub(r'(19|20)[0-9][0-9]', ' SpecialYear ', text)\n",
    "    \n",
    "    # remove other digits\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "    \n",
    "    # Remove websites\n",
    "    text = ' '.join(['SpecialWebsite' if 'http' in t or 'www' in t else t for t in text.split(' ') ])\n",
    "\n",
    "    text = match_puncs_re.sub(r' \\1 ', text)\n",
    "\n",
    "    # remove extra spaces\n",
    "    text = re.sub(\"\\s+\",\" \", text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "def convert_tokens(text):\n",
    "    if is_acronym(text):\n",
    "        return 'ACRONYM'\n",
    "    return text\n",
    "\n",
    "def is_acronym(text):\n",
    "    if len(text) < 3:\n",
    "        return False\n",
    "    if text.isupper():\n",
    "        return True\n",
    "\n",
    "def is_text_broken(tokens):\n",
    "    # Some texts are like 'p a dsdv a d a ds f b', remove them\n",
    "    if len(tokens) == 0:\n",
    "        return True\n",
    "\n",
    "    if len(tokens) < 50:\n",
    "        return False\n",
    "\n",
    "    one_char_token_ratio = len([l for l in tokens if len(l) == 1]) / len(tokens)\n",
    "    return one_char_token_ratio > 0.2\n",
    "\n",
    "def split_to_smaller_sent(tokens, s_size, overlap_size):\n",
    "    # output sentences will be s_size + overlap_size long\n",
    "    small_sents = []\n",
    "\n",
    "    if len(tokens) <= s_size:\n",
    "        return [tokens]\n",
    "\n",
    "    n_parts = len(tokens) // s_size\n",
    "    if len(tokens) % s_size != 0:\n",
    "        n_parts += 1\n",
    "\n",
    "    i_part = 0\n",
    "    end_i = 0\n",
    "    while end_i < len(tokens):\n",
    "        start_i = i_part * s_size\n",
    "        if i_part > 0:\n",
    "            start_i -= overlap_size\n",
    "\n",
    "        end_i = min(len(tokens), start_i + s_size)\n",
    "\n",
    "        small_sents.append(tokens[start_i: end_i])\n",
    "        i_part += 1\n",
    "\n",
    "    return small_sents\n",
    "\n",
    "def join_tuple_tokens(tuples):\n",
    "    return ' '.join([t[1] for t in tuples])\n",
    "\n",
    "def get_index(lst, el):\n",
    "    idx = []\n",
    "    for i, lst_el in enumerate(lst):\n",
    "        if el in lst_el:\n",
    "            idx.append(i)\n",
    "\n",
    "    return idx\n",
    "\n",
    "def get_connected_uppercase(tokens):\n",
    "    # Acronyms should not be a part of connected uppercase texts\n",
    "    tokens = [t if not is_acronym(t) else '*****' for t in tokens ]\n",
    "    if len(tokens) == 0:\n",
    "        return []\n",
    "    \n",
    "    groups = []\n",
    "    this_group_tokens = []\n",
    "    in_group = False\n",
    "    last_token_connection = False\n",
    "\n",
    "    group_len = 0\n",
    "    n_long_tokens = 0\n",
    "    for token in tokens:\n",
    "        token_lower = token.lower()\n",
    "        if token[0].isupper():\n",
    "            in_group = True\n",
    "            \n",
    "            if token_lower not in connection_tokens:\n",
    "                if len(token) > 2:\n",
    "                    n_long_tokens += 1\n",
    "                group_len += 1\n",
    "                last_token_connection = False\n",
    "                this_group_tokens.append(token)\n",
    "            else:\n",
    "                last_token_connection = True\n",
    "                \n",
    "                # Prevent connection tokens to be the first\n",
    "                if group_len > 0:\n",
    "                    this_group_tokens.append(token)\n",
    "                \n",
    "        else:\n",
    "            if token_lower not in connection_tokens:\n",
    "                if in_group:\n",
    "                    if group_len > 2 and n_long_tokens > 0:\n",
    "                        if last_token_connection:\n",
    "                            this_group_tokens = this_group_tokens[:-1]\n",
    "                        groups.append(this_group_tokens)\n",
    "                    this_group_tokens = []\n",
    "                \n",
    "                last_token_connection = False\n",
    "                group_len = 0\n",
    "                n_long_tokens = 0\n",
    "                in_group = False\n",
    "                \n",
    "            elif in_group:\n",
    "                last_token_connection = True\n",
    "                this_group_tokens.append(token)\n",
    "                \n",
    "                \n",
    "    if in_group:\n",
    "        if group_len > 2 and n_long_tokens > 0:\n",
    "            if last_token_connection:\n",
    "                this_group_tokens = this_group_tokens[:-1]\n",
    "            groups.append(this_group_tokens)\n",
    "    \n",
    "    return groups\n",
    "\n",
    "def index_list_in_list(lst, search_lst):\n",
    "    for i_start in range(len(lst) - len(search_lst) + 1):\n",
    "        i_end = i_start + len(search_lst)\n",
    "        if lst[i_start:i_end] == search_lst:\n",
    "            return i_start\n",
    "        \n",
    "    raise ValueError(f'{lst} not found in {search_lst[:50]}')\n",
    "\n",
    "def convert_sentence(tokens, labels):\n",
    "    connected_uppercase = get_connected_uppercase(tokens)\n",
    "    tokens = [t if not is_acronym(t) else 'ACRONYM' for t in tokens]\n",
    "    \n",
    "    \"\"\"for conn_tokens in connected_uppercase:\n",
    "        i_start = index_list_in_list(tokens, conn_tokens)\n",
    "        i_end = i_start + len(conn_tokens)\n",
    "        \n",
    "        tokens = tokens[:i_start] + ['UPPERCASEENTITY'] + tokens[i_end:]\n",
    "        labels = labels[:i_start] + ['B'] + labels[i_end:]\"\"\"\n",
    "        \n",
    "    return tokens, labels\n",
    "\n",
    "def process_pos_sentence(sentence):\n",
    "    global n_broken_sent\n",
    "    global last_doc_labels\n",
    "\n",
    "    bert_sentence = text_cleaning_for_bert(sentence)\n",
    "    label_sentence = text_cleaning_upper(bert_sentence).lower()\n",
    "\n",
    "    if is_text_broken(label_sentence.split(' ')): # Can't use bert cleaning for this, because all punc.s are padded with spaces\n",
    "        n_broken_sent += 1\n",
    "        return\n",
    "    \n",
    "    bert_tokens = bert_sentence.split(' ')\n",
    "    ### STEP 1: Split into fixed sized sentences ###\n",
    "    for small_sentence_tokens in split_to_smaller_sent(bert_tokens, s_size = 64, overlap_size = 20):\n",
    "\n",
    "        small_bert_sentence = ' '.join(small_sentence_tokens)\n",
    "\n",
    "        # Need to remove punc.s and uppercase letters to find labels\n",
    "        small_label_sentence = text_cleaning(small_bert_sentence)\n",
    "\n",
    "        has_label = False\n",
    "        sent_labels = []\n",
    "        ### STEP 2: Match labels ###\n",
    "        # Check if contains labels\n",
    "        for clean_label in existing_labels:\n",
    "            if re.search(r'\\b{}\\b'.format(clean_label), small_label_sentence):\n",
    "                has_label = True\n",
    "\n",
    "                # Remove label from the text, to only match the largest label\n",
    "                small_label_sentence = small_label_sentence.replace(clean_label, '')\n",
    "                sent_labels.append(clean_label)\n",
    "\n",
    "        small_sent_targets = ['O' for _ in range(len(small_sentence_tokens))]\n",
    "\n",
    "        if has_label:\n",
    "            # Tokenize labels for matching\n",
    "            sent_label_tokens = [l.split(' ') for l in sent_labels]\n",
    "\n",
    "            # Get index, token tuples for clean tokens. Indices are for raw tokens\n",
    "            small_sent_tuples = [(i, token.lower()) for i, token in enumerate(small_sentence_tokens) if text_cleaning_upper(token) != '']\n",
    "\n",
    "            ### STEP 3: Set corresponding targets for each label ###\n",
    "            # Target: (B, I, O), Label: adni\n",
    "            for l in sent_labels:\n",
    "                l_tokens = l.split(' ')\n",
    "                small_sent_joined = [join_tuple_tokens(small_sent_tuples[i: i + len(l_tokens)]) for i in range(len(small_sent_tuples) - len(l_tokens) + 1)]\n",
    "\n",
    "                label_start_idx = get_index(small_sent_joined, l) # list of indices\n",
    "                for label_start_i in label_start_idx:\n",
    "                    label_end_i = label_start_i + len(l_tokens) - 1\n",
    "\n",
    "                    target_start_i = small_sent_tuples[label_start_i][0]\n",
    "                    target_end_i = small_sent_tuples[label_end_i][0]\n",
    "\n",
    "                    # Do not use the same tokens for multiple labels\n",
    "                    #small_sent_tuples = small_sent_tuples[:label_start_i] + small_sent_tuples[label_end_i:]\n",
    "\n",
    "                    try:\n",
    "                        if small_sent_targets[target_start_i] == 'O': # If not was already labeled\n",
    "                            small_sent_targets[target_start_i] = 'B'\n",
    "                            if target_end_i - target_start_i > 0:\n",
    "                                for i in range(target_start_i+1, target_end_i+1):\n",
    "                                    small_sent_targets[i] = 'I'\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print('DEBUG')\n",
    "                        print(small_sentence_tokens)\n",
    "                        print(len(small_sentence_tokens))\n",
    "                        print(len(small_sent_targets))\n",
    "                        print(target_start_i)\n",
    "                        print(small_sent_joined)\n",
    "                        print('DEBUG')\n",
    "                        raise e\n",
    "        \n",
    "        ### STEP 4: Add sentence output to lists ###\n",
    "        small_sentence_tokens, small_sent_targets = convert_sentence(small_sentence_tokens, small_sent_targets)\n",
    "        if has_label:\n",
    "            pos_sentences_processed.append(small_sentence_tokens)\n",
    "            pos_labels.append(small_sent_targets)\n",
    "        else:\n",
    "            neg_sentences_processed.append(small_sentence_tokens)\n",
    "            neg_labels.append(small_sent_targets)\n",
    "\n",
    "\"\"\"def process_neg_sentence(sentence):\n",
    "    global n_broken_sent\n",
    "    \n",
    "    bert_sentence = text_cleaning_upper(sentence)\n",
    "    label_sentence = bert_sentence.lower()\n",
    "\n",
    "    if is_text_broken(label_sentence.split(' ')): # Can't use bert cleaning for this, because all punc.s are padded with spaces\n",
    "        n_broken_sent += 1\n",
    "        return\n",
    "\n",
    "    bert_tokens = bert_sentence.split(' ')\n",
    "    \n",
    "    ### STEP 1: Split into fixed sized sentences ###\n",
    "    for small_sentence_tokens in split_to_smaller_sent(bert_tokens, s_size = 64, overlap_size = 20):\n",
    "        small_sent_targets = ['O' for _ in range(len(bert_tokens))]\n",
    "\n",
    "        neg_sentences_processed.append([convert_tokens(t) for t in small_sentence_tokens])\n",
    "        neg_labels.append(small_sent_targets)\"\"\"\n",
    "\n",
    "#process_pos_sentence(pos_sentences[2472])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'During the biomass study , the tide was coming in moving from a low tide at : GMT with a water level of . m to high tide at : GMT with a water level of . m ( NOAA tide gauge Atlantic City , NJ ) .'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_cleaning_for_bert(pos_sentences[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create NER Dataset and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 971668/971668 [03:01<00:00, 5354.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "broken sentences: 4032\n",
      "n_pos_no_label: 0\n",
      "pos_proc size: 264346\n",
      "neg_proc size: 821066\n"
     ]
    }
   ],
   "source": [
    "assert len(pos_sentences) > 0\n",
    "\n",
    "pos_sentences_processed = []\n",
    "neg_sentences_processed = []\n",
    "pos_labels = []\n",
    "neg_labels = []\n",
    "\n",
    "n_pos_no_label = 0\n",
    "n_broken_sent = 0\n",
    "\n",
    "for sent in tqdm(pos_sentences):\n",
    "    process_pos_sentence(sent)\n",
    "\n",
    "\"\"\"for sent in tqdm(neg_sentences):\n",
    "        process_neg_sentence(sent)\"\"\"\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(f'data/bert_ner_processed/pos.pkl', 'wb') as f:\n",
    "    pickle.dump(pos_sentences_processed, f)\n",
    "\n",
    "with open(f'data/bert_ner_processed/neg.pkl', 'wb') as f:\n",
    "    pickle.dump(neg_sentences_processed, f)\n",
    "\n",
    "with open(f'data/bert_ner_processed/pos_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(pos_labels, f)\n",
    "\n",
    "with open(f'data/bert_ner_processed/neg_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(neg_labels, f)\n",
    "\n",
    "\n",
    "print('')\n",
    "print(f'broken sentences: {n_broken_sent}')\n",
    "print(f'n_pos_no_label: {n_pos_no_label}')\n",
    "print(f'pos_proc size: {len(pos_sentences_processed)}')\n",
    "print(f'neg_proc size: {len(neg_sentences_processed)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'fact',\n",
       " ',',\n",
       " 'organizations',\n",
       " 'are',\n",
       " 'now',\n",
       " 'identifying',\n",
       " 'digital',\n",
       " 'skills',\n",
       " 'or',\n",
       " 'computer',\n",
       " 'literacy',\n",
       " 'as',\n",
       " 'one',\n",
       " 'of',\n",
       " 'their',\n",
       " 'core',\n",
       " 'values',\n",
       " 'for',\n",
       " 'employability',\n",
       " '(',\n",
       " 'such',\n",
       " 'as',\n",
       " 'the',\n",
       " 'US',\n",
       " 'Department',\n",
       " 'of',\n",
       " 'Education',\n",
       " ',',\n",
       " 'the',\n",
       " 'US',\n",
       " 'Department',\n",
       " 'of',\n",
       " 'commerce',\n",
       " ',',\n",
       " 'the',\n",
       " 'ACRONYM',\n",
       " 'Program',\n",
       " 'for',\n",
       " 'the',\n",
       " 'International',\n",
       " 'Assessment',\n",
       " 'of',\n",
       " 'Adult',\n",
       " 'Competencies',\n",
       " 'and',\n",
       " 'the',\n",
       " 'European',\n",
       " 'Commission',\n",
       " ')',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_sentences_processed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fact</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>organizations</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>are</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>now</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>identifying</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>digital</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>skills</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>or</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>computer</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>literacy</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>as</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>one</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>their</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>core</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>values</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>for</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>employability</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>(</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>such</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>as</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>US</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Department</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Education</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>US</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Department</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>commerce</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>ACRONYM</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Program</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>for</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>the</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>International</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Assessment</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>of</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Adult</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Competencies</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>and</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>European</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Commission</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>)</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            token label\n",
       "0              In     O\n",
       "1            fact     O\n",
       "2               ,     O\n",
       "3   organizations     O\n",
       "4             are     O\n",
       "5             now     O\n",
       "6     identifying     O\n",
       "7         digital     O\n",
       "8          skills     O\n",
       "9              or     O\n",
       "10       computer     O\n",
       "11       literacy     O\n",
       "12             as     O\n",
       "13            one     O\n",
       "14             of     O\n",
       "15          their     O\n",
       "16           core     O\n",
       "17         values     O\n",
       "18            for     O\n",
       "19  employability     O\n",
       "20              (     O\n",
       "21           such     O\n",
       "22             as     O\n",
       "23            the     O\n",
       "24             US     O\n",
       "25     Department     O\n",
       "26             of     O\n",
       "27      Education     O\n",
       "28              ,     O\n",
       "29            the     O\n",
       "30             US     O\n",
       "31     Department     O\n",
       "32             of     O\n",
       "33       commerce     O\n",
       "34              ,     O\n",
       "35            the     O\n",
       "36        ACRONYM     O\n",
       "37        Program     B\n",
       "38            for     I\n",
       "39            the     I\n",
       "40  International     I\n",
       "41     Assessment     I\n",
       "42             of     I\n",
       "43          Adult     I\n",
       "44   Competencies     I\n",
       "45            and     O\n",
       "46            the     O\n",
       "47       European     O\n",
       "48     Commission     O\n",
       "49              )     O\n",
       "50              .     O"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_ex = 0\n",
    "pd.DataFrame({'token': pos_sentences_processed[i_ex], 'label': pos_labels[i_ex]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load NER Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos size: 264346\n",
      "neg size: 821066\n",
      "pos label size: 264346\n",
      "neg label size: 821066\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f'data/bert_ner_processed/pos.pkl', 'rb') as f:\n",
    "    pos_sentences_processed = pickle.load(f)\n",
    "\n",
    "with open(f'data/bert_ner_processed/neg.pkl', 'rb') as f:\n",
    "    neg_sentences_processed = pickle.load(f)\n",
    "\n",
    "with open(f'data/bert_ner_processed/pos_labels.pkl', 'rb') as f:\n",
    "    pos_labels = pickle.load(f)\n",
    "\n",
    "with open(f'data/bert_ner_processed/neg_labels.pkl', 'rb') as f:\n",
    "    neg_labels = pickle.load(f)\n",
    "\n",
    "print(f'pos size: {len(pos_sentences_processed)}')\n",
    "print(f'neg size: {len(neg_sentences_processed)}')\n",
    "print(f'pos label size: {len(pos_labels)}')\n",
    "print(f'neg label size: {len(neg_labels)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f'data/gov_data_selected.pkl', 'rb') as f:\n",
    "    unique_gov_names = pickle.load(f)\n",
    "    \n",
    "unique_gov_names += existing_labels\n",
    "\n",
    "def replace_target(x, lst):\n",
    "    if x['label'].iloc[0] == 'O':\n",
    "        # if not a dataset name, do not augment\n",
    "        lst.append(x)\n",
    "    else:\n",
    "        random_name = random.choice(unique_gov_names)\n",
    "        random_name_tokens = random_name.split(' ')\n",
    "        if random_name.islower():\n",
    "            if len(random_name_tokens) > 1:\n",
    "                random_name_tokens = [t for t in random_name_tokens if len(t) > 0]\n",
    "                random_name_tokens = [r[0].upper() + r[1:] if not r.lower() in connection_tokens else r for r in random_name_tokens]\n",
    "            else:\n",
    "                random_name_tokens = ['ACRONYM']\n",
    "        else:\n",
    "            random_name_tokens = [t if not is_acronym(t) else 'ACRONYM' for t in random_name_tokens]\n",
    "\n",
    "        new_x = pd.DataFrame()\n",
    "        # Replace tokens\n",
    "        new_x['token'] = random_name_tokens\n",
    "        new_x['label'] = 'I'\n",
    "        new_x.loc[new_x.index == 0, 'label'] = 'B'\n",
    "        lst.append(new_x)\n",
    "\n",
    "def augment_sentence(tokens, labels, augment_chance = 0.8):\n",
    "    if random.uniform(0,1) > augment_chance:\n",
    "        # No augmentation\n",
    "        return tokens, labels\n",
    "\n",
    "    df_pieces = []\n",
    "    sent_df = pd.DataFrame({'token': tokens, 'label': labels})\n",
    "    sent_df['label_o'] = sent_df.label == 'O'\n",
    "\n",
    "    gb = sent_df.groupby((sent_df['label_o'].shift() != sent_df['label_o']).cumsum())\n",
    "    for name, group in gb:\n",
    "        replace_target(group, df_pieces)\n",
    "\n",
    "    sent_df = pd.concat(df_pieces, ignore_index = True, axis = 0)\n",
    "\n",
    "    return list(sent_df.token.values), list(sent_df.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 264346/264346 [16:20<00:00, 269.68it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 264346/264346 [14:57<00:00, 294.57it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 264346/264346 [14:39<00:00, 300.47it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 264346/264346 [14:37<00:00, 301.17it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 264346/264346 [14:36<00:00, 301.62it/s]\n"
     ]
    }
   ],
   "source": [
    "pos_sentences_processed_aug = []\n",
    "pos_labels_aug = []\n",
    "\n",
    "for _ in range(5):\n",
    "    for s_tokens, s_labels in tqdm(zip(pos_sentences_processed, pos_labels), total = len(pos_labels)):\n",
    "        aug_tokens, aug_labels = augment_sentence(s_tokens, s_labels)\n",
    "        pos_sentences_processed_aug.append(aug_tokens)\n",
    "        pos_labels_aug.append(aug_labels)\n",
    "\n",
    "pos_sentences_processed = pos_sentences_processed_aug\n",
    "pos_labels = pos_labels_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Plant</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wall</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Degradative</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Compounds</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>-</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>year</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>publicprivate</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>partnership</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            token label\n",
       "0             The     O\n",
       "1           Plant     B\n",
       "2            Wall     I\n",
       "3     Degradative     I\n",
       "4       Compounds     I\n",
       "..            ...   ...\n",
       "56              -     O\n",
       "57           year     O\n",
       "58  publicprivate     O\n",
       "59    partnership     O\n",
       "60              .     O\n",
       "\n",
       "[61 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_ex = 1001\n",
    "pd.DataFrame({'token': pos_sentences_processed[i_ex], 'label': pos_labels[i_ex]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "neg_size = 500000\n",
    "neg_idx = np.random.permutation(len(neg_labels))\n",
    "neg_sentences_processed = [neg_sentences_processed[i] for i in neg_idx[:neg_size]]\n",
    "neg_labels = [neg_labels[i] for i in neg_idx[:neg_size]]\n",
    "\n",
    "sentences = pos_sentences_processed + neg_sentences_processed\n",
    "labels = pos_labels + neg_labels\n",
    "\n",
    "idx = np.random.permutation(len(sentences))\n",
    "sentences = [sentences[i] for i in idx]\n",
    "labels = [labels[i] for i in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'data/bert_ner_data/train_sentences.pkl', 'wb') as f:\n",
    "    pickle.dump(sentences, f)\n",
    "\n",
    "with open(f'data/bert_ner_data/train_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(labels, f)\n",
    "    \n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1821730\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f'data/bert_ner_data/train_sentences.pkl', 'rb') as f:\n",
    "    sentences = pickle.load(f)\n",
    "\n",
    "with open(f'data/bert_ner_data/train_labels.pkl', 'rb') as f:\n",
    "    labels = pickle.load(f)\n",
    "    \n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sentences[:1500000]\n",
    "labels = labels[:1500000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tune Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import csv\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "import statistics as stats\n",
    "from bert_sklearn import BertTokenClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building sklearn token classifier...\n"
     ]
    }
   ],
   "source": [
    "model = BertTokenClassifier(\n",
    "    bert_model='scibert-scivocab-cased',\n",
    "    num_mlp_hiddens= 500,\n",
    "    max_seq_length=64, \n",
    "    epochs=1,\n",
    "    #gradient accumulation\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=3e-5,\n",
    "    train_batch_size=8,#batch size for training\n",
    "    eval_batch_size=8, #batch size for evaluation\n",
    "    validation_fraction=0.0, \n",
    "    #ignore the tokens with label ‘O’\n",
    "    ignore_label=['O']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ozano\\.conda\\envs\\torch\\lib\\site-packages\\bert_sklearn\\utils.py:26: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(X)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading scibert-scivocab-cased model...\n",
      "Defaulting to linear classifier/regressor\n",
      "Loading Pytorch checkpoint\n",
      "train data size: 1500000, validation data size: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training  :   0%|                                                  | 3/750000 [02:41<8699:34:03, 41.76s/it, loss=0.436]C:\\Users\\ozano\\.conda\\envs\\torch\\lib\\site-packages\\bert_sklearn\\model\\pytorch_pretrained\\optimization.py:275: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:1005.)\n",
      "  next_m.mul_(beta1).add_(1 - beta1, grad)\n",
      "Training  :  34%|██████████████▊                             | 253202/750000 [7:34:48<16:37:32,  8.30it/s, loss=0.0286]"
     ]
    }
   ],
   "source": [
    "model.fit(sentences, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to disk\n",
    "savefile='data/sklearn_bert_ner_cased_all_sents.bin'\n",
    "model.save(savefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "aac00fb1da9c94ab374af15aaaf3bdbaafa792d2239349bc70bec9f747decd69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
