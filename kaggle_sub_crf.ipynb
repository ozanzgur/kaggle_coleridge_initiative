{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python378jvsc74a57bd001de0ae40aa8a02f5dce4eb302558c2ba1e499509f0a7c12e0772621a5681a06",
   "display_name": "Python 3.7.8 64-bit ('env': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "test_example_names = [fn.split('.')[0] for fn in os.listdir('data/test')]"
   ]
  },
  {
   "source": [
    "### Utils"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_example_by_name(name):\n",
    "    doc_path = os.path.join('data/test', name + '.json')\n",
    "    with open(doc_path) as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def delete_file(filename):\n",
    "    if os.path.exists(filename):\n",
    "        os.remove(filename)"
   ]
  },
  {
   "source": [
    "### Feature Extraction Class"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "_RE_COMBINE_WHITESPACE = re.compile(r\"\\s+\")\n",
    "\n",
    "def get_features(s):\n",
    "    return [(w, t) for w, t in zip(\\\n",
    "            s['TOKEN'].values.tolist(),\n",
    "            s['TARGET'].values.tolist()                          \n",
    "        )]\n",
    "\n",
    "import string\n",
    "puncs = [c for c in string.punctuation]\n",
    "\n",
    "def mask_numbers(text):\n",
    "        # Replace each numeric char with '#'\n",
    "        \n",
    "        def repl(m):\n",
    "            return f\" {'#' * len(m.group())} \"\n",
    "        text = re.sub(r'[0-9]+', repl, text)\n",
    "        return text\n",
    "\n",
    "def make_single_whitespace(text):\n",
    "    return _RE_COMBINE_WHITESPACE.sub(\" \", text).strip()\n",
    "\n",
    "class TextFeatureExtractor:\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        # Initialize super\n",
    "        \n",
    "        # Load parameters\n",
    "        def_args = dict()\n",
    "        \n",
    "        # Extract related arguments\n",
    "        for k, def_val in def_args.items():\n",
    "            self.__dict__.update({k: kwargs.get(k, def_val)})\n",
    "\n",
    "    def transform(self, x):\n",
    "        x.update({'output': [_df2features(x['output'])]})\n",
    "        return x\n",
    "\n",
    "    def _mask_numbers(self, text):\n",
    "        # Replace each numeric char with #\n",
    "        \n",
    "        def repl(m):\n",
    "            return f\" {'#' * len(m.group())} \"\n",
    "        text = re.sub(r'[0-9]+', repl, text)\n",
    "        return text\n",
    "\n",
    "    def fit_transform(self, data, train_filenames, val_filenames):\n",
    "        self.train_filenames = train_filenames\n",
    "        self.val_filenames = val_filenames\n",
    "\n",
    "        \n",
    "        output = {}\n",
    "\n",
    "        # Process each set\n",
    "        for setname in ['train', 'val']:\n",
    "            docs = []\n",
    "            for f in tqdm(self.__dict__.get(f'{setname}_filenames')):\n",
    "                df_slice = data[f]\n",
    "\n",
    "                assert not df_slice['TOKEN'].isnull().any(), 'All tokens must have a value'\n",
    "                df_slice['TARGET'] = df_slice['TARGET'].fillna('OTHER')\n",
    "                df_slice['TOKEN'] = df_slice['TOKEN'].values.astype('U')\n",
    "                df_slice['TOKEN'] = df_slice['TOKEN'].apply(mask_numbers)\n",
    "                df_slice['TOKEN'] = df_slice['TOKEN'].apply(make_single_whitespace)\n",
    "\n",
    "                data_slice = get_features(df_slice)\n",
    "                docs.append(data_slice)\n",
    "            \n",
    "            X = [_doc2features(s) for s in tqdm(docs)]\n",
    "            y = [_doc2labels(s) for s in tqdm(docs)]\n",
    "\n",
    "            del docs\n",
    "            \n",
    "            assert(len(X) == len(y))\n",
    "            \n",
    "            output[f'{setname}_data'] = (X, y)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class DocGetter(object):\n",
    "    def __init__(self, data):\n",
    "        self.n_doc = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, t) for w, t in zip(\\\n",
    "            s['TOKEN'].values.tolist(), \n",
    "            s['TARGET'].values.tolist()\n",
    "                                                                     \n",
    "        )]\n",
    "        self.grouped = self.data.groupby('FILENAME').apply(agg_func)\n",
    "        self.docs = [s for s in self.grouped]\n",
    "\n",
    "def _text2features(text):\n",
    "    \"\"\"Returns a list of examples.\n",
    "    \"\"\"\n",
    "    #words = wordpunct_tokenize(text)\n",
    "    return [_word2features(words, i) for i in range(len(words))]\n",
    "\n",
    "def _df2features(df):\n",
    "    \"\"\"Returns a list of examples.\n",
    "    \"\"\"\n",
    "    for col in ['TOKEN']:\n",
    "        assert(col in df.columns)\n",
    "    \n",
    "    df['TOKEN'] = df['TOKEN'].values.astype('U')\n",
    "    df['TOKEN'] = df['TOKEN'].apply(mask_numbers)\n",
    "    df['TOKEN'] = df['TOKEN'].apply(make_single_whitespace)\n",
    "    \n",
    "    agg_func = lambda s: [(w,) for w in s['TOKEN'].values.tolist()]\n",
    "    feature_list = agg_func(df)\n",
    "    words = [s for s in feature_list]\n",
    "    \n",
    "    return _doc2features(words)\n",
    "\n",
    "def _word2features(words, i):\n",
    "    word = words[i]\n",
    "    if not isinstance(word, str):\n",
    "        word = word[0]\n",
    "    \n",
    "    # isfirstname = fe_isname.is_firstname(word)\n",
    "    # islastname = fe_isname.is_lastname(word)\n",
    "    # isname = isfirstname or islastname\n",
    "    \n",
    "    # if isfirstname and islastname:\n",
    "    #     word = '#NAME#'\n",
    "    # elif isfirstname:\n",
    "    #     word = '#FIRSTNAME#'\n",
    "    # elif islastname:\n",
    "    #     word = '#LASTNAME#'\n",
    "\n",
    "    digit_count = sum(c=='#' for c in word)\n",
    "    length = len(word)\n",
    "    assert length > 0, \"All tokens must have length > 0\"\n",
    "\n",
    "    features = {\n",
    "        #'bias': 1.0,\n",
    "        #'word.index': i\n",
    "    }\n",
    "    \n",
    "    # Add line & word indices\n",
    "    # features.update({\n",
    "    #     'word.lineindex': li,\n",
    "    #     'word.wordindex': wi,\n",
    "        \n",
    "    # })\n",
    "    \n",
    "    # if isname:\n",
    "    #     features.update({\n",
    "    #     'word.lower()': word\n",
    "        \n",
    "    # })\n",
    "    #else:\n",
    "    # If all digits\n",
    "    if word.isdigit():\n",
    "        features.update({\n",
    "            'isd': True,\n",
    "            'dct': digit_count,\n",
    "            '4dg': digit_count == 4,\n",
    "            'dgr': 1.0,\n",
    "            'len': length\n",
    "        })\n",
    "    else: # Not all digit\n",
    "        features.update({\n",
    "            'isd': False,\n",
    "            'dgr': digit_count / length,\n",
    "            'dct': digit_count,\n",
    "            'len': length,\n",
    "            'wor': word\n",
    "        })#'pnc': np.mean(np.array([c in puncs for c in word]))\n",
    "\n",
    "    if i > 0:\n",
    "        word_other = words[i-1][0]\n",
    "        features.update({\n",
    "            '-1': word_other,\n",
    "            #'-1:word.isupper()': word1.isupper()\n",
    "        })\n",
    "        if i > 1:\n",
    "            word_other = words[i-2][0]\n",
    "            features.update({\n",
    "                '-2': word_other,\n",
    "                #'-2:word.isupper()': word2.isupper()\n",
    "            })\n",
    "            if i > 2:\n",
    "                word_other = words[i-3][0]\n",
    "                features.update({\n",
    "                    '-3': word_other,\n",
    "                    #'-3:word.isupper()': word_other.isupper()\n",
    "                })\n",
    "                if i > 3:\n",
    "                    word_other = words[i-4][0]\n",
    "                    features.update({\n",
    "                        '-4': word_other,\n",
    "                        #'-4:word.isupper()': word_other.isupper()\n",
    "                    })\n",
    "                    if i > 4:\n",
    "                        word_other = words[i-5][0]\n",
    "                        features.update({\n",
    "                            '-5': word_other\n",
    "                        })\n",
    "                        if i > 5:\n",
    "                            word_other = words[i-6][0]\n",
    "                            features.update({\n",
    "                                '-6': word_other\n",
    "                            })\n",
    "                            \n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "    if i < len(words)-1:\n",
    "        word_other = words[i+1][0]\n",
    "        features.update({\n",
    "            '+1':  word_other,\n",
    "            #'+1:word.isupper()': word1.isupper()\n",
    "        })\n",
    "        if i < len(words)-2:\n",
    "            word_other = words[i+2][0]\n",
    "            features.update({\n",
    "                '+2':  word_other,\n",
    "                #'+2:word.isupper()': word2.isupper()\n",
    "            })\n",
    "            if i < len(words)-3:\n",
    "                word_other = words[i+3][0]\n",
    "                features.update({\n",
    "                    '+3':  word_other,\n",
    "                    #'+3:word.isupper()': word_other.isupper()\n",
    "                })\n",
    "                \n",
    "                if i < len(words)-4:\n",
    "                    word_other = words[i+4][0]\n",
    "                    features.update({\n",
    "                        '+4':  word_other,\n",
    "                        #'+4:word.isupper()': word_other.isupper()\n",
    "                    })\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "    return features\n",
    "\n",
    "def _doc2features(doc):\n",
    "    \"\"\"Returns a list of examples.\n",
    "    \"\"\"\n",
    "    words = [(ex[0],) for ex in doc]\n",
    "    return [_word2features(words, i) for i in range(len(words))]\n",
    "\n",
    "def _doc2labels(doc):\n",
    "    return [s[-1] for s in doc]\n",
    "def _doc2tokens(doc):\n",
    "    return [s[0] for s in doc]"
   ]
  },
  {
   "source": [
    "### Preprocessing Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tokenize_doc(doc_json):\n",
    "    doc_text = ' '.join([clean_text(sec['text']) for sec in doc])\n",
    "    doc_text = make_single_whitespace(doc_text)\n",
    "\n",
    "    doc_tokens = doc_text.split(' ')\n",
    "    return doc_tokens\n",
    "\n",
    "def clean_text(txt):\n",
    "    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower())"
   ]
  },
  {
   "source": [
    "### Load Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "model = joblib.load('pipeline_model.joblib')"
   ]
  },
  {
   "source": [
    "### Create Submission"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = TextFeatureExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = []\n",
    "ids = []\n",
    "for test_id in test_example_names:\n",
    "    \n",
    "    # Load and preprocess\n",
    "    doc = load_test_example_by_name(test_id)\n",
    "    doc_tokens = preprocess_tokenize_doc(doc)\n",
    "\n",
    "    # Extract features\n",
    "    x = {'output': pd.DataFrame({'TOKEN': doc_tokens})}\n",
    "    x = feature_extractor.transform(x)\n",
    "\n",
    "    # Predict\n",
    "    pred = model.predict(x['output'])\n",
    "    pred = pred[0]\n",
    "    pred = np.array([int(p) for p in pred])\n",
    "\n",
    "    # Get corresponding tokens\n",
    "    pos_pred_idx = [i[0] for i in np.argwhere(pred == 1)]\n",
    "    pred_tokens = [doc_tokens[i] for i in pos_pred_idx]\n",
    "\n",
    "    pred_joined = ' '.join(pred_tokens)\n",
    "\n",
    "    test_preds.append(pred_joined)\n",
    "    ids.append(test_id)\n",
    "\n",
    "sub_df = pd.DataFrame(columns = ['Id', 'PredictionString'])\n",
    "sub_df['Id'] = ids\n",
    "sub_df['PredictionString'] = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                     Id  \\\n",
       "0  2100032a-7c33-4bff-97ef-690822c43466   \n",
       "1  2f392438-e215-4169-bebf-21ac4ff253e1   \n",
       "2  3f316b38-1a24-45a9-8d8c-4e05a42257c6   \n",
       "3  8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60   \n",
       "\n",
       "                                    PredictionString  \n",
       "0  the alzheimer s disease neuroimaging initiativ...  \n",
       "1  the trends in international mathematics and sc...  \n",
       "2            noaa storm surge inundation slosh model  \n",
       "3                        rural urban continuum codes  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>PredictionString</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2100032a-7c33-4bff-97ef-690822c43466</td>\n      <td>the alzheimer s disease neuroimaging initiativ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2f392438-e215-4169-bebf-21ac4ff253e1</td>\n      <td>the trends in international mathematics and sc...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3f316b38-1a24-45a9-8d8c-4e05a42257c6</td>\n      <td>noaa storm surge inundation slosh model</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60</td>\n      <td>rural urban continuum codes</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "pd.options.display.max_rows = 25\n",
    "sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}