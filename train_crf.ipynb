{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python378jvsc74a57bd001de0ae40aa8a02f5dce4eb302558c2ba1e499509f0a7c12e0772621a5681a06",
   "display_name": "Python 3.7.8 64-bit ('env': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "TODO:\n",
    "- Adjust fuzz ratio to catch all labels"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "train_example_paths = glob.glob('data/train/*.json')\n",
    "test_example_paths = glob.glob('data/test/*.json')\n",
    "\n",
    "train_example_names = [fn.split('.')[0] for fn in os.listdir('data/train')]\n",
    "test_example_names = [fn.split('.')[0] for fn in os.listdir('data/test')]\n",
    "\n",
    "metadata = pd.read_csv('data/train.csv')\n",
    "metadata_train = metadata.loc[metadata.Id.isin(train_example_names)]\n",
    "metadata_test = metadata.loc[metadata.Id.isin(test_example_names)]\n",
    "\n",
    "metadata = pd.read_csv('data/train.csv')\n",
    "metadata_train = metadata.loc[metadata.Id.isin(train_example_names)]\n",
    "metadata_test = metadata.loc[metadata.Id.isin(test_example_names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(txt):\n",
    "    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower())\n",
    "\n",
    "def remove_punc(txt):\n",
    "    return re.sub('[^A-Za-z0-9]+', ' ', str(txt))\n",
    "\n",
    "def get_doc_id(doc_path):\n",
    "    return os.path.split(train_example_names[0])[-1].split('.')[0]\n",
    "\n",
    "def load_train_example(i: int):\n",
    "    doc_path = train_example_paths[i]\n",
    "    with open(doc_path) as f:\n",
    "        data = json.load(f)\n",
    "    return {'doc': data, 'meta': metadata.loc[metadata.Id == get_doc_id(doc_path)]}\n",
    "\n",
    "def load_train_example_by_name(name):\n",
    "    doc_path = os.path.join('data/train', name + '.json')\n",
    "    with open(doc_path) as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def delete_file(filename):\n",
    "    if os.path.exists(filename):\n",
    "        os.remove(filename)"
   ]
  },
  {
   "source": [
    "## Split Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train size: 12168\nval size: 2148\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "docIdx = train_example_names.copy()\n",
    "random.seed(42)\n",
    "random.shuffle(docIdx)\n",
    "\n",
    "train_ratio = 0.85\n",
    "n_train = int(len(docIdx) * train_ratio)\n",
    "n_val = len(docIdx) - n_train\n",
    "\n",
    "train_idx = docIdx[:n_train]\n",
    "val_idx = docIdx[n_train:]\n",
    "\n",
    "print(f'train size: {len(train_idx)}')\n",
    "print(f'val size: {len(val_idx)}')"
   ]
  },
  {
   "source": [
    "## Generate Dataset and Features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_RE_COMBINE_WHITESPACE = re.compile(r\"\\s+\")\n",
    "\n",
    "def preprocess_tokenize_doc(doc_json):\n",
    "    doc_text = ' '.join([remove_punc(sec['text']) for sec in doc_json])\n",
    "    doc_text = make_single_whitespace(doc_text)\n",
    "    \n",
    "    doc_tokens = doc_text.split(' ')\n",
    "    return doc_tokens\n",
    "\n",
    "def indices(lst, element):\n",
    "    result = [i for i, token in enumerate(lst) if element in token]\n",
    "    return result\n",
    "\n",
    "def make_single_whitespace(text):\n",
    "    return _RE_COMBINE_WHITESPACE.sub(\" \", text).strip()"
   ]
  },
  {
   "source": [
    "## Create dataframe for tokens and targets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cols = ['TOKEN', 'TARGET']\n",
    "\n",
    "def get_doc(doc_id, reduce_tokens = False):\n",
    "    doc_labels = list(metadata_train.loc[metadata_train.Id == doc_id, 'dataset_label'].values)\n",
    "    doc_labels = [make_single_whitespace(remove_punc(l.strip())).lower() for l in doc_labels]\n",
    "\n",
    "    doc = load_train_example_by_name(doc_id)\n",
    "    doc_tokens = preprocess_tokenize_doc(doc)\n",
    "    doc_tokens_lower = [t.lower() for t in doc_tokens]\n",
    "    \n",
    "    # Targets for dataset names will be 1\n",
    "    target_arr = np.zeros(len(doc_tokens) ,dtype = 'uint8')\n",
    "\n",
    "    # Keep n tokens before and after targets\n",
    "    keep_df = pd.Series(np.zeros(len(doc_tokens), dtype = 'bool'))\n",
    "\n",
    "    for l in doc_labels:\n",
    "        n_label_tokens = len(l.split(' '))\n",
    "        doc_tokens_joined = [' '.join(doc_tokens_lower[i:i+n_label_tokens]) for i in range(len(doc_tokens_lower) - n_label_tokens + 1)]\n",
    "        \n",
    "        occurrences = indices(doc_tokens_joined, l)\n",
    "\n",
    "        assert len(occurrences) != 0, f'Label {l} not found in doc {doc_id}'\n",
    "        for o in occurrences:\n",
    "            keep_start = max(0, o - 250)\n",
    "            keep_end = min(o + 250 + n_label_tokens, len(doc_tokens))\n",
    "            keep_df[keep_start: keep_end] = True\n",
    "            for i in range(n_label_tokens):\n",
    "                target_arr[o + i] = 1\n",
    "\n",
    "    doc_df['TOKEN'] = doc_tokens\n",
    "    doc_df['TARGET'] = target_arr\n",
    "    doc_df['TARGET'] = doc_df['TARGET'].astype('str')\n",
    "    if reduce_tokens:\n",
    "        doc_df = doc_df.loc[keep_df]\n",
    "\n",
    "    return doc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import crf_feature_extractor\n",
    "\n",
    "feature_extractor = crf_feature_extractor.TextFeatureExtractor()"
   ]
  },
  {
   "source": [
    "## Augmentation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset names\n",
    "with open('data/dataset_names_processed.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    us_dataset_names = f.readlines()\n",
    "    us_dataset_names = [n for n in us_dataset_names if len(n) > 25]\n",
    "    us_dataset_names = [make_single_whitespace(n) for n in us_dataset_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "n_examples = len(train_idx)\n",
    "n_generator_repeat = 3\n",
    "#augment_chance = 1.0\n",
    "\n",
    "def replace_target(x):\n",
    "    if x.TARGET.iloc[0] == '0':\n",
    "        # if not a dataset name, do not augment\n",
    "        return x\n",
    "    else:\n",
    "        name_len = 0\n",
    "        random_name_tokens = None\n",
    "        while name_len < len(x):\n",
    "            random_name_tokens = random.choice(us_dataset_names).split(' ')\n",
    "            name_len = len(random_name_tokens)\n",
    "\n",
    "        # Replace tokens\n",
    "        x['TOKEN'] = random_name_tokens[:len(x)]\n",
    "\n",
    "        return x\n",
    "\n",
    "def x_train_generator():\n",
    "    i_repeat = 0\n",
    "    while i_repeat < n_generator_repeat:\n",
    "        i_repeat += 1\n",
    "        print(f'X_train generator repeat: {i_repeat}')\n",
    "        for doc_id in train_idx[:n_examples]:\n",
    "            doc_df = get_doc(doc_id, reduce_tokens = True)\n",
    "            features = None\n",
    "            if (i_repeat > 1):\n",
    "                # Do augmentation\n",
    "                new_df = doc_df.copy()\n",
    "\n",
    "                #display(new_df.loc[new_df.TARGET == '1'])\n",
    "\n",
    "                # Replace target tokens\n",
    "                new_df = new_df.groupby((new_df.TARGET.shift() != new_df.TARGET).cumsum()).apply(lambda x: replace_target(x)).reset_index()\n",
    "\n",
    "                #display(new_df.loc[new_df.TARGET == '1'])\n",
    "                features = feature_extractor.transform({'output': new_df})['output'][0]\n",
    "                del new_df\n",
    "            else:\n",
    "                # No augmentation\n",
    "                features = feature_extractor.transform({'output': doc_df})['output'][0]\n",
    "            \n",
    "            yield features\n",
    "\n",
    "def x_val_generator():\n",
    "    for doc_id in val_idx[:n_examples]:\n",
    "        doc_df = get_doc(doc_id, reduce_tokens = False)\n",
    "        yield feature_extractor.transform({'output': doc_df})['output'][0]\n",
    "\n",
    "def y_train_generator():\n",
    "    for _ in range(n_generator_repeat):\n",
    "        for doc_id in train_idx[:n_examples]:\n",
    "            doc_df = doc_dfs[doc_id]\n",
    "\n",
    "def y_val_generator():\n",
    "    for doc_id in val_idx:\n",
    "        doc_df = doc_dfs[doc_id]\n",
    "        yield list(doc_df.TARGET.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'train_data': (x_train_generator, y_train_generator),\n",
    "    'val_data': (x_val_generator, y_val_generator)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "528"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "len(doc_dfs[train_idx[2]])"
   ]
  },
  {
   "source": [
    "## Create CRF Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlmodels import sklearn_model\n",
    "\n",
    "from sklearn_crfsuite import CRF\n",
    "from mlmodels.search.hparameters import crf_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "sklearn_model hparameters: {'minimize_metric': False}\n",
      "Minimize metric:\n",
      "False\n",
      "No metric found.\n",
      "sklearn_model hparameters: {'algorithm': 'arow', 'all_possible_states': True, 'all_possible_transitions': False, 'epsilon': 0.031628548199471695, 'gamma': 4.2607215826442255, 'min_freq': 0, 'variance': 0.9707160561354112, 'model_class': <class 'sklearn_crfsuite.estimator.CRF'>}\n",
      "X_train generator repeat: 1\n",
      "X_train generator repeat: 2\n",
      "  0%|          | 0/12 [10:10<?, ?trial/s, best loss=?]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delete_file('trials_sklearn')\n",
    "\n",
    "model = sklearn_model.SklearnModel1(minimize_metric = False)\n",
    "\n",
    "res = model.search(data, crf_params.search_space, crf_params.search_fixed, num_iter = 12)\n",
    "best_hparams = res['best_params']\n",
    "best_hparams.update(crf_params.search_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "sklearn_model hparameters: {'minimize_metric': False, 'algorithm': 'lbfgs', 'all_possible_states': True, 'all_possible_transitions': False, 'c1': 12.360212411014425, 'c2': 0.43523465616536766, 'delta': 0.0026425311669412586, 'epsilon': 0.0012256426222802155, 'linesearch': 'MoreThuente', 'max_linesearch': 41, 'min_freq': 11, 'num_memories': 24, 'period': 9, 'model_class': <class 'sklearn_crfsuite.estimator.CRF'>}\n",
      "X_train generator repeat: 1\n",
      "X_train generator repeat: 2\n",
      "X_train generator repeat: 3\n",
      "X_train generator repeat: 4\n",
      "C:\\projects\\env\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'metric': 0.9207596158647204}"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "model = sklearn_model.SklearnModel1(minimize_metric = False, **best_hparams)\n",
    "res = model.fit(data)\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_text(doc_id):\n",
    "    doc = load_train_example_by_name(doc_id)\n",
    "    doc_tokens = preprocess_tokenize_doc(doc)\n",
    "    return ' '.join(doc_tokens)# TODO: unlazy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = []\n",
    "ids = []\n",
    "for doc_id in val_idx[:5]:\n",
    "    \n",
    "    # Load and preprocess\n",
    "    doc = load_train_example_by_name(doc_id)\n",
    "    doc_tokens = preprocess_tokenize_doc(doc)\n",
    "\n",
    "    # Extract features\n",
    "    x = {'output': pd.DataFrame({'TOKEN': doc_tokens})}\n",
    "    x = feature_extractor.transform(x)\n",
    "\n",
    "    # Predict\n",
    "    pred = model.predict([x])\n",
    "    pred = pred[0]['output'][0]\n",
    "    pred = np.array([int(p) for p in pred])\n",
    "\n",
    "    # Token idx\n",
    "    pos_pred_idx = [i[0] for i in np.argwhere(pred == 1)]\n",
    "    pred_delimited = []\n",
    "    if len(pred > 0):\n",
    "        pred_prev = pos_pred_idx[0]\n",
    "        for p in pos_pred_idx:\n",
    "            if p - pred_prev > 3:\n",
    "                pred_delimited.append(-1)\n",
    "            \n",
    "            pred_delimited.append(p)\n",
    "            pred_prev = p\n",
    "\n",
    "    # Get corresponding tokens\n",
    "    pred_tokens = [('|' if i == -1 else doc_tokens[i]) for i in pred_delimited]\n",
    "    \n",
    "    pred_joined = ' '.join(pred_tokens)\n",
    "    pred_tokens = pred_joined.split(' | ')\n",
    "    preds_joined = list(set(pred_tokens))\n",
    "\n",
    "    preds_joined_cleaned = preds_joined#[]\n",
    "\n",
    "    \"\"\"for p in preds_joined:\n",
    "        label_similarities = [fuzz.ratio(l, p) for l in labels]\n",
    "        if max(label_similarities) >= 70:\n",
    "            preds_joined_cleaned.append(labels[np.argmax(label_similarities)])\"\"\"\n",
    "        \n",
    "    # 2 options: remove if not similar to any label, or keep without modification\n",
    "\n",
    "    preds_joined_cleaned = list(set(preds_joined_cleaned))\n",
    "    test_preds.append('|'.join(preds_joined_cleaned))\n",
    "    ids.append(doc_id)\n",
    "\n",
    "sub_df = pd.DataFrame(columns = ['Id', 'PredictionString'])\n",
    "sub_df['Id'] = ids\n",
    "sub_df['PredictionString'] = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                     Id  \\\n",
       "0  7a9fcb80-85b1-47bb-a3a5-4069c69b4068   \n",
       "1  d6bce360-e056-42ba-9b18-e687407f1661   \n",
       "2  09f36a02-f4d1-4fb3-840c-5c7a2285b17c   \n",
       "3  9e08d7d3-8f8c-4b0a-9445-f9cfba0813b7   \n",
       "4  e657b6c3-e32c-4daf-a092-3de500980f37   \n",
       "\n",
       "                          PredictionString  \n",
       "0                                     adni  \n",
       "1       early childhood longitudinal study  \n",
       "2    baltimore longitudinal study of aging  \n",
       "3  agricultural resource management survey  \n",
       "4                                     adni  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>PredictionString</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7a9fcb80-85b1-47bb-a3a5-4069c69b4068</td>\n      <td>adni</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>d6bce360-e056-42ba-9b18-e687407f1661</td>\n      <td>early childhood longitudinal study</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>09f36a02-f4d1-4fb3-840c-5c7a2285b17c</td>\n      <td>baltimore longitudinal study of aging</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9e08d7d3-8f8c-4b0a-9445-f9cfba0813b7</td>\n      <td>agricultural resource management survey</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>e657b6c3-e32c-4daf-a092-3de500980f37</td>\n      <td>adni</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "pd.options.display.max_rows = 25\n",
    "pd.options.display.max_colwidth = 150\n",
    "sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'2013 a synthetic panel is constructed using annual cross sectional farm level observations from the agricultural resource management survey arms cohorts are formed by grouping farms into similar categories based upon farm production type r'"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "str_find = 'agricultural resource management survey'\n",
    "doc_id = '9e08d7d3-8f8c-4b0a-9445-f9cfba0813b7'.strip()\n",
    "text = get_doc_text(doc_id)\n",
    "\n",
    "str_i = text.index(str_find)\n",
    "text[str_i - 100:str_i + 100 + len(str_find)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(metadata_train.cleaned_label.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['national education longitudinal study',\n",
       " 'noaa tidal station',\n",
       " 'slosh model',\n",
       " 'noaa c cap',\n",
       " 'aging integrated database agid ',\n",
       " 'alzheimers disease neuroimaging initiative',\n",
       " 'aging integrated database',\n",
       " 'noaa national water level observation network',\n",
       " 'noaa water level station',\n",
       " 'baltimore longitudinal study of aging blsa ',\n",
       " 'national water level observation network',\n",
       " 'arms farm financial and crop production practices',\n",
       " 'beginning postsecondary student',\n",
       " 'noaa sea lake and overland surges from hurricanes',\n",
       " 'noaa tide gauge',\n",
       " 'the national institute on aging genetics of alzheimer s disease data storage site',\n",
       " 'national center for education statistics common core of data',\n",
       " 'national science foundation survey of industrial research and development',\n",
       " 'baccalaureate and beyond',\n",
       " 'noaa international best track archive for climate stewardship',\n",
       " 'agricultural resource management survey',\n",
       " 'national teacher and principal survey',\n",
       " 'international best track archive for climate stewardship',\n",
       " 'nsf higher education research and development survey',\n",
       " 'national science foundation survey of earned doctorates',\n",
       " 'school survey on crime and safety',\n",
       " 'the national institute on aging genetics of alzheimer s disease data storage site niagads ',\n",
       " 'national oceanic and atmospheric administration world ocean database',\n",
       " 'beginning postsecondary students longitudinal study',\n",
       " 'nces common core of data',\n",
       " 'program for the international assessment of adult competencies',\n",
       " 'survey of earned doctorates',\n",
       " 'baltimore longitudinal study of aging',\n",
       " 'early childhood longitudinal study',\n",
       " 'adni',\n",
       " 'national science foundation survey of graduate students and postdoctorates in science and engineering',\n",
       " 'trends in international mathematics and science study',\n",
       " 'national oceanic and atmospheric administration c cap',\n",
       " 'nsf survey of earned doctorates',\n",
       " 'noaa tide station',\n",
       " 'education longitudinal study',\n",
       " 'optimum interpolation sea surface temperature',\n",
       " 'national oceanic and atmospheric administration optimum interpolation sea surface temperature',\n",
       " 'alzheimer s disease neuroimaging initiative adni ',\n",
       " 'baccalaureate and beyond longitudinal study',\n",
       " 'agricultural resources management survey',\n",
       " 'beginning postsecondary students',\n",
       " 'ibtracs',\n",
       " 'coastal change analysis program',\n",
       " 'survey of graduate students and postdoctorates in science and engineering',\n",
       " 'national assessment of education progress',\n",
       " 'sea surface temperature optimum interpolation',\n",
       " 'high school longitudinal study',\n",
       " 'nsf survey of graduate students and postdoctorates in science and engineering',\n",
       " 'national science foundation survey of doctorate recipients',\n",
       " 'survey of doctorate recipients',\n",
       " 'coastal change analysis program land cover',\n",
       " 'survey of industrial research and development',\n",
       " 'world ocean database',\n",
       " 'rural urban continuum codes',\n",
       " 'noaa optimum interpolation sea surface temperature',\n",
       " 'noaa world ocean database',\n",
       " 'common core of data',\n",
       " 'higher education research and development survey',\n",
       " 'noaa storm surge inundation',\n",
       " 'national weather service nws storm surge risk',\n",
       " 'survey of science and engineering research facilities',\n",
       " 'nsf survey of industrial research and development',\n",
       " 'national science foundation survey of science and engineering research facilities',\n",
       " 'national science foundation higher education research and development survey',\n",
       " 'national center for science and engineering statistics survey of earned doctorates',\n",
       " 'national center for science and engineering statistics survey of science and engineering research facilities',\n",
       " 'national center for science and engineering statistics survey of graduate students and postdoctorates in science and engineering',\n",
       " 'national center for science and engineering statistics survey of doctorate recipients',\n",
       " 'national center for science and engineering statistics survey of industrial research and development',\n",
       " 'national center for science and engineering statistics higher education research and development survey',\n",
       " 'nsf survey of science and engineering research facilities',\n",
       " 'ffrdc research and development survey',\n",
       " 'nsf ffrdc research and development survey',\n",
       " 'survey of state government research and development',\n",
       " 'ncses survey of doctorate recipients',\n",
       " 'ncses survey of graduate students and postdoctorates in science and engineering',\n",
       " 'anss comprehensive earthquake catalog',\n",
       " 'anss comprehensive catalog',\n",
       " 'advanced national seismic system anss comprehensive catalog comcat ',\n",
       " 'advanced national seismic system comprehensive catalog',\n",
       " 'census of agriculture',\n",
       " 'usda census of agriculture',\n",
       " 'nass census of agriculture',\n",
       " 'north american breeding bird survey',\n",
       " 'north american breeding bird survey bbs ',\n",
       " 'usgs north american breeding bird survey',\n",
       " 'covid 19 open research dataset cord 19 ',\n",
       " 'covid 19 open research dataset',\n",
       " 'covid open research dataset',\n",
       " 'covid 19 open research data',\n",
       " 'complexity science hub covid 19 control strategies list cccsl ',\n",
       " 'complexity science hub covid 19 control strategies list',\n",
       " 'cccsl',\n",
       " 'our world in data covid 19 dataset',\n",
       " 'our world in data covid 19',\n",
       " 'our world in data',\n",
       " 'jh crown registry',\n",
       " 'characterizing health associated risks and your baseline disease in sars cov 2 charybdis ',\n",
       " 'characterizing health associated risks and your baseline disease in sars cov 2',\n",
       " 'covid 19 death data',\n",
       " 'sars cov 2 genome sequence',\n",
       " 'sars cov 2 genome sequences',\n",
       " 'covid 19 genome sequence',\n",
       " 'covid 19 genome sequences',\n",
       " '2019 ncov genome sequence',\n",
       " '2019 ncov genome sequences',\n",
       " 'sars cov 2 full genome sequence',\n",
       " 'sars cov 2 full genome sequences',\n",
       " 'sars cov 2 complete genome sequence',\n",
       " 'sars cov 2 complete genome sequences',\n",
       " '2019 ncov complete genome sequences',\n",
       " 'genome sequences of sars cov 2',\n",
       " 'genome sequence of sars cov 2',\n",
       " 'genome sequence of covid 19',\n",
       " 'genome sequences of covid 19',\n",
       " 'genome sequence of 2019 ncov',\n",
       " 'genome sequences of 2019 ncov',\n",
       " 'covid 19 image data collection',\n",
       " 'rsna international covid 19 open radiology database ricord ',\n",
       " 'rsna international covid 19 open radiology database',\n",
       " 'rsna international covid open radiology database',\n",
       " 'cas covid 19 antiviral candidate compounds dataset',\n",
       " 'cas covid 19 antiviral candidate compounds data set',\n",
       " 'cas covid 19 antiviral candidate compounds data']"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}