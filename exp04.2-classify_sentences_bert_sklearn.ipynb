{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd01b4c7016e99d31c2e7c892573dc93dbd4548eb0a0f5dca22fbf3a690830b4e66",
   "display_name": "Python 3.8.8 64-bit ('torch': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "e68e0ea63623fb9ac5793b248651fce59782b1ca0d072c96c440758c36acff31"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\ozano\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import random\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "#from fuzzywuzzy import fuzz\n",
    "\n",
    "train_example_paths = glob.glob('data/train/*.json')\n",
    "test_example_paths = glob.glob('data/test/*.json')\n",
    "\n",
    "train_example_names = [fn.split('.')[0] for fn in os.listdir('data/train')]\n",
    "test_example_names = [fn.split('.')[0] for fn in os.listdir('data/test')]\n",
    "\n",
    "metadata = pd.read_csv('data/train.csv')\n",
    "docIdx = train_example_names.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_example_by_name(name):\n",
    "    doc_path = os.path.join('data/train', name + '.json')\n",
    "    with open(doc_path) as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def load_test_example_by_name(name):\n",
    "    doc_path = os.path.join('data/test', name + '.json')\n",
    "    with open(doc_path) as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "source": [
    "## Create dataframe for tokens and targets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "\n",
    "match_puncs_re = r\"([.,!?()\\-;\\[\\]+\\\\\\/@:<>#_{}&%'*=\" + r'\"' + r\"|])\"\n",
    "match_puncs_re = re.compile(match_puncs_re)\n",
    "\n",
    "def text_cleaning(text):\n",
    "    text = re.sub('[^A-Za-z0-9]+', ' ', str(text)).strip() # remove unnecessary literals\n",
    "\n",
    "    text = re.sub(r'\\[[0-9]+]', ' specialreference ', text)\n",
    "\n",
    "    # Remove years\n",
    "    text = re.sub(r'(19|20)[0-9][0-9]', ' specialyear ', text)\n",
    "\n",
    "    # remove other digits\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "\n",
    "    # remove extra spaces\n",
    "    text = re.sub(\"\\s+\",\" \", text)\n",
    "\n",
    "    # Remove websites\n",
    "    text = ' '.join(['specialwebsite' if 'http' in t or 'www' in t else t for t in text.split(' ') ])\n",
    "\n",
    "    return text.lower()\n",
    "\n",
    "def text_cleaning_for_bert(text):\n",
    "    # Keeps puncs, pads them with whitespaces\n",
    "\n",
    "    text = text.replace('^', ' ')\n",
    "    text = unidecode.unidecode(text)\n",
    "\n",
    "    # Remove websites\n",
    "    text = ' '.join(['specialwebsite' if 'http' in t or 'www' in t else t for t in text.split(' ') ])\n",
    "\n",
    "    text = match_puncs_re.sub(r' \\1 ', text)\n",
    "\n",
    "    # remove extra spaces\n",
    "    text = re.sub(\"\\s+\",\" \", text)\n",
    "\n",
    "    return text.lower()\n",
    "\n",
    "def text_cleaning_for_label(text):\n",
    "    text = text.replace('^', ' ')\n",
    "    text = unidecode.unidecode(text)\n",
    "\n",
    "    text = re.sub('[^A-Za-z0-9]+', ' ', str(text)).strip() # remove unnecessary literals\n",
    "\n",
    "    # Remove websites\n",
    "    text = ' '.join(['specialwebsite' if 'http' in t or 'www' in t else t for t in text.split(' ') ])\n",
    "\n",
    "    # remove extra spaces\n",
    "    text = re.sub(\"\\s+\",\" \", text)\n",
    "\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "##### STEP 1: Make a list of the known labels provided to us\n",
    "\n",
    "temp_1 = [text_cleaning(x) for x in metadata['dataset_label']]\n",
    "temp_2 = [text_cleaning(x) for x in metadata['dataset_title']]\n",
    "temp_3 = [text_cleaning(x) for x in metadata['cleaned_label']]\n",
    "\n",
    "existing_labels = temp_1 + temp_2 + temp_3\n",
    "existing_labels = [l.lower() for l in existing_labels]\n",
    "existing_labels = list(set(existing_labels))\n",
    "\n",
    "# Sort labels by length in descending order\n",
    "existing_labels = sorted(existing_labels, key = len, reverse= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sentences = []\n",
    "neg_sentences = []\n",
    "\n",
    "def process_doc(doc_id):\n",
    "    doc_json = load_train_example_by_name(doc_id)\n",
    "    doc_text = ' '.join([sec['text'] for sec in doc_json])\n",
    "\n",
    "    # Tokenize sentencewise\n",
    "    sentences = sent_tokenize(doc_text)\n",
    "\n",
    "    adni_count = 0\n",
    "    for sentence in sentences:\n",
    "        clean_sentence = text_cleaning(sentence)\n",
    "\n",
    "        has_label = False\n",
    "        label_is_adni = False\n",
    "        for clean_label in existing_labels:\n",
    "            if clean_label in clean_sentence:\n",
    "                has_label = True\n",
    "\n",
    "                # Remove label from the text, or model will overfit\n",
    "                clean_sentence = clean_sentence.replace(clean_label, '')\n",
    "                if 'adni' in clean_label or 'alzheimer' in clean_label:\n",
    "                    adni_count += 1\n",
    "                    label_is_adni = True\n",
    "\n",
    "        if has_label and (adni_count <= 2 or not label_is_adni):\n",
    "            clean_sentence = re.sub(\"\\s+\",\" \", clean_sentence)\n",
    "            pos_sentences.append(clean_sentence)\n",
    "        else:\n",
    "            if random.uniform(0, 1) < 0.25:\n",
    "                neg_sentences.append(clean_sentence)\n",
    "\n",
    "#get_doc(docIdx[0])[0]"
   ]
  },
  {
   "source": [
    "## Create Dataset for All Documents"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 14316/14316 [06:06<00:00, 39.07it/s]\n",
      "pos size: 28781\n",
      "neg size: 1038592\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc_id in tqdm(docIdx):\n",
    "    process_doc(doc_id)\n",
    "\n",
    "print('')\n",
    "print(f'pos size: {len(pos_sentences)}')\n",
    "print(f'neg size: {len(neg_sentences)}')"
   ]
  },
  {
   "source": [
    "## Save Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pos size: 28781\nneg size: 1038592\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f'data/sentence_classification_data_sklearn/pos.pkl', 'wb') as f:\n",
    "    pickle.dump(pos_sentences, f)\n",
    "\n",
    "with open(f'data/sentence_classification_data_sklearn/neg.pkl', 'wb') as f:\n",
    "    pickle.dump(neg_sentences, f)\n",
    "\n",
    "print(f'pos size: {len(pos_sentences)}')\n",
    "print(f'neg size: {len(neg_sentences)}')"
   ]
  },
  {
   "source": [
    "## Load Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pos size: 28781\nneg size: 1038592\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f'data/sentence_classification_data_sklearn/pos.pkl', 'rb') as f:\n",
    "    pos_sentences = pickle.load(f)\n",
    "\n",
    "with open(f'data/sentence_classification_data_sklearn/neg.pkl', 'rb') as f:\n",
    "    neg_sentences = pickle.load(f)\n",
    "\n",
    "print(f'pos size: {len(pos_sentences)}')\n",
    "print(f'neg size: {len(neg_sentences)}')"
   ]
  },
  {
   "source": [
    "## Preprocess Sentences"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sentences_processed = []\n",
    "neg_sentences_processed = []\n",
    "n_broken_sent = 0\n",
    "\n",
    "def is_text_broken(tokens):\n",
    "    # Some texts are like 'p a dsdv a d a ds f b', remove them\n",
    "    if len(tokens) == 0:\n",
    "        return True\n",
    "\n",
    "    if len(tokens) < 50:\n",
    "        return False\n",
    "\n",
    "    one_char_token_ratio = len([l for l in tokens if len(l) == 1]) / len(tokens)\n",
    "    return one_char_token_ratio > 0.2\n",
    "\n",
    "def split_to_smaller_sent(tokens, s_size, overlap_size):\n",
    "    # output sentences will be s_size + overlap_size long\n",
    "    small_sents = []\n",
    "\n",
    "    if len(tokens) <= s_size:\n",
    "        return [tokens]\n",
    "\n",
    "    n_parts = len(tokens) // s_size\n",
    "    if len(tokens) % s_size != 0:\n",
    "        n_parts += 1\n",
    "\n",
    "    for i_part in range(n_parts):\n",
    "        start_i = i_part * s_size\n",
    "        if i_part > 0:\n",
    "            start_i -= overlap_size\n",
    "\n",
    "        end_i = min(len(tokens), (i_part + 1) * s_size)\n",
    "\n",
    "        small_sents.append(tokens[start_i: end_i])\n",
    "\n",
    "    return small_sents\n",
    "\n",
    "def join_tuple_tokens(tuples):\n",
    "    return ' '.join([t[1] for t in tuples])\n",
    "\n",
    "def get_index(lst, el):\n",
    "    try:\n",
    "        return lst.index(el)\n",
    "    except ValueError as e:\n",
    "        for i, lst_el in enumerate(lst):\n",
    "            if el in lst_el:\n",
    "                return i\n",
    "        \n",
    "    raise ValueError(f'Element {el} not found in {lst}')\n",
    "\n",
    "def process_pos_sentence(sentence):\n",
    "    global n_broken_sent\n",
    "\n",
    "    bert_sentence = text_cleaning_for_bert(sentence)\n",
    "    label_sentence = text_cleaning_for_label(sentence)\n",
    "\n",
    "    if is_text_broken(label_sentence.split(' ')): # Can't use bert cleaning for this, because all punc.s are padded with spaces\n",
    "        n_broken_sent += 1\n",
    "        return\n",
    "    \n",
    "    bert_tokens = bert_sentence.split(' ')\n",
    "    ### STEP 1: Split into fixed sized sentences ###\n",
    "    for small_sentence_tokens in split_to_smaller_sent(bert_tokens, s_size = 125, overlap_size = 25):\n",
    "        small_bert_sentence = ' '.join(small_sentence_tokens)\n",
    "        pos_sentences_processed.append(small_bert_sentence)\n",
    "\n",
    "\n",
    "def process_neg_sentence(sentence):\n",
    "    global n_broken_sent\n",
    "\n",
    "    bert_sentence = text_cleaning_for_bert(sentence)\n",
    "    label_sentence = text_cleaning_for_label(sentence)\n",
    "\n",
    "    if is_text_broken(label_sentence.split(' ')): # Can't use bert cleaning for this, because all punc.s are padded with spaces\n",
    "        n_broken_sent += 1\n",
    "        return\n",
    "    \n",
    "    bert_tokens = bert_sentence.split(' ')\n",
    "    ### STEP 1: Split into fixed sized sentences ###\n",
    "    for small_sentence_tokens in split_to_smaller_sent(bert_tokens, s_size = 125, overlap_size = 25):\n",
    "        small_bert_sentence = ' '.join(small_sentence_tokens)\n",
    "        neg_sentences_processed.append(small_bert_sentence)\n",
    "\n",
    "#process_pos_sentence(pos_sentences[2472])"
   ]
  },
  {
   "source": [
    "## Generate Processed Sentences and Save"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 28781/28781 [00:01<00:00, 25064.10it/s]\n",
      "100%|██████████| 1038592/1038592 [00:34<00:00, 30325.67it/s]\n",
      "pos size: 29198\n",
      "neg size: 1038066\n"
     ]
    }
   ],
   "source": [
    "pos_sentences_processed = []\n",
    "neg_sentences_processed = []\n",
    "n_broken_sent = 0\n",
    "\n",
    "for s in tqdm(pos_sentences):\n",
    "    process_pos_sentence(s)\n",
    "\n",
    "for s in tqdm(neg_sentences):\n",
    "    process_neg_sentence(s)\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(f'data/sentence_classification_data_sklearn/pos_proc.pkl', 'wb') as f:\n",
    "    pickle.dump(pos_sentences_processed, f)\n",
    "\n",
    "with open(f'data/sentence_classification_data_sklearn/neg_proc.pkl', 'wb') as f:\n",
    "    pickle.dump(neg_sentences_processed, f)\n",
    "\n",
    "print(f'pos size: {len(pos_sentences_processed)}')\n",
    "print(f'neg size: {len(neg_sentences_processed)}')"
   ]
  },
  {
   "source": [
    "## Load Processed Sentences"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pos size: 29198\nneg size: 1038066\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f'data/sentence_classification_data_sklearn/pos_proc.pkl', 'rb') as f:\n",
    "    pos_sentences_processed = pickle.load(f)\n",
    "\n",
    "with open(f'data/sentence_classification_data_sklearn/neg_proc.pkl', 'rb') as f:\n",
    "    neg_sentences_processed = pickle.load(f)\n",
    "\n",
    "print(f'pos size: {len(pos_sentences_processed)}')\n",
    "print(f'neg size: {len(neg_sentences_processed)}')"
   ]
  },
  {
   "source": [
    "## Create Training Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_idx = np.random.permutation(len(neg_sentences_processed))\n",
    "neg_sentences_processed = [neg_sentences_processed[i] for i in perm_idx[:150000]]\n",
    "\n",
    "all_sentences = pos_sentences_processed + neg_sentences_processed\n",
    "y = np.zeros(len(all_sentences))\n",
    "y[:len(pos_sentences_processed)] = 1\n",
    "\n",
    "print(f'pos size: {len(pos_sentences_processed)}')\n",
    "print(f'neg size: {len(neg_sentences_processed)}')\n",
    "print(f'all_sentences size: {len(all_sentences)}')\n",
    "\n",
    "del pos_sentences_processed\n",
    "del neg_sentences_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"from sklearn.model_selection import train_test_split\\n\\nprint('Splitting data...')\\nX_train, X_val, y_train, y_val = train_test_split(all_sentences, y, test_size=0.20, random_state=42)\\n\\ndel all_sentences\\ndel y\""
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "\"\"\"from sklearn.model_selection import train_test_split\n",
    "\n",
    "print('Splitting data...')\n",
    "X_train, X_val, y_train, y_val = train_test_split(all_sentences, y, test_size=0.20, random_state=42)\n",
    "\n",
    "del all_sentences\n",
    "del y\"\"\""
   ]
  },
  {
   "source": [
    "## Train Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import csv\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "import statistics as stats\n",
    "from bert_sklearn import BertClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Building sklearn text classifier...\n"
     ]
    }
   ],
   "source": [
    "model = BertClassifier(bert_model='scibert-scivocab-uncased',\n",
    "                        validation_fraction= 0.15,\n",
    "                        max_seq_length=150,\n",
    "                        train_batch_size=1,\n",
    "                        warmup_proportion=0.02,\n",
    "                        gradient_accumulation_steps=1\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading scibert-scivocab-uncased model...\n",
      "Defaulting to linear classifier/regressor\n",
      "Loading Pytorch checkpoint\n",
      "train data size: 85000, validation data size: 15000\n",
      "Training  :   0%|          | 0/85000 [00:00<?, ?it/s]C:\\Users\\ozano\\.conda\\envs\\torch\\lib\\site-packages\\bert_sklearn\\model\\pytorch_pretrained\\optimization.py:275: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:1005.)\n",
      "  next_m.mul_(beta1).add_(1 - beta1, grad)\n",
      "Training  :   0%|          | 86/85000 [00:24<3:59:14,  5.92it/s, loss=0.817]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x0000019D35E25550>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\ozano\\.conda\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1324, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"C:\\Users\\ozano\\.conda\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1297, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"C:\\Users\\ozano\\.conda\\envs\\torch\\lib\\multiprocessing\\process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"C:\\Users\\ozano\\.conda\\envs\\torch\\lib\\multiprocessing\\popen_spawn_win32.py\", line 108, in wait\n",
      "    res = _winapi.WaitForSingleObject(int(self._handle), msecs)\n",
      "KeyboardInterrupt: \n",
      "Training  :   0%|          | 86/85000 [00:25<6:54:46,  3.41it/s, loss=0.817]"
     ]
    }
   ],
   "source": [
    "model.fit(all_sentences[:100000], y[:100000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to disk\n",
    "savefile='data/sklearn_bert_classification.bin'\n",
    "model.save(savefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_process_doc(doc_id):\n",
    "    doc_json = load_test_example_by_name(doc_id)\n",
    "    doc_text = ' '.join([sec['text'] for sec in doc_json])\n",
    "\n",
    "    # Tokenize sentences\n",
    "    sentences = sent_tokenize(doc_text)\n",
    "\n",
    "    sentences_cleaned = [text_cleaning(s).lower() for s in sentences]\n",
    "\n",
    "    X_test = vectorizer.transform(sentences_cleaned)\n",
    "\n",
    "    return X_test, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['rural urban continuum codes']"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "test_doc_id = test_example_names[3]\n",
    "X_test, sentences = test_process_doc(test_doc_id)\n",
    "doc_preds = model.predict([{'output': X_test}])[0]['output']\n",
    "\n",
    "pos_pred_idx = np.argwhere(doc_preds == 1)[:, 0]\n",
    "pos_pred_sentences = [sentences[i] for i in pos_pred_idx]\n",
    "\n",
    "list(metadata.loc[metadata.Id == test_doc_id, 'cleaned_label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[\"Results from shoppers' intercept survey data collected at 13 stores in LI areas in nine Northeastern locations were compared with those obtained using secondary household food purchasing data from the Information Resource Incorporated (IRI) Consumer Network Panel (CNP) courtesy of the USDA Economic Research Service (ERS), and food expenditures from the Consumer Expenditure Survey (CES) of the US Bureau of Labor Statistics.\",\n",
       " \"Participants' county of residence was linked with the USDA (2013) Rural-Urban Continuum Codes (RUCCs) (United States Department of Agriculture, 2013) to determine if the household was located in an urban or rural area.\"]"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "pos_pred_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check model using eli5"
   ]
  },
  {
   "source": [
    "## Create NER Sentences"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}