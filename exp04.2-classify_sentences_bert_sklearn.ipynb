{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ozano\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import random\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "#from fuzzywuzzy import fuzz\n",
    "\n",
    "train_example_paths = glob.glob('data/train/*.json')\n",
    "test_example_paths = glob.glob('data/test/*.json')\n",
    "\n",
    "train_example_names = [fn.split('.')[0] for fn in os.listdir('data/train')]\n",
    "test_example_names = [fn.split('.')[0] for fn in os.listdir('data/test')]\n",
    "\n",
    "metadata = pd.read_csv('data/train.csv')\n",
    "docIdx = train_example_names.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_example_by_name(name):\n",
    "    doc_path = os.path.join('data/train', name + '.json')\n",
    "    with open(doc_path) as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def load_test_example_by_name(name):\n",
    "    doc_path = os.path.join('data/test', name + '.json')\n",
    "    with open(doc_path) as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataframe for tokens and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "\n",
    "match_puncs_re = r\"([.,!?()\\-;\\[\\]+\\\\\\/@:<>#_{}&%'*=\" + r'\"' + r\"|])\"\n",
    "match_puncs_re = re.compile(match_puncs_re)\n",
    "\n",
    "def text_cleaning(text):\n",
    "    text = re.sub('[^A-Za-z0-9]+', ' ', str(text)).strip() # remove unnecessary literals\n",
    "\n",
    "    text = re.sub(r'\\[[0-9]+]', ' specialreference ', text)\n",
    "\n",
    "    # Remove years\n",
    "    text = re.sub(r'(19|20)[0-9][0-9]', ' specialyear ', text)\n",
    "\n",
    "    # remove other digits\n",
    "    text = re.sub(r'\\d+', ' ', text)\n",
    "\n",
    "    # remove extra spaces\n",
    "    text = re.sub(\"\\s+\",\" \", text)\n",
    "\n",
    "    # Remove websites\n",
    "    text = ' '.join(['specialwebsite' if 'http' in t or 'www' in t else t for t in text.split(' ') ])\n",
    "\n",
    "    return text.lower()\n",
    "\n",
    "def text_cleaning_for_bert(text):\n",
    "    # Keeps puncs, pads them with whitespaces\n",
    "\n",
    "    text = text.replace('^', ' ')\n",
    "    text = unidecode.unidecode(text)\n",
    "\n",
    "    # Remove websites\n",
    "    text = ' '.join(['specialwebsite' if 'http' in t or 'www' in t else t for t in text.split(' ') ])\n",
    "\n",
    "    text = match_puncs_re.sub(r' \\1 ', text)\n",
    "\n",
    "    # remove extra spaces\n",
    "    text = re.sub(\"\\s+\",\" \", text)\n",
    "\n",
    "    return text.lower()\n",
    "\n",
    "def text_cleaning_for_label(text):\n",
    "    text = text.replace('^', ' ')\n",
    "    text = unidecode.unidecode(text)\n",
    "\n",
    "    text = re.sub('[^A-Za-z0-9]+', ' ', str(text)).strip() # remove unnecessary literals\n",
    "\n",
    "    # Remove websites\n",
    "    text = ' '.join(['specialwebsite' if 'http' in t or 'www' in t else t for t in text.split(' ') ])\n",
    "\n",
    "    # remove extra spaces\n",
    "    text = re.sub(\"\\s+\",\" \", text)\n",
    "\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "##### STEP 1: Make a list of the known labels provided to us\n",
    "\n",
    "temp_1 = [text_cleaning(x) for x in metadata['dataset_label']]\n",
    "temp_2 = [text_cleaning(x) for x in metadata['dataset_title']]\n",
    "temp_3 = [text_cleaning(x) for x in metadata['cleaned_label']]\n",
    "\n",
    "existing_labels = temp_1 + temp_2 + temp_3\n",
    "existing_labels = [l.lower() for l in existing_labels]\n",
    "existing_labels = list(set(existing_labels))\n",
    "\n",
    "# Sort labels by length in descending order\n",
    "existing_labels = sorted(existing_labels, key = len, reverse= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sentences = []\n",
    "neg_sentences = []\n",
    "\n",
    "def process_doc(doc_id):\n",
    "    doc_json = load_train_example_by_name(doc_id)\n",
    "    doc_text = ' '.join([sec['text'] for sec in doc_json])\n",
    "\n",
    "    # Tokenize sentencewise\n",
    "    sentences = sent_tokenize(doc_text)\n",
    "\n",
    "    adni_count = 0\n",
    "    for sentence in sentences:\n",
    "        clean_sentence = text_cleaning(sentence)\n",
    "\n",
    "        has_label = False\n",
    "        label_is_adni = False\n",
    "        for clean_label in existing_labels:\n",
    "            if clean_label in clean_sentence:\n",
    "                has_label = True\n",
    "\n",
    "                # Remove label from the text, or model will overfit\n",
    "                clean_sentence = clean_sentence.replace(clean_label, '')\n",
    "                if 'adni' in clean_label or 'alzheimer' in clean_label:\n",
    "                    adni_count += 1\n",
    "                    label_is_adni = True\n",
    "\n",
    "        if has_label and (adni_count <= 2 or not label_is_adni):\n",
    "            clean_sentence = re.sub(\"\\s+\",\" \", clean_sentence)\n",
    "            pos_sentences.append(clean_sentence)\n",
    "        else:\n",
    "            if random.uniform(0, 1) < 0.25:\n",
    "                neg_sentences.append(clean_sentence)\n",
    "\n",
    "#get_doc(docIdx[0])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset for All Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14316/14316 [06:06<00:00, 39.07it/s]\n",
      "pos size: 28781\n",
      "neg size: 1038592\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc_id in tqdm(docIdx):\n",
    "    process_doc(doc_id)\n",
    "\n",
    "print('')\n",
    "print(f'pos size: {len(pos_sentences)}')\n",
    "print(f'neg size: {len(neg_sentences)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos size: 28781\n",
      "neg size: 1038592\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f'data/sentence_classification_data_sklearn/pos.pkl', 'wb') as f:\n",
    "    pickle.dump(pos_sentences, f)\n",
    "\n",
    "with open(f'data/sentence_classification_data_sklearn/neg.pkl', 'wb') as f:\n",
    "    pickle.dump(neg_sentences, f)\n",
    "\n",
    "print(f'pos size: {len(pos_sentences)}')\n",
    "print(f'neg size: {len(neg_sentences)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos size: 28781\n",
      "neg size: 1038592\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f'data/sentence_classification_data_sklearn/pos.pkl', 'rb') as f:\n",
    "    pos_sentences = pickle.load(f)\n",
    "\n",
    "with open(f'data/sentence_classification_data_sklearn/neg.pkl', 'rb') as f:\n",
    "    neg_sentences = pickle.load(f)\n",
    "\n",
    "print(f'pos size: {len(pos_sentences)}')\n",
    "print(f'neg size: {len(neg_sentences)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sentences_processed = []\n",
    "neg_sentences_processed = []\n",
    "n_broken_sent = 0\n",
    "\n",
    "def is_text_broken(tokens):\n",
    "    # Some texts are like 'p a dsdv a d a ds f b', remove them\n",
    "    if len(tokens) == 0:\n",
    "        return True\n",
    "\n",
    "    if len(tokens) < 50:\n",
    "        return False\n",
    "\n",
    "    one_char_token_ratio = len([l for l in tokens if len(l) == 1]) / len(tokens)\n",
    "    return one_char_token_ratio > 0.2\n",
    "\n",
    "def split_to_smaller_sent(tokens, s_size, overlap_size):\n",
    "    # output sentences will be s_size + overlap_size long\n",
    "    small_sents = []\n",
    "\n",
    "    if len(tokens) <= s_size:\n",
    "        return [tokens]\n",
    "\n",
    "    n_parts = len(tokens) // s_size\n",
    "    if len(tokens) % s_size != 0:\n",
    "        n_parts += 1\n",
    "\n",
    "    for i_part in range(n_parts):\n",
    "        start_i = i_part * s_size\n",
    "        if i_part > 0:\n",
    "            start_i -= overlap_size\n",
    "\n",
    "        end_i = min(len(tokens), (i_part + 1) * s_size)\n",
    "\n",
    "        small_sents.append(tokens[start_i: end_i])\n",
    "\n",
    "    return small_sents\n",
    "\n",
    "def join_tuple_tokens(tuples):\n",
    "    return ' '.join([t[1] for t in tuples])\n",
    "\n",
    "def get_index(lst, el):\n",
    "    try:\n",
    "        return lst.index(el)\n",
    "    except ValueError as e:\n",
    "        for i, lst_el in enumerate(lst):\n",
    "            if el in lst_el:\n",
    "                return i\n",
    "        \n",
    "    raise ValueError(f'Element {el} not found in {lst}')\n",
    "\n",
    "def process_pos_sentence(sentence):\n",
    "    global n_broken_sent\n",
    "\n",
    "    bert_sentence = text_cleaning_for_bert(sentence)\n",
    "    label_sentence = text_cleaning_for_label(sentence)\n",
    "\n",
    "    if is_text_broken(label_sentence.split(' ')): # Can't use bert cleaning for this, because all punc.s are padded with spaces\n",
    "        n_broken_sent += 1\n",
    "        return\n",
    "    \n",
    "    bert_tokens = bert_sentence.split(' ')\n",
    "    ### STEP 1: Split into fixed sized sentences ###\n",
    "    for small_sentence_tokens in split_to_smaller_sent(bert_tokens, s_size = 125, overlap_size = 25):\n",
    "        small_bert_sentence = ' '.join(small_sentence_tokens)\n",
    "        pos_sentences_processed.append(small_bert_sentence)\n",
    "\n",
    "\n",
    "def process_neg_sentence(sentence):\n",
    "    global n_broken_sent\n",
    "\n",
    "    bert_sentence = text_cleaning_for_bert(sentence)\n",
    "    label_sentence = text_cleaning_for_label(sentence)\n",
    "\n",
    "    if is_text_broken(label_sentence.split(' ')): # Can't use bert cleaning for this, because all punc.s are padded with spaces\n",
    "        n_broken_sent += 1\n",
    "        return\n",
    "    \n",
    "    bert_tokens = bert_sentence.split(' ')\n",
    "    ### STEP 1: Split into fixed sized sentences ###\n",
    "    for small_sentence_tokens in split_to_smaller_sent(bert_tokens, s_size = 125, overlap_size = 25):\n",
    "        small_bert_sentence = ' '.join(small_sentence_tokens)\n",
    "        neg_sentences_processed.append(small_bert_sentence)\n",
    "\n",
    "#process_pos_sentence(pos_sentences[2472])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Processed Sentences and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28781/28781 [00:01<00:00, 25064.10it/s]\n",
      "100%|██████████| 1038592/1038592 [00:34<00:00, 30325.67it/s]\n",
      "pos size: 29198\n",
      "neg size: 1038066\n"
     ]
    }
   ],
   "source": [
    "pos_sentences_processed = []\n",
    "neg_sentences_processed = []\n",
    "n_broken_sent = 0\n",
    "\n",
    "for s in tqdm(pos_sentences):\n",
    "    process_pos_sentence(s)\n",
    "\n",
    "for s in tqdm(neg_sentences):\n",
    "    process_neg_sentence(s)\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(f'data/sentence_classification_data_sklearn/pos_proc.pkl', 'wb') as f:\n",
    "    pickle.dump(pos_sentences_processed, f)\n",
    "\n",
    "with open(f'data/sentence_classification_data_sklearn/neg_proc.pkl', 'wb') as f:\n",
    "    pickle.dump(neg_sentences_processed, f)\n",
    "\n",
    "print(f'pos size: {len(pos_sentences_processed)}')\n",
    "print(f'neg size: {len(neg_sentences_processed)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Processed Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos size: 29198\n",
      "neg size: 1038066\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f'data/sentence_classification_data_sklearn/pos_proc.pkl', 'rb') as f:\n",
    "    pos_sentences_processed = pickle.load(f)\n",
    "\n",
    "with open(f'data/sentence_classification_data_sklearn/neg_proc.pkl', 'rb') as f:\n",
    "    neg_sentences_processed = pickle.load(f)\n",
    "\n",
    "print(f'pos size: {len(pos_sentences_processed)}')\n",
    "print(f'neg size: {len(neg_sentences_processed)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos size: 0\n",
      "neg size: 0\n",
      "all_sentences size: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "perm_idx = np.random.permutation(len(neg_sentences_processed))\n",
    "neg_sentences_processed = [neg_sentences_processed[i] for i in perm_idx[:150000]]\n",
    "\n",
    "all_sentences = pos_sentences_processed + neg_sentences_processed\n",
    "y = np.zeros(len(all_sentences))\n",
    "y[:len(pos_sentences_processed)] = 1\n",
    "\n",
    "print(f'pos size: {len(pos_sentences_processed)}')\n",
    "print(f'neg size: {len(neg_sentences_processed)}')\n",
    "print(f'all_sentences size: {len(all_sentences)}')\n",
    "\n",
    "#del pos_sentences_processed\n",
    "#del neg_sentences_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from sklearn.model_selection import train_test_split\\n\\nprint('Splitting data...')\\nX_train, X_val, y_train, y_val = train_test_split(all_sentences, y, test_size=0.20, random_state=42)\\n\\ndel all_sentences\\ndel y\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from sklearn.model_selection import train_test_split\n",
    "\n",
    "print('Splitting data...')\n",
    "X_train, X_val, y_train, y_val = train_test_split(all_sentences, y, test_size=0.20, random_state=42)\n",
    "\n",
    "del all_sentences\n",
    "del y\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import csv\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "import statistics as stats\n",
    "from bert_sklearn import BertClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building sklearn text classifier...\n"
     ]
    }
   ],
   "source": [
    "model = BertClassifier(bert_model='scibert-scivocab-uncased',\n",
    "                        validation_fraction= 0.15,\n",
    "                        max_seq_length=150,\n",
    "                        train_batch_size=4,\n",
    "                        warmup_proportion=0.1,\n",
    "                        gradient_accumulation_steps=1,\n",
    "                        epochs = 3\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading scibert-scivocab-uncased model...\n",
      "Defaulting to linear classifier/regressor\n",
      "Loading Pytorch checkpoint\n",
      "train data size: 152319, validation data size: 26879\n",
      "Training  :   0%|          | 0/38080 [00:00<?, ?it/s]C:\\Users\\ozano\\.conda\\envs\\torch\\lib\\site-packages\\bert_sklearn\\model\\pytorch_pretrained\\optimization.py:275: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:1005.)\n",
      "  next_m.mul_(beta1).add_(1 - beta1, grad)\n",
      "Training  : 100%|██████████| 38080/38080 [3:11:08<00:00,  3.32it/s, loss=0.278]\n",
      "Validating: 100%|██████████| 3360/3360 [09:06<00:00,  6.15it/s]Epoch 1, Train loss: 0.2776, Val loss: 0.3431, Val accy: 91.89%\n",
      "\n",
      "Training  : 100%|██████████| 38080/38080 [3:27:22<00:00,  3.06it/s, loss=0.307]\n",
      "Validating: 100%|██████████| 3360/3360 [09:27<00:00,  5.92it/s]Epoch 2, Train loss: 0.3072, Val loss: 0.3292, Val accy: 92.80%\n",
      "\n",
      "Training  : 100%|██████████| 38080/38080 [3:06:57<00:00,  3.39it/s, loss=0.242]\n",
      "Validating: 100%|██████████| 3360/3360 [08:42<00:00,  6.43it/s]Epoch 3, Train loss: 0.2418, Val loss: 0.2385, Val accy: 94.34%\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertClassifier(bert_model='scibert-scivocab-uncased', do_lower_case=True,\n",
       "               label_list=array([0., 1.]), max_seq_length=150,\n",
       "               train_batch_size=4, validation_fraction=0.15)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(all_sentences, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to disk\n",
    "savefile='data/sklearn_bert_classification.bin'\n",
    "model.save(savefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_test_sentence(sentence):\n",
    "    sentence = text_cleaning(sentence)\n",
    "    output_sents = []\n",
    "\n",
    "    bert_sentence = text_cleaning_for_bert(sentence)\n",
    "    label_sentence = text_cleaning_for_label(sentence)\n",
    "\n",
    "    if is_text_broken(label_sentence.split(' ')): # Can't use bert cleaning for this, because all punc.s are padded with spaces\n",
    "        return output_sents\n",
    "    \n",
    "    bert_tokens = bert_sentence.split(' ')\n",
    "    ### STEP 1: Split into fixed sized sentences ###\n",
    "    for small_sentence_tokens in split_to_smaller_sent(bert_tokens, s_size = 125, overlap_size = 25):\n",
    "        small_bert_sentence = ' '.join(small_sentence_tokens)\n",
    "        output_sents.append(small_bert_sentence)\n",
    "\n",
    "    return output_sents\n",
    "\n",
    "def test_process_doc(doc_id):\n",
    "    doc_json = load_test_example_by_name(doc_id)\n",
    "    doc_text = ' '.join([sec['text'] for sec in doc_json])\n",
    "\n",
    "    # Tokenize sentences\n",
    "    sentences = sent_tokenize(doc_text)\n",
    "    output_sents = []\n",
    "    for s in sentences:\n",
    "        output_sents.extend(process_test_sentence(s))\n",
    "    return output_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 38/38 [00:12<00:00,  2.97it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['rural urban continuum codes']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_doc_id = test_example_names[3]\n",
    "sentences = test_process_doc(test_doc_id)\n",
    "doc_preds = model.predict(sentences)\n",
    "\n",
    "pos_pred_idx = np.argwhere(doc_preds == 1)[:, 0]\n",
    "pos_pred_sentences = [sentences[i] for i in pos_pred_idx]\n",
    "\n",
    "list(metadata.loc[metadata.Id == test_doc_id, 'cleaned_label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['results from shoppers intercept survey data collected at stores in li areas in nine northeastern locations were compared with those obtained using secondary household food purchasing data from the information resource incorporated iri consumer network panel cnp courtesy of the usda economic research service ers and food expenditures from the consumer expenditure survey ces of the us bureau of labor statistics',\n",
       " 'the origin of the work presented here is a year project conducted in the ne with a longterm goal of determining whether greater reliance on regionally produced food might improve the food security of the region and the community food security of li communities throughout the region',\n",
       " 'using secondary data from the specialyear nielsen homescan survey rahkovsky and snyder specialyear reported that regardless of income consumers shop at different food stores over a year s time',\n",
       " 'using the national dataset foodaps which tracks weekly shopping todd and scharadin specialyear found insignificant differences in shopping locations and frequency between urban and rural households',\n",
       " ' specialyear we chose the specific items for many reasons including that they would be staple foods for many li households in the ne',\n",
       " 'the study stores were located in urban and rural locations in zip codes where the median household income was near or below the median income for the state and county',\n",
       " 'the usda ers provided the data via a third party access agreement',\n",
       " 'this definition follows that used by the ers usda in their food access research atlas',\n",
       " 'participants county of residence was linked with the usda specialyear rural urban continuum codes ruccs united states department of agriculture specialyear to determine if the household was located in an urban or rural area',\n",
       " 'participants residing in counties in metro areas ruccs were categorized as urban while all others ruccs were categorized as rural',\n",
       " 'purchasing data on the food items in the survey were combined into categories and sub categories from the usda what we eat in america methodology used in the national health and nutrition examination survey national food survey usda specialyear ',\n",
       " 'iri cnp characteristics of purchasers and non purchasers of mb items using the specialyear cnp data we analyzed purchasing patterns of mbis in the ne across nli and li households and urban and rural households',\n",
       " 'in the produce category we found a higher percentage of hispanics being purchasers of fruit similar to nielson s specialyear findings',\n",
       " 'using data from the specialyear specialyear ces we analyzed food category purchases by income level which are presented in table ',\n",
       " 'source authors elaboration on iri data']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_pred_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc_id = test_example_names[1]\n",
    "sentences = test_process_doc(test_doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check model using eli5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create NER Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this report describes how the education system in the united states compares with education systems in the other group of eight g countries canada france germany italy japan the russian federation the united kingdom that are among the world s most economically developed countries and among the united states largest economic partners']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, list found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-93-128247e0bcb3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: sequence item 0: expected str instance, list found"
     ]
    }
   ],
   "source": [
    "text = ' '.join(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'genetic and neuroimaging data on a sub sample of individuals from the data set were used to test this association'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = pos_sentences_processed[4]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('genetic and neuroimaging', 0.020744656539556924)\n",
      "('test this association', 0.020744656539556924)\n",
      "('sample of individuals', 0.03439110359613085)\n",
      "('neuroimaging data', 0.05033870131288873)\n",
      "('data set', 0.05033870131288873)\n",
      "('genetic', 0.11145728654016383)\n",
      "('association', 0.11145728654016383)\n"
     ]
    }
   ],
   "source": [
    "language = \"en\"\n",
    "max_ngram_size = 3\n",
    "deduplication_thresold = 0.9\n",
    "deduplication_algo = 'seqm'\n",
    "windowSize = 5\n",
    "numOfKeywords = 7\n",
    "\n",
    "kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_thresold, dedupFunc=deduplication_algo, windowsSize=windowSize, top=numOfKeywords, features=None)\n",
    "keywords = kw_extractor.extract_keywords(text)\n",
    "\n",
    "for kw in keywords:\n",
    "\tprint(kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "e68e0ea63623fb9ac5793b248651fce59782b1ca0d072c96c440758c36acff31"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
