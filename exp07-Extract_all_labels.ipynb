{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd01b4c7016e99d31c2e7c892573dc93dbd4548eb0a0f5dca22fbf3a690830b4e66",
   "display_name": "Python 3.8.8 64-bit ('torch': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\ozano\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import random\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "import pickle\n",
    "\n",
    "train_example_names = [fn.split('.')[0] for fn in os.listdir('data/train')]\n",
    "test_example_names = [fn.split('.')[0] for fn in os.listdir('data/test')]\n",
    "\n",
    "metadata = pd.read_csv('data/train.csv')\n",
    "docIdx = train_example_names.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_example_by_name(name):\n",
    "    doc_path = os.path.join('data/train', name + '.json')\n",
    "    with open(doc_path) as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def text_cleaning(text):\n",
    "    text = re.sub('[^A-Za-z0-9]+', ' ', str(text)).strip() # remove unnecessary literals\n",
    "\n",
    "    # remove extra spaces\n",
    "    text = re.sub(\"\\s+\",\" \", text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "def process_doc(doc_id):\n",
    "    doc_json = load_train_example_by_name(doc_id)\n",
    "    doc_text = ' '.join([sec['text'] for sec in doc_json])\n",
    "\n",
    "    # Tokenize sentencewise\n",
    "    sentences = sent_tokenize(doc_text)\n",
    "    return sentences\n",
    "\n",
    "def get_doc(doc_id):\n",
    "    sents = process_doc(doc_id)\n",
    "    return sents#[text_cleaning(s) for s in sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_tokens = {'s', 'of', 'and', 'in', 'on', 'for', 'from', 'the'}\n",
    "\n",
    "def group_ok(group_df):\n",
    "    if len(group_df) < 2:\n",
    "        return False\n",
    "    \n",
    "    nonconn_count = len(group_df) - group_df.is_conn_token.sum()\n",
    "    if nonconn_count < 2:\n",
    "        return False\n",
    "\n",
    "    if group_df.is_long_token.sum() < 2:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def get_connected_uppercase(tokens):\n",
    "    if len(tokens) < 5:\n",
    "        return []\n",
    "\n",
    "    sent_df = pd.DataFrame({'token': tokens})\n",
    "    sent_df['is_conn_token'] = sent_df.token.apply(lambda x: x.lower() in connection_tokens)\n",
    "    sent_df['is_long_token'] = sent_df.token.str.len() > 4\n",
    "    sent_df['in_name'] = sent_df.apply(lambda x: x.token[0].isupper() if not x.is_conn_token else True, axis = 1)\n",
    "\n",
    "    in_name_pieces = []\n",
    "    groups = sent_df.groupby((sent_df.in_name.shift() != sent_df.in_name).cumsum())\n",
    "    for name, group in groups:\n",
    "        if group.in_name.iloc[0]:\n",
    "            in_name_pieces.append(group)\n",
    "\n",
    "    return [list(p['token'].values) for p in in_name_pieces if group_ok(p)]\n",
    "\n",
    "def has_connected_uppercase(tokens):\n",
    "    if len(tokens) < 5:\n",
    "        return []\n",
    "\n",
    "    group_len = 0\n",
    "    for token in tokens:\n",
    "        token_lower = token.lower()\n",
    "        if len(token) > 1 and token[0].isupper():\n",
    "            if token_lower not in connection_tokens:\n",
    "                group_len += 1\n",
    "                if group_len > 2:\n",
    "                    return True\n",
    "\n",
    "        else:\n",
    "            if token_lower not in connection_tokens:\n",
    "                group_len = 0\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "n_sents: 2201: 100%|██████████| 100/100 [00:01<00:00, 80.98it/s]\n"
     ]
    }
   ],
   "source": [
    "possible_labels = []\n",
    "pbar = tqdm(docIdx[:100])\n",
    "\n",
    "for doc_id in pbar:\n",
    "    sents = get_doc(doc_id)\n",
    "\n",
    "    for s in sents:\n",
    "        if has_connected_uppercase(text_cleaning(s).split(' ')):\n",
    "            possible_labels.append(s)\n",
    "\n",
    "    pbar.set_description(f'n_sents: {len(possible_labels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "possible_labels = list(set(possible_labels))\n",
    "\n",
    "with open(\"data/possible_labels.txt\", \"w\", encoding = 'utf-8') as f:\n",
    "    for l in possible_labels:\n",
    "        f.write(l + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}