{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2020/07/transfer-learning-for-nlp-fine-tuning-bert-for-text-classification/\n",
    "### Model from:\n",
    "\n",
    "https://github.com/allenai/scibert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import random\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "import pickle\n",
    "\n",
    "train_example_names = [fn.split('.')[0] for fn in os.listdir('data/train')]\n",
    "test_example_names = [fn.split('.')[0] for fn in os.listdir('data/test')]\n",
    "\n",
    "metadata = pd.read_csv('data/train.csv')\n",
    "docIdx = train_example_names.copy()\n",
    "\n",
    "connection_tokens = {'s', 'of', 'and', 'in', 'on', 'for', 'from', 'the', 'act', 'coast', 'future', 'system', 'per'}"
   ]
  },
  {
   "source": [
    "## Dataset Name Selection"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "291984\n",
      "59594\n"
     ]
    }
   ],
   "source": [
    "def text_cleaning(text):\n",
    "    text = re.sub('[^A-Za-z]+', ' ', str(text)).strip() # remove unnecessary literals\n",
    "\n",
    "    # remove extra spaces\n",
    "    text = re.sub(\"\\s+\",\" \", text)\n",
    "\n",
    "    return text.lower().strip()\n",
    "\n",
    "def is_name_ok(text):\n",
    "    if len([c for c in text if c.isalnum()]) < 4:\n",
    "        return False\n",
    "    \n",
    "    tokens = [t for t in text.split(' ') if len(t) > 3]\n",
    "    tokens = [t for t in tokens if not t in connection_tokens]\n",
    "    if len(tokens) < 3:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "existing_labels = [text_cleaning(x) for x in metadata['dataset_label']] +\\\n",
    "                  [text_cleaning(x) for x in metadata['dataset_title']] +\\\n",
    "                  [text_cleaning(x) for x in metadata['cleaned_label']]\n",
    "\n",
    "to_remove = [\n",
    "    'frequently asked questions', 'total maximum daily load tmd', 'health care facilities',\n",
    "    'traumatic brain injury', 'north pacific high', 'droplet number concentration', 'great slave lake',\n",
    "    'census block groups'\n",
    "]\n",
    "\n",
    "\"\"\"SPLIT_STEP_SIZE = 2\n",
    "def split_label(text):\n",
    "    tokens = text.split(' ')\n",
    "    if len(tokens) < 6:\n",
    "        return [' '.join(tokens)]\n",
    "        \n",
    "    else:\n",
    "        new_labels = []\n",
    "        for label_size in range(max(4, len(tokens) - 5), len(tokens), SPLIT_STEP_SIZE):\n",
    "            if tokens[label_size - 1].lower() not in connection_tokens:\n",
    "                n_nonconn = len(['' for t in tokens if t.lower() not in connection_tokens])\n",
    "                if n_nonconn > 3:\n",
    "                    new_labels.append(' '.join(tokens[:label_size]))\n",
    "            \n",
    "        return new_labels\"\"\"\n",
    "\n",
    "df = pd.read_csv(r'D:\\projects\\kaggle_coleridge_initiative\\data\\gov_data_280000.csv')\n",
    "print(len(df))\n",
    "\n",
    "\n",
    "df['title'] = df.title.apply(text_cleaning)\n",
    "titles = list(df.title.unique())\n",
    "titles = [t for t in titles if not t in to_remove]\n",
    "df = pd.DataFrame({'title': titles})\n",
    "df = df.loc[df.title.apply(is_name_ok)]\n",
    "df = pd.concat([df, pd.DataFrame({'title': existing_labels})], ignore_index= True).reset_index(drop = True)\n",
    "titles = list(df.title.unique())\n",
    "df = pd.DataFrame({'title': titles})\n",
    "df['title'] = df.title.apply(text_cleaning)\n",
    "\n",
    "# Sort labels by length in ascending order\n",
    "existing_labels = sorted(list(df.title.values), key = len, reverse = True)\n",
    "existing_labels = [l for l in existing_labels if len(l.split(' ')) < 10]\n",
    "del df\n",
    "\n",
    "\"\"\"extended_existing_labels = []\n",
    "for l in tqdm(existing_labels):\n",
    "    extended_existing_labels.extend(split_label(l))\n",
    "\n",
    "existing_labels = extended_existing_labels\n",
    "existing_labels = list(set(existing_labels))\n",
    "existing_labels = sorted(existing_labels, key = len, reverse = True)\"\"\"\n",
    "\n",
    "print(len(existing_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataframe for tokens and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_example_by_name(name):\n",
    "    doc_path = os.path.join('data/train', name + '.json')\n",
    "    with open(doc_path) as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def load_test_example_by_name(name):\n",
    "    doc_path = os.path.join('data/test', name + '.json')\n",
    "    with open(doc_path) as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning_upper(text):\n",
    "    text = re.sub('[^A-Za-z]+', ' ', str(text)).strip() # remove unnecessary literals\n",
    "\n",
    "    # remove extra spaces\n",
    "    text = re.sub(\"\\s+\",\" \", text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "def has_connected_uppercase(tokens):\n",
    "    if len(tokens) < 5:\n",
    "        return []\n",
    "\n",
    "    group_len = 0\n",
    "    for token in tokens:\n",
    "        token_lower = token.lower()\n",
    "        if token[0].isupper():\n",
    "            if token_lower not in connection_tokens:\n",
    "                group_len += 1\n",
    "                if group_len > 2:\n",
    "                    return True\n",
    "\n",
    "        else:\n",
    "            if token_lower not in connection_tokens:\n",
    "                group_len = 0\n",
    "\n",
    "    return False\n",
    "\n",
    "def sent_has_acronym(tokens):\n",
    "    # Acronym check\n",
    "    for token in tokens:\n",
    "        if len(token) > 3 and token.isupper():\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def sent_is_candidate(clean_sentence):\n",
    "    tokens = clean_sentence.split(' ')\n",
    "    \n",
    "    if sent_has_acronym(tokens):\n",
    "        return True\n",
    "    else:\n",
    "        return has_connected_uppercase(tokens)\n",
    "        "
   ]
  },
  {
   "source": [
    "pos_sentences = []\n",
    "neg_sentences = []\n",
    "docs_no_pos = []\n",
    "total_sentences = 0\n",
    "\n",
    "\n",
    "\n",
    "def process_doc(doc_id):\n",
    "    \"\"\" Accept sentences with acronyms or uppercase words in succession as candidates.\n",
    "    From those candidates, positives are the ones that contain a label.\n",
    "\n",
    "    \"\"\"\n",
    "    global total_sentences\n",
    "    doc_json = load_train_example_by_name(doc_id)\n",
    "    doc_text = ' '.join([sec['text'] for sec in doc_json])\n",
    "    doc_has_pos = False\n",
    "\n",
    "    # Tokenize sentencewise\n",
    "    sentences = sent_tokenize(doc_text)\n",
    "    total_sentences += len(sentences)\n",
    "\n",
    "    for sentence in sentences:\n",
    "        clean_sentence = text_cleaning_upper(sentence)\n",
    "        is_candidate = sent_is_candidate(clean_sentence)\n",
    "\n",
    "        has_label = False\n",
    "        if is_candidate:\n",
    "            clean_sentence_lower = clean_sentence.lower()\n",
    "            for clean_label in existing_labels:\n",
    "                if clean_label in clean_sentence_lower:\n",
    "                    has_label = True\n",
    "                    break\n",
    "        \n",
    "        # Store sentence in list if candidate\n",
    "        # Non-candidate sentences are discarded\n",
    "        if has_label:\n",
    "            pos_sentences.append(sentence)\n",
    "            doc_has_pos = True\n",
    "        elif is_candidate:\n",
    "            neg_sentences.append(sentence)\n",
    "\n",
    "    if not doc_has_pos:\n",
    "        docs_no_pos.append(doc_id)\n",
    "\n",
    "#process_doc('0026563b-d5b3-417d-bd25-7656b97a044f')"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and Save Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "pos_size: 85, neg_size: 1182, no pos label doc: 0, n_sentences: 6066:   0%|          | 24/14316 [00:14<2:22:20,  1.67it/s]\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3427, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-6-bde752d6b0ef>\", line 11, in <module>\n",
      "    process_doc(doc_id)\n",
      "  File \"<ipython-input-5-515ec2b42963>\", line 40, in process_doc\n",
      "    neg_sentences.append(sentence)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2054, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\inspect.py\", line 1503, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\inspect.py\", line 1461, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\inspect.py\", line 751, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\inspect.py\", line 720, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\inspect.py\", line 705, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3427, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-6-bde752d6b0ef>\", line 11, in <module>\n",
      "    process_doc(doc_id)\n",
      "  File \"<ipython-input-5-515ec2b42963>\", line 40, in process_doc\n",
      "    neg_sentences.append(sentence)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2054, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3347, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3444, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2056, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(etype,\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1367, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1267, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1124, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2054, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\inspect.py\", line 1503, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\inspect.py\", line 1461, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\inspect.py\", line 751, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\inspect.py\", line 720, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\inspect.py\", line 705, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-bde752d6b0ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc_id\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mprocess_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     pbar.set_description(\\\n",
      "\u001b[1;32m<ipython-input-5-515ec2b42963>\u001b[0m in \u001b[0;36mprocess_doc\u001b[1;34m(doc_id)\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_candidate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m             \u001b[0mneg_sentences\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2053\u001b[0m                         \u001b[1;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2054\u001b[1;33m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2055\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_ast_nodes\u001b[1;34m(self, nodelist, cell_name, interactivity, compiler, result)\u001b[0m\n\u001b[0;32m   3346\u001b[0m                         \u001b[0masy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3347\u001b[1;33m                     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_code\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0masync_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0masy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3348\u001b[0m                         \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2055\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2056\u001b[1;33m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[0;32m   2057\u001b[0m                                             value, tb, tb_offset=tb_offset)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1367\u001b[1;33m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[0;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1266\u001b[0m             \u001b[1;31m# Verbose modes need a full traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1267\u001b[1;33m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[0;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1124\u001b[1;33m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[0;32m   1125\u001b[0m                                                                tb_offset)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1082\u001b[1;33m         \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[1;34m(etype, value, records)\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2053\u001b[0m                         \u001b[1;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2054\u001b[1;33m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2055\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TypeError' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\async_helpers.py\u001b[0m in \u001b[0;36m_pseudo_sync_runner\u001b[1;34m(coro)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \"\"\"\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m         \u001b[0mcoro\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_async\u001b[1;34m(self, raw_cell, store_history, silent, shell_futures, transformed_cell, preprocessing_exc_tuple)\u001b[0m\n\u001b[0;32m   3153\u001b[0m                     \u001b[0minteractivity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'async'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3155\u001b[1;33m                 has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n\u001b[0m\u001b[0;32m   3156\u001b[0m                        interactivity=interactivity, compiler=compiler, result=result)\n\u001b[0;32m   3157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_ast_nodes\u001b[1;34m(self, nodelist, cell_name, interactivity, compiler, result)\u001b[0m\n\u001b[0;32m   3364\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3365\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_before_exec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3366\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowtraceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3367\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2054\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2055\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2056\u001b[1;33m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[0;32m   2057\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[0;32m   2058\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1365\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1367\u001b[1;33m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[0;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0;32m   1369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1265\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1266\u001b[0m             \u001b[1;31m# Verbose modes need a full traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1267\u001b[1;33m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[0;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1269\u001b[0m             )\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1140\u001b[0m         \u001b[0mchained_exc_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mevalue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1142\u001b[1;33m             formatted_exceptions += self.format_exception_as_a_whole(etype, evalue, etb, lines_of_context,\n\u001b[0m\u001b[0;32m   1143\u001b[0m                                                                      chained_exceptions_tb_offset)\n\u001b[0;32m   1144\u001b[0m             \u001b[0mexception\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_parts_of_chained_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1082\u001b[1;33m         \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[1;34m(etype, value, records)\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m     \u001b[1;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "assert len(docIdx) > 0\n",
    "\n",
    "pos_sentences = []\n",
    "neg_sentences = []\n",
    "docs_no_pos = []\n",
    "total_sentences = 0\n",
    "\n",
    "pbar = tqdm(docIdx)\n",
    "for doc_id in pbar:\n",
    "    process_doc(doc_id)\n",
    "    pbar.set_description(\\\n",
    "        f'pos_size: {len(pos_sentences)}, neg_size: {len(neg_sentences)}, no pos label doc: {len(docs_no_pos)}, n_sentences: {total_sentences}')\n",
    "\n",
    "with open(f'data/bert_ner_sentences/pos.pkl', 'wb') as f:\n",
    "    pickle.dump(pos_sentences, f)\n",
    "\n",
    "with open(f'data/bert_ner_sentences/neg.pkl', 'wb') as f:\n",
    "    pickle.dump(neg_sentences, f)\n",
    "\n",
    "print(f'pos size: {len(pos_sentences)}')\n",
    "print(f'neg size: {len(neg_sentences)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metadata.loc[metadata.Id == docs_no_pos[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f'data/bert_ner_sentences/pos.pkl', 'rb') as f:\n",
    "    pos_sentences = pickle.load(f)\n",
    "\n",
    "with open(f'data/bert_ner_sentences/neg.pkl', 'rb') as f:\n",
    "    neg_sentences = pickle.load(f)\n",
    "\n",
    "print(f'pos size: {len(pos_sentences)}')\n",
    "print(f'neg size: {len(neg_sentences)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sentences_processed = []\n",
    "neg_sentences_processed = []\n",
    "pos_labels = []\n",
    "neg_labels = []\n",
    "\n",
    "n_broken_sent = 0\n",
    "n_pos_no_label = 0\n",
    "\n",
    "def convert_tokens(text):\n",
    "    if is_acronym(text):\n",
    "        return 'ACRONYM'\n",
    "    return text\n",
    "\n",
    "def is_acronym(text):\n",
    "    if len(text) < 4:\n",
    "        return False\n",
    "    if text.isupper():\n",
    "        return True\n",
    "\n",
    "def is_text_broken(tokens):\n",
    "    # Some texts are like 'p a dsdv a d a ds f b', remove them\n",
    "    if len(tokens) == 0:\n",
    "        return True\n",
    "\n",
    "    if len(tokens) < 50:\n",
    "        return False\n",
    "\n",
    "    one_char_token_ratio = len([l for l in tokens if len(l) == 1]) / len(tokens)\n",
    "    return one_char_token_ratio > 0.2\n",
    "\n",
    "def split_to_smaller_sent(tokens, s_size, overlap_size):\n",
    "    # output sentences will be s_size + overlap_size long\n",
    "    small_sents = []\n",
    "\n",
    "    if len(tokens) <= s_size:\n",
    "        return [tokens]\n",
    "\n",
    "    n_parts = len(tokens) // s_size\n",
    "    if len(tokens) % s_size != 0:\n",
    "        n_parts += 1\n",
    "\n",
    "    i_part = 0\n",
    "    end_i = 0\n",
    "    while end_i < len(tokens):\n",
    "        start_i = i_part * s_size\n",
    "        if i_part > 0:\n",
    "            start_i -= overlap_size\n",
    "\n",
    "        end_i = min(len(tokens), start_i + s_size)\n",
    "\n",
    "        small_sents.append(tokens[start_i: end_i])\n",
    "        i_part += 1\n",
    "\n",
    "    return small_sents\n",
    "\n",
    "def join_tuple_tokens(tuples):\n",
    "    return ' '.join([t[1] for t in tuples])\n",
    "\n",
    "def get_index(lst, el):\n",
    "    idx = []\n",
    "    for i, lst_el in enumerate(lst):\n",
    "        if el in lst_el:\n",
    "            idx.append(i)\n",
    "\n",
    "    return idx\n",
    "\n",
    "def process_pos_sentence(sentence):\n",
    "    global n_broken_sent\n",
    "    global last_doc_labels\n",
    "\n",
    "    bert_sentence = text_cleaning_upper(sentence)\n",
    "    label_sentence = bert_sentence.lower()\n",
    "\n",
    "    if is_text_broken(label_sentence.split(' ')): # Can't use bert cleaning for this, because all punc.s are padded with spaces\n",
    "        n_broken_sent += 1\n",
    "        return\n",
    "    \n",
    "    bert_tokens = bert_sentence.split(' ')\n",
    "    ### STEP 1: Split into fixed sized sentences ###\n",
    "    for small_sentence_tokens in split_to_smaller_sent(bert_tokens, s_size = 64, overlap_size = 20):\n",
    "\n",
    "        small_bert_sentence = ' '.join(small_sentence_tokens)\n",
    "\n",
    "        # Need to remove punc.s and uppercase letters to find labels\n",
    "        small_label_sentence = small_bert_sentence.lower()\n",
    "\n",
    "        has_label = False\n",
    "        sent_labels = []\n",
    "        ### STEP 2: Match labels ###\n",
    "        # Check if contains labels\n",
    "        for clean_label in existing_labels:\n",
    "            if clean_label in small_label_sentence:\n",
    "                has_label = True\n",
    "\n",
    "                # Remove label from the text, to only match the largest label\n",
    "                small_label_sentence = small_label_sentence.replace(clean_label, '')\n",
    "                sent_labels.append(clean_label)\n",
    "\n",
    "        small_sent_targets = ['O' for _ in range(len(small_sentence_tokens))]\n",
    "\n",
    "        if has_label:\n",
    "            # Tokenize labels for matching\n",
    "            sent_label_tokens = [l.split(' ') for l in sent_labels]\n",
    "\n",
    "            # Get index, token tuples for clean tokens. Indices are for raw tokens\n",
    "            small_sent_tuples = [(i, token.lower()) for i, token in enumerate(small_sentence_tokens) if text_cleaning_upper(token) != '']\n",
    "\n",
    "            ### STEP 3: Set corresponding targets for each label ###\n",
    "            # Target: (B, I, O), Label: adni\n",
    "            for l in sent_labels:\n",
    "                l_tokens = l.split(' ')\n",
    "                small_sent_joined = [join_tuple_tokens(small_sent_tuples[i: i + len(l_tokens)]) for i in range(len(small_sent_tuples) - len(l_tokens) + 1)]\n",
    "\n",
    "                label_start_idx = get_index(small_sent_joined, l) # list of indices\n",
    "                for label_start_i in label_start_idx:\n",
    "                    label_end_i = label_start_i + len(l_tokens) - 1\n",
    "\n",
    "                    target_start_i = small_sent_tuples[label_start_i][0]\n",
    "                    target_end_i = small_sent_tuples[label_end_i][0]\n",
    "\n",
    "                    # Do not use the same tokens for multiple labels\n",
    "                    #small_sent_tuples = small_sent_tuples[:label_start_i] + small_sent_tuples[label_end_i:]\n",
    "\n",
    "                    try:\n",
    "                        if small_sent_targets[target_start_i] == 'O': # If not was already labeled\n",
    "                            small_sent_targets[target_start_i] = 'B'\n",
    "                            if target_end_i - target_start_i > 0:\n",
    "                                for i in range(target_start_i+1, target_end_i+1):\n",
    "                                    small_sent_targets[i] = 'I'\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print('DEBUG')\n",
    "                        print(small_sentence_tokens)\n",
    "                        print(len(small_sentence_tokens))\n",
    "                        print(len(small_sent_targets))\n",
    "                        print(target_start_i)\n",
    "                        print(small_sent_joined)\n",
    "                        print('DEBUG')\n",
    "                        raise e\n",
    "        \n",
    "        ### STEP 4: Add sentence output to lists ###\n",
    "        if has_label:\n",
    "            pos_sentences_processed.append([convert_tokens(t) for t in small_sentence_tokens])\n",
    "            pos_labels.append(small_sent_targets)\n",
    "        \"\"\"else:\n",
    "            neg_sentences_processed.append(small_sentence_tokens)\n",
    "            neg_labels.append(small_sent_targets)\"\"\"\n",
    "\n",
    "def process_neg_sentence(sentence):\n",
    "    global n_broken_sent\n",
    "    \n",
    "    bert_sentence = text_cleaning_upper(sentence)\n",
    "    label_sentence = bert_sentence.lower()\n",
    "\n",
    "    if is_text_broken(label_sentence.split(' ')): # Can't use bert cleaning for this, because all punc.s are padded with spaces\n",
    "        n_broken_sent += 1\n",
    "        return\n",
    "\n",
    "    bert_tokens = bert_sentence.split(' ')\n",
    "    \n",
    "    ### STEP 1: Split into fixed sized sentences ###\n",
    "    for small_sentence_tokens in split_to_smaller_sent(bert_tokens, s_size = 64, overlap_size = 20):\n",
    "        small_sent_targets = ['O' for _ in range(len(bert_tokens))]\n",
    "\n",
    "        neg_sentences_processed.append([convert_tokens(t) for t in small_sentence_tokens])\n",
    "        neg_labels.append(small_sent_targets)\n",
    "\n",
    "#process_pos_sentence(pos_sentences[2472])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create NER Dataset and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert len(pos_sentences) > 0\n",
    "\n",
    "pos_sentences_processed = []\n",
    "neg_sentences_processed = []\n",
    "pos_labels = []\n",
    "neg_labels = []\n",
    "\n",
    "n_pos_no_label = 0\n",
    "n_broken_sent = 0\n",
    "\n",
    "for sent in tqdm(pos_sentences):\n",
    "    process_pos_sentence(sent)\n",
    "\n",
    "for sent in tqdm(neg_sentences):\n",
    "    process_neg_sentence(sent)\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(f'data/bert_ner_data/pos.pkl', 'wb') as f:\n",
    "    pickle.dump(pos_sentences_processed, f)\n",
    "\n",
    "with open(f'data/bert_ner_data/neg.pkl', 'wb') as f:\n",
    "    pickle.dump(neg_sentences_processed, f)\n",
    "\n",
    "with open(f'data/bert_ner_data/pos_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(pos_labels, f)\n",
    "\n",
    "with open(f'data/bert_ner_data/neg_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(neg_labels, f)\n",
    "\n",
    "\n",
    "print('')\n",
    "print(f'broken sentences: {n_broken_sent}')\n",
    "print(f'n_pos_no_label: {n_pos_no_label}')\n",
    "print(f'pos_proc size: {len(pos_sentences_processed)}')\n",
    "print(f'neg_proc size: {len(neg_sentences_processed)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load NER Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f'data/bert_ner_data/pos.pkl', 'rb') as f:\n",
    "    pos_sentences_processed = pickle.load(f)\n",
    "\n",
    "with open(f'data/bert_ner_data/neg.pkl', 'rb') as f:\n",
    "    neg_sentences_processed = pickle.load(f)\n",
    "\n",
    "with open(f'data/bert_ner_data/pos_labels.pkl', 'rb') as f:\n",
    "    pos_labels = pickle.load(f)\n",
    "\n",
    "with open(f'data/bert_ner_data/neg_labels.pkl', 'rb') as f:\n",
    "    neg_labels = pickle.load(f)\n",
    "\n",
    "print(f'pos size: {len(pos_sentences_processed)}')\n",
    "print(f'neg size: {len(neg_sentences_processed)}')\n",
    "print(f'pos label size: {len(pos_labels)}')\n",
    "print(f'neg label size: {len(neg_labels)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_target(x, lst):\n",
    "    if x['label'].iloc[0] == 'O':\n",
    "        # if not a dataset name, do not augment\n",
    "        lst.append(x)\n",
    "    else:\n",
    "        random_name_tokens = random.choice(existing_labels).split(' ')\n",
    "        random_name_tokens = [r[0].upper() + r[1:] if not r.lower() in connection_tokens else r for r in random_name_tokens]\n",
    "\n",
    "        new_x = pd.DataFrame()\n",
    "        # Replace tokens\n",
    "        new_x['token'] = random_name_tokens\n",
    "        new_x['label'] = 'I'\n",
    "        new_x.loc[new_x.index == 0, 'label'] = 'B'\n",
    "        lst.append(new_x)\n",
    "\n",
    "def augment_sentence(tokens, labels, augment_chance = 0.9):\n",
    "    if random.uniform(0,1) > augment_chance:\n",
    "        # No augmentation\n",
    "        return tokens, labels\n",
    "\n",
    "    df_pieces = []\n",
    "    sent_df = pd.DataFrame({'token': tokens, 'label': labels})\n",
    "    sent_df['label_o'] = sent_df.label == 'O'\n",
    "\n",
    "    gb = sent_df.groupby((sent_df['label_o'].shift() != sent_df['label_o']).cumsum())\n",
    "    for name, group in gb:\n",
    "        replace_target(group, df_pieces)\n",
    "\n",
    "    sent_df = pd.concat(df_pieces, ignore_index = True, axis = 0)\n",
    "\n",
    "    return list(sent_df.token.values), list(sent_df.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sentences_processed_aug = []\n",
    "pos_labels_aug = []\n",
    "\n",
    "for _ in range(5):\n",
    "    for s_tokens, s_labels in tqdm(zip(pos_sentences_processed, pos_labels), total = len(pos_labels)):\n",
    "        aug_tokens, aug_labels = augment_sentence(s_tokens, s_labels)\n",
    "        pos_sentences_processed_aug.append(aug_tokens)\n",
    "        pos_labels_aug.append(aug_labels)\n",
    "\n",
    "pos_sentences_processed = pos_sentences_processed_aug\n",
    "pos_labels = pos_labels_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sentences_processed_aug[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "neg_size = 500000\n",
    "neg_idx = np.random.permutation(len(neg_labels))\n",
    "neg_sentences_processed = [neg_sentences_processed[i] for i in neg_idx[:neg_size]]\n",
    "neg_labels = [neg_labels[i] for i in neg_idx[:neg_size]]\n",
    "\n",
    "sentences = pos_sentences_processed + neg_sentences_processed\n",
    "labels = pos_labels + neg_labels\n",
    "\n",
    "del pos_sentences_processed\n",
    "del neg_sentences_processed\n",
    "del pos_labels\n",
    "del neg_labels\n",
    "\n",
    "\"\"\"print('Splitting data...')\n",
    "train_sents, val_sents, train_labels, val_labels = train_test_split(sentences, labels, test_size=0.20, random_state=42)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tune Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import csv\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "import statistics as stats\n",
    "from bert_sklearn import BertTokenClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building sklearn token classifier...\n"
     ]
    }
   ],
   "source": [
    "model = BertTokenClassifier(bert_model='scibert_scivocab_cased',\n",
    "                             num_mlp_hiddens= 500,\n",
    "                             max_seq_length=150, \n",
    "                             epochs=1,\n",
    "                             #gradient accumulation\n",
    "                             gradient_accumulation_steps=4,\n",
    "                             learning_rate=3e-5,\n",
    "                             train_batch_size=8,#batch size for training\n",
    "                             eval_batch_size=8, #batch size for evaluation\n",
    "                             validation_fraction=0.15, \n",
    "                             #ignore the tokens with label O\n",
    "                             ignore_label=['O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ozano\\.conda\\envs\\torch\\lib\\site-packages\\bert_sklearn\\utils.py:26: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(X)\n",
      "100%|| 213450/213450 [00:03<00:00, 63214.25B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading bert-base-cased model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 435779157/435779157 [03:08<00:00, 2309556.37B/s]\n",
      "100%|| 433/433 [00:00<00:00, 108206.25B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to linear classifier/regressor\n",
      "Loading Pytorch checkpoint\n",
      "train data size: 782136, validation data size: 138024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training  :   0%|                                                  | 3/391068 [01:28<2174:31:13, 20.02s/it, loss=0.401]C:\\Users\\ozano\\.conda\\envs\\torch\\lib\\site-packages\\bert_sklearn\\model\\pytorch_pretrained\\optimization.py:275: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:1005.)\n",
      "  next_m.mul_(beta1).add_(1 - beta1, grad)\n",
      "Training  :  51%|                     | 198471/391068 [8:09:34<7:01:26,  7.62it/s, loss=0.00272]"
     ]
    }
   ],
   "source": [
    "model.fit(sentences, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to disk\n",
    "savefile='data/sklearn_bert_ner_cased.bin'\n",
    "model.save(savefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_sklearn import load_model\n",
    "bert_model = load_model(r'data/sklearn_bert_ner_cased.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = model.predict(val_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_i = 101\n",
    "pd.DataFrame({'token': val_sents[ex_i], 'pred':val_preds[ex_i]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ex_sent = val_sents[101]\n",
    "ex_sent = neg_sentences_processed[10]\n",
    "ex_pred = model.predict([ex_sent])\n",
    "\n",
    "pd.DataFrame({'token': ex_sent, 'pred':ex_pred[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sentences_processed[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd0b69a98d3df882577ba469635c4ab08c5ae67eaedfd3a57f311f98966a6edb2d0",
   "display_name": "Python 3.8.5 64-bit ('torch': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "aac00fb1da9c94ab374af15aaaf3bdbaafa792d2239349bc70bec9f747decd69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}