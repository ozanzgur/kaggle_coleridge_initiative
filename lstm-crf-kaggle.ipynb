{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import random\nimport torch\nimport numpy as np\nimport argparse\nimport os\nimport time\nfrom torch.utils.data import DataLoader\nimport torch.optim as optim","metadata":{"execution":{"iopub.status.busy":"2021-05-22T14:32:21.042955Z","iopub.execute_input":"2021-05-22T14:32:21.043363Z","iopub.status.idle":"2021-05-22T14:32:22.257655Z","shell.execute_reply.started":"2021-05-22T14:32:21.043280Z","shell.execute_reply":"2021-05-22T14:32:22.256845Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ntest_data_dir = r'/kaggle/input/coleridgeinitiative-show-us-the-data/test'\ntest_example_names = [fn.split('.')[0] for fn in os.listdir(test_data_dir)]\n\nsample_sub = pd.read_csv('../input/coleridgeinitiative-show-us-the-data/sample_submission.csv')\nmetadata = pd.read_csv('../input/coleridgeinitiative-show-us-the-data/train.csv')\nlabels = list(metadata.cleaned_label.unique())\nlabels = [l.strip() for l in labels]\n\n# Sort labels by length (desc)\nlabels = sorted(labels, key = len, reverse = True)\n\nsample_sub","metadata":{"execution":{"iopub.status.busy":"2021-05-22T14:37:51.255789Z","iopub.execute_input":"2021-05-22T14:37:51.256108Z","iopub.status.idle":"2021-05-22T14:37:51.393518Z","shell.execute_reply.started":"2021-05-22T14:37:51.256079Z","shell.execute_reply":"2021-05-22T14:37:51.392727Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"                                     Id  PredictionString\n0  2100032a-7c33-4bff-97ef-690822c43466               NaN\n1  2f392438-e215-4169-bebf-21ac4ff253e1               NaN\n2  3f316b38-1a24-45a9-8d8c-4e05a42257c6               NaN\n3  8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60               NaN","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>PredictionString</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2100032a-7c33-4bff-97ef-690822c43466</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2f392438-e215-4169-bebf-21ac4ff253e1</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3f316b38-1a24-45a9-8d8c-4e05a42257c6</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8e6996b4-ca08-4c0b-bed2-aaf07a4c6a60</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom torch.nn.utils.rnn import pad_sequence\nimport pickle\n\n\ndef normalize_word(word):\n    new_word = \"\"\n    for char in word:\n        if char.isdigit():\n            new_word += '0'\n        else:\n            new_word += char\n    return new_word\n\nclass WordVocabulary(object):\n    def __init__(self, number_normalized):\n        self.number_normalized = number_normalized\n        self._pad = -1\n        self._unk = -1\n        self.index = 0\n\n        self._pad = self.index\n        self.index += 1\n        self._unk = self.index\n        self.index += 1\n        \n        self.load()\n\n    def unk(self):\n        return self._unk\n\n    def pad(self):\n        return self._pad\n\n    def size(self):\n        return len(self._id_to_word)\n\n    def word_to_id(self, word):\n        if word in self._word_to_id:\n            return self._word_to_id[word]\n        return self.unk()\n\n    def id_to_word(self, cur_id):\n        return self._id_to_word[cur_id]\n\n    def items(self):\n        return self._word_to_id.items()\n    \n    def load(self):\n        with open('../input/coleridgevocabulary/word_to_id.pickle', 'rb') as handle:\n            self._word_to_id = pickle.load(handle)\n        with open('../input/coleridgevocabulary/id_to_word.pickle', 'rb') as handle:\n            self._id_to_word= pickle.load(handle)\n\n\nclass LabelVocabulary(object):\n    def __init__(self):\n        self._pad = -1\n        self.index = 0\n\n        self._pad = self.index\n        self.index += 1\n        \n        self.load()\n\n    def pad(self):\n        return self._pad\n\n    def size(self):\n        return len(self._id_to_label)\n\n    def label_to_id(self, label):\n        return self._label_to_id[label]\n\n    def id_to_label(self, cur_id):\n        return self._id_to_label[cur_id]\n    \n    def load(self):\n        with open('../input/coleridgevocabulary/id_to_label.pickle', 'rb') as handle:\n            self._id_to_label = pickle.load(handle)\n        with open('../input/coleridgevocabulary/label_to_id.pickle', 'rb') as handle:\n            self._label_to_id= pickle.load(handle)\n\n\nclass Alphabet(object):\n    def __init__(self):\n        self._pad = -1\n        self._unk = -1\n        self.index = 0\n\n        self._pad = self.index\n        self.index += 1\n\n        self._unk = self.index\n        self.index += 1\n        \n        self.load()\n\n    def pad(self):\n        return self._pad\n\n    def unk(self):\n        return self._unk\n\n    def size(self):\n        return len(self._id_to_char)\n\n    def char_to_id(self, char):\n        if char in self._char_to_id:\n            return self._char_to_id[char]\n        return self.unk()\n\n    def id_to_char(self, cur_id):\n        return self._id_to_char[cur_id]\n\n    def items(self):\n        return self._char_to_id.items()\n    \n    def load(self):\n        with open('../input/coleridgevocabulary/id_to_char.pickle', 'rb') as handle:\n            self._id_to_char = pickle.load(handle)\n        with open('../input/coleridgevocabulary/char_to_id.pickle', 'rb') as handle:\n            self._char_to_id= pickle.load(handle)\n\n\ndef my_collate(key, batch_tensor):\n    if key == 'char':\n        batch_tensor = pad_char(batch_tensor)\n        return batch_tensor\n    else:\n        word_seq_lengths = torch.LongTensor(list(map(len, batch_tensor)))\n        _, word_perm_idx = word_seq_lengths.sort(0, descending=True)\n        batch_tensor.sort(key=lambda x: len(x), reverse=True)\n        tensor_length = [len(sq) for sq in batch_tensor]\n        batch_tensor = pad_sequence(batch_tensor, batch_first=True, padding_value=0)\n        return batch_tensor, tensor_length, word_perm_idx\n\n\ndef my_collate_fn(batch):\n    return {key: my_collate(key, [d[key] for d in batch]) for key in batch[0]}\n\n\ndef pad_char(chars):\n    batch_size = len(chars)\n    max_seq_len = max(map(len, chars))\n    pad_chars = [chars[idx] + [[0]] * (max_seq_len - len(chars[idx])) for idx in range(len(chars))]\n    length_list = [list(map(len, pad_char)) for pad_char in pad_chars]\n    max_word_len = max(map(max, length_list))\n    char_seq_tensor = torch.zeros((batch_size, max_seq_len, max_word_len)).long()\n    char_seq_lengths = torch.LongTensor(length_list)\n    for idx, (seq, seqlen) in enumerate(zip(pad_chars, char_seq_lengths)):\n        for idy, (word, wordlen) in enumerate(zip(seq, seqlen)):\n            char_seq_tensor[idx, idy, :wordlen] = torch.LongTensor(word)\n\n    return char_seq_tensor\n\n\ndef load_pretrain_emb(embedding_path):\n    embedd_dim = 100\n    embedd_dict = dict()\n    with open(embedding_path, 'r', encoding=\"utf8\") as file:\n        for line in file:\n            line = line.strip()\n            if len(line) == 0:\n                continue\n            tokens = line.split()\n            if not embedd_dim + 1 == len(tokens):\n                continue\n            embedd = np.empty([1, embedd_dim])\n            embedd[:] = tokens[1:]\n            first_col = tokens[0]\n            embedd_dict[first_col] = embedd\n    return embedd_dict, embedd_dim\n\n\ndef build_pretrain_embedding(embedding_path, word_vocab, embedd_dim=100):\n    embedd_dict = dict()\n    if embedding_path is not None:\n        embedd_dict, embedd_dim = load_pretrain_emb(embedding_path)\n    vocab_size = word_vocab.size()\n    scale = np.sqrt(3.0 / embedd_dim)\n    pretrain_emb = np.empty([word_vocab.size(), embedd_dim])\n    perfect_match = 0\n    case_match = 0\n    not_match = 0\n    for word, index in word_vocab.items():\n        if word in embedd_dict:\n            pretrain_emb[index, :] = embedd_dict[word]\n            perfect_match += 1\n        elif word.lower() in embedd_dict:\n            pretrain_emb[index, :] = embedd_dict[word.lower()]\n            case_match += 1\n        else:\n            pretrain_emb[index, :] = np.random.uniform(-scale, scale, [1, embedd_dim])\n            not_match += 1\n\n    pretrain_emb[0, :] = np.zeros((1, embedd_dim))\n    pretrained_size = len(embedd_dict)\n    print(\"Embedding:\\n     pretrain word:%s, prefect match:%s, case_match:%s, oov:%s, oov%%:%s\" % (\n        pretrained_size, perfect_match, case_match, not_match, (not_match + 0.) / vocab_size))\n    return pretrain_emb\n\n\ndef lr_decay(optimizer, epoch, decay_rate, init_lr):\n    lr = init_lr / (1 + decay_rate * epoch)\n    print(\" Learning rate is set as:\", lr)\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n    return optimizer\n\n\ndef get_mask(batch_tensor):\n    mask = batch_tensor.eq(0)\n    mask = mask.eq(0)\n    return mask\n","metadata":{"execution":{"iopub.status.busy":"2021-05-22T14:32:30.060027Z","iopub.execute_input":"2021-05-22T14:32:30.060379Z","iopub.status.idle":"2021-05-22T14:32:30.096266Z","shell.execute_reply.started":"2021-05-22T14:32:30.060347Z","shell.execute_reply":"2021-05-22T14:32:30.095455Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def process_tokens(tokens, word_vocab, alphabet, number_normalized):\n    tokens = [t.strip() for t in tokens]\n    \n    if number_normalized:\n        tokens = [normalize_word(t) for t in tokens]\n        \n    token_ids = [self.word_vocab.word_to_id(t) for t in tokens]\n    tokens_tensor = torch.tensor(token_ids).long()\n    \n    seq_char_list = list()\n    for token in tokens:\n        char_ids = [self.alphabet.char_to_id(c) for c in token]\n        seq_char_list.append(char_ids)\n        \n    chars_tensor = torch.tensor(seq_char_list).long()\n    return {'text': tokens_tensor, 'char': chars_tensor}","metadata":{"execution":{"iopub.status.busy":"2021-05-22T14:32:31.035662Z","iopub.execute_input":"2021-05-22T14:32:31.036071Z","iopub.status.idle":"2021-05-22T14:32:31.042735Z","shell.execute_reply.started":"2021-05-22T14:32:31.036029Z","shell.execute_reply":"2021-05-22T14:32:31.041716Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from __future__ import print_function\nimport torch\nimport torch.autograd as autograd\nimport torch.nn as nn\n\nSTART_TAG = -2\nSTOP_TAG = -1\n\n\n# Compute log sum exp in a numerically stable way for the forward algorithm\ndef log_sum_exp(vec, m_size):\n    \"\"\"\n    calculate log of exp sum\n    args:\n        vec (batch_size, vanishing_dim, hidden_dim) : input tensor\n        m_size : hidden_dim\n    return:\n        batch_size, hidden_dim\n    \"\"\"\n    _, idx = torch.max(vec, 1)  # B * 1 * M\n    max_score = torch.gather(vec, 1, idx.view(-1, 1, m_size)).view(-1, 1, m_size)  # B * M\n    return max_score.view(-1, m_size) + torch.log(torch.sum(torch.exp(vec - max_score.expand_as(vec)), 1)).view(-1,\n                                                                                                                m_size)  # B * M\n\n\nclass CRF(nn.Module):\n\n    def __init__(self, tagset_size, gpu):\n        super(CRF, self).__init__()\n        print(\"build CRF...\")\n        self.gpu = gpu\n        # Matrix of transition parameters.  Entry i,j is the score of transitioning from i to j.\n        self.tagset_size = tagset_size\n        # # We add 2 here, because of START_TAG and STOP_TAG\n        # # transitions (f_tag_size, t_tag_size), transition value from f_tag to t_tag\n        init_transitions = torch.zeros(self.tagset_size + 2, self.tagset_size + 2)\n        init_transitions[:, START_TAG] = -10000.0\n        init_transitions[STOP_TAG, :] = -10000.0\n        init_transitions[:, 0] = -10000.0\n        init_transitions[0, :] = -10000.0\n        # if self.gpu:\n        #     init_transitions = init_transitions.cuda()\n        self.transitions = nn.Parameter(init_transitions)\n\n        # self.transitions = nn.Parameter(torch.Tensor(self.tagset_size+2, self.tagset_size+2))\n        # self.transitions.data.zero_()\n\n    def _calculate_PZ(self, feats, mask):\n        \"\"\"\n            input:\n                feats: (batch, seq_len, self.tag_size+2)\n                masks: (batch, seq_len)\n        \"\"\"\n        batch_size = feats.size(0)\n        seq_len = feats.size(1)\n        tag_size = feats.size(2)\n        # print feats.view(seq_len, tag_size)\n        assert (tag_size == self.tagset_size + 2)\n        mask = mask.transpose(1, 0).contiguous()\n        ins_num = seq_len * batch_size\n        ## be careful the view shape, it is .view(ins_num, 1, tag_size) but not .view(ins_num, tag_size, 1)\n        feats = feats.transpose(1, 0).contiguous().view(ins_num, 1, tag_size).expand(ins_num, tag_size, tag_size)\n        ## need to consider start\n        scores = feats + self.transitions.view(1, tag_size, tag_size).expand(ins_num, tag_size, tag_size)\n        scores = scores.view(seq_len, batch_size, tag_size, tag_size)\n        # build iter\n        seq_iter = enumerate(scores)\n        _, inivalues = next(seq_iter)  # bat_size * from_target_size * to_target_size\n        # only need start from start_tag\n        partition = inivalues[:, START_TAG, :].clone().view(batch_size, tag_size, 1)  # bat_size * to_target_size\n\n        ## add start score (from start to all tag, duplicate to batch_size)\n        # partition = partition + self.transitions[START_TAG,:].view(1, tag_size, 1).expand(batch_size, tag_size, 1)\n        # iter over last scores\n        for idx, cur_values in seq_iter:\n            # previous to_target is current from_target\n            # partition: previous results log(exp(from_target)), #(batch_size * from_target)\n            # cur_values: bat_size * from_target * to_target\n\n            cur_values = cur_values + partition.contiguous().view(batch_size, tag_size, 1).expand(batch_size, tag_size,\n                                                                                                  tag_size)\n            cur_partition = log_sum_exp(cur_values, tag_size)\n            # print cur_partition.data\n\n            # (bat_size * from_target * to_target) -> (bat_size * to_target)\n            # partition = utils.switch(partition, cur_partition, mask[idx].view(bat_size, 1).expand(bat_size, self.tagset_size)).view(bat_size, -1)\n            mask_idx = mask[idx, :].view(batch_size, 1).expand(batch_size, tag_size)\n\n            ## effective updated partition part, only keep the partition value of mask value = 1\n            masked_cur_partition = cur_partition.masked_select(mask_idx)\n            ## let mask_idx broadcastable, to disable warning\n            mask_idx = mask_idx.contiguous().view(batch_size, tag_size, 1)\n\n            ## replace the partition where the maskvalue=1, other partition value keeps the same\n            partition.masked_scatter_(mask_idx, masked_cur_partition)\n        # until the last state, add transition score for all partition (and do log_sum_exp) then select the value in STOP_TAG\n        cur_values = self.transitions.view(1, tag_size, tag_size).expand(batch_size, tag_size,\n                                                                         tag_size) + partition.contiguous().view(\n            batch_size, tag_size, 1).expand(batch_size, tag_size, tag_size)\n        cur_partition = log_sum_exp(cur_values, tag_size)\n        final_partition = cur_partition[:, STOP_TAG]\n        return final_partition.sum(), scores\n\n    def _viterbi_decode(self, feats, mask):\n        \"\"\"\n            input:\n                feats: (batch, seq_len, self.tag_size+2)\n                mask: (batch, seq_len)\n            output:\n                decode_idx: (batch, seq_len) decoded sequence\n                path_score: (batch, 1) corresponding score for each sequence (to be implementated)\n        \"\"\"\n        batch_size = feats.size(0)\n        seq_len = feats.size(1)\n        tag_size = feats.size(2)\n        assert (tag_size == self.tagset_size + 2)\n        ## calculate sentence length for each sentence\n        length_mask = torch.sum(mask.long(), dim=1).view(batch_size, 1).long()\n        ## mask to (seq_len, batch_size)\n        mask = mask.transpose(1, 0).contiguous()\n        ins_num = seq_len * batch_size\n        ## be careful the view shape, it is .view(ins_num, 1, tag_size) but not .view(ins_num, tag_size, 1)\n        feats = feats.transpose(1, 0).contiguous().view(ins_num, 1, tag_size).expand(ins_num, tag_size, tag_size)\n        ## need to consider start\n        scores = feats + self.transitions.view(1, tag_size, tag_size).expand(ins_num, tag_size, tag_size)\n        scores = scores.view(seq_len, batch_size, tag_size, tag_size)\n\n        # build iter\n        seq_iter = enumerate(scores)\n        ## record the position of best score\n        back_points = list()\n        partition_history = list()\n        ##  reverse mask (bug for mask = 1- mask, use this as alternative choice)\n        # mask = 1 + (-1)*mask\n        mask = (1 - mask.long()).byte()\n        _, inivalues = next(seq_iter)  # bat_size * from_target_size * to_target_size\n        # only need start from start_tag\n        partition = inivalues[:, START_TAG, :].clone().view(batch_size, tag_size)  # bat_size * to_target_size\n        # print \"init part:\",partition.size()\n        partition_history.append(partition)\n        # iter over last scores\n        for idx, cur_values in seq_iter:\n            # previous to_target is current from_target\n            # partition: previous results log(exp(from_target)), #(batch_size * from_target)\n            # cur_values: batch_size * from_target * to_target\n            cur_values = cur_values + partition.contiguous().view(batch_size, tag_size, 1).expand(batch_size, tag_size,\n                                                                                                  tag_size)\n            ## forscores, cur_bp = torch.max(cur_values[:,:-2,:], 1) # do not consider START_TAG/STOP_TAG\n            # print \"cur value:\", cur_values.size()\n            partition, cur_bp = torch.max(cur_values, 1)\n            # print \"partsize:\",partition.size()\n            # exit(0)\n            # print partition\n            # print cur_bp\n            # print \"one best, \",idx\n            partition_history.append(partition)\n            ## cur_bp: (batch_size, tag_size) max source score position in current tag\n            ## set padded label as 0, which will be filtered in post processing\n            cur_bp.masked_fill_(mask[idx].view(batch_size, 1).expand(batch_size, tag_size), 0)\n            back_points.append(cur_bp)\n        # exit(0)\n        ### add score to final STOP_TAG\n        partition_history = torch.cat(partition_history, 0).view(seq_len, batch_size, -1).transpose(1,\n                                                                                                    0).contiguous()  ## (batch_size, seq_len. tag_size)\n        ### get the last position for each setences, and select the last partitions using gather()\n        last_position = length_mask.view(batch_size, 1, 1).expand(batch_size, 1, tag_size) - 1\n        last_partition = torch.gather(partition_history, 1, last_position).view(batch_size, tag_size, 1)\n        ### calculate the score from last partition to end state (and then select the STOP_TAG from it)\n        last_values = last_partition.expand(batch_size, tag_size, tag_size) + self.transitions.view(1, tag_size,\n                                                                                                    tag_size).expand(\n            batch_size, tag_size, tag_size)\n        _, last_bp = torch.max(last_values, 1)\n        pad_zero = autograd.Variable(torch.zeros(batch_size, tag_size)).long()\n        if self.gpu:\n            pad_zero = pad_zero.cuda()\n        back_points.append(pad_zero)\n        back_points = torch.cat(back_points).view(seq_len, batch_size, tag_size)\n\n        ## select end ids in STOP_TAG\n        pointer = last_bp[:, STOP_TAG]\n        insert_last = pointer.contiguous().view(batch_size, 1, 1).expand(batch_size, 1, tag_size)\n        back_points = back_points.transpose(1, 0).contiguous()\n        ## move the end ids(expand to tag_size) to the corresponding position of back_points to replace the 0 values\n        # print \"lp:\",last_position\n        # print \"il:\",insert_last\n        back_points.scatter_(1, last_position, insert_last)\n        # print \"bp:\",back_points\n        # exit(0)\n        back_points = back_points.transpose(1, 0).contiguous()\n        ## decode from the end, padded position ids are 0, which will be filtered if following evaluation\n        decode_idx = autograd.Variable(torch.LongTensor(seq_len, batch_size))\n        if self.gpu:\n            decode_idx = decode_idx.cuda()\n        decode_idx[-1] = pointer.detach()\n        for idx in range(len(back_points) - 2, -1, -1):\n            pointer = torch.gather(back_points[idx], 1, pointer.contiguous().view(batch_size, 1))\n            decode_idx[idx] = pointer.detach().view(batch_size)\n        path_score = None\n        decode_idx = decode_idx.transpose(1, 0)\n        return path_score, decode_idx\n\n    def _score_sentence(self, scores, mask, tags):\n        \"\"\"\n            input:\n                scores: variable (seq_len, batch, tag_size, tag_size)\n                mask: (batch, seq_len)\n                tags: tensor  (batch, seq_len)\n            output:\n                score: sum of score for gold sequences within whole batch\n        \"\"\"\n        # Gives the score of a provided tag sequence\n        batch_size = scores.size(1)\n        seq_len = scores.size(0)\n        tag_size = scores.size(2)\n        ## convert tag value into a new format, recorded label bigram information to index\n        new_tags = autograd.Variable(torch.LongTensor(batch_size, seq_len))\n        if self.gpu:\n            new_tags = new_tags.cuda()\n        for idx in range(seq_len):\n            if idx == 0:\n                ## start -> first score\n                new_tags[:, 0] = (tag_size - 2) * tag_size + tags[:, 0]\n\n            else:\n                new_tags[:, idx] = tags[:, idx - 1] * tag_size + tags[:, idx]\n\n        ## transition for label to STOP_TAG\n        end_transition = self.transitions[:, STOP_TAG].contiguous().view(1, tag_size).expand(batch_size, tag_size)\n        ## length for batch,  last word position = length - 1\n        length_mask = torch.sum(mask.long(), dim=1).view(batch_size, 1).long()\n        ## index the label id of last word\n        end_ids = torch.gather(tags, 1, length_mask - 1)\n\n        ## index the transition score for end_id to STOP_TAG\n        end_energy = torch.gather(end_transition, 1, end_ids)\n\n        ## convert tag as (seq_len, batch_size, 1)\n        new_tags = new_tags.transpose(1, 0).contiguous().view(seq_len, batch_size, 1)\n        ### need convert tags id to search from 400 positions of scores\n        tg_energy = torch.gather(scores.view(seq_len, batch_size, -1), 2, new_tags).view(seq_len,\n                                                                                         batch_size)  # seq_len * bat_size\n        ## mask transpose to (seq_len, batch_size)\n        tg_energy = tg_energy.masked_select(mask.transpose(1, 0))\n\n        # ## calculate the score from START_TAG to first label\n        # start_transition = self.transitions[START_TAG,:].view(1, tag_size).expand(batch_size, tag_size)\n        # start_energy = torch.gather(start_transition, 1, tags[0,:])\n\n        ## add all score together\n        # gold_score = start_energy.sum() + tg_energy.sum() + end_energy.sum()\n        gold_score = tg_energy.sum() + end_energy.sum()\n        return gold_score\n\n    def neg_log_likelihood_loss(self, feats, mask, tags):\n        # nonegative log likelihood\n        batch_size = feats.size(0)\n        forward_score, scores = self._calculate_PZ(feats, mask)\n        gold_score = self._score_sentence(scores, mask, tags)\n        # print \"batch, f:\", forward_score.data[0], \" g:\", gold_score.data[0], \" dis:\", forward_score.data[0] - gold_score.data[0]\n        # exit(0)\n        return forward_score - gold_score\n","metadata":{"execution":{"iopub.status.busy":"2021-05-22T14:32:32.985066Z","iopub.execute_input":"2021-05-22T14:32:32.985758Z","iopub.status.idle":"2021-05-22T14:32:33.055753Z","shell.execute_reply.started":"2021-05-22T14:32:32.985716Z","shell.execute_reply":"2021-05-22T14:32:33.054763Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nimport numpy as np\n\nclass NamedEntityRecog(nn.Module):\n    def __init__(self, vocab_size, word_embed_dim, word_hidden_dim, alphabet_size, char_embedding_dim, char_hidden_dim,\n                 feature_extractor, tag_num, dropout, pretrain_embed=None, use_char=False, use_crf=False, use_gpu=False):\n        super(NamedEntityRecog, self).__init__()\n        self.use_crf = use_crf\n        self.use_char = use_char\n        self.drop = nn.Dropout(dropout)\n        self.input_dim = word_embed_dim\n        self.feature_extractor = feature_extractor\n\n        self.embeds = nn.Embedding(vocab_size, word_embed_dim, padding_idx=0)\n        if pretrain_embed is not None:\n            self.embeds.weight.data.copy_(torch.from_numpy(pretrain_embed))\n        else:\n            self.embeds.weight.data.copy_(torch.from_numpy(self.random_embedding(vocab_size, word_embed_dim)))\n\n        if self.use_char:\n            self.input_dim += char_hidden_dim\n            self.char_feature = CharCNN(alphabet_size, char_embedding_dim, char_hidden_dim, dropout)\n\n        if feature_extractor == 'lstm':\n            self.lstm = nn.LSTM(self.input_dim, word_hidden_dim, batch_first=True, bidirectional=True)\n        else:\n            self.word2cnn = nn.Linear(self.input_dim, word_hidden_dim*2)\n            self.cnn_list = list()\n            for _ in range(4):\n                self.cnn_list.append(nn.Conv1d(word_hidden_dim*2, word_hidden_dim*2, kernel_size=3, padding=1))\n                self.cnn_list.append(nn.ReLU())\n                self.cnn_list.append(nn.Dropout(dropout))\n                self.cnn_list.append(nn.BatchNorm1d(word_hidden_dim*2))\n            self.cnn = nn.Sequential(*self.cnn_list)\n        \n        if self.use_crf:\n            self.hidden2tag = nn.Linear(word_hidden_dim * 2, tag_num + 2)\n            self.crf = CRF(tag_num, use_gpu)\n        else:\n            self.hidden2tag = nn.Linear(word_hidden_dim * 2, tag_num)\n\n    def random_embedding(self, vocab_size, embedding_dim):\n        pretrain_emb = np.empty([vocab_size, embedding_dim])\n        scale = np.sqrt(3.0 / embedding_dim)\n        for index in range(1, vocab_size):\n            pretrain_emb[index, :] = np.random.uniform(-scale, scale, [1, embedding_dim])\n        return pretrain_emb\n\n    def neg_log_likelihood_loss(self, word_inputs, word_seq_lengths, char_inputs, batch_label, mask):\n        batch_size = word_inputs.size(0)\n        seq_len = word_inputs.size(1)\n        word_embeding = self.embeds(word_inputs)\n        word_list = [word_embeding]\n        if self.use_char:\n            char_features = self.char_feature(char_inputs).contiguous().view(batch_size, seq_len, -1)\n            word_list.append(char_features)\n        word_embeding = torch.cat(word_list, 2)\n        word_represents = self.drop(word_embeding)\n        if self.feature_extractor == 'lstm':\n            packed_words = pack_padded_sequence(word_represents, word_seq_lengths, True)\n            hidden = None\n            lstm_out, hidden = self.lstm(packed_words, hidden)\n            lstm_out, _ = pad_packed_sequence(lstm_out)\n            lstm_out = lstm_out.transpose(0, 1)\n            feature_out = self.drop(lstm_out)\n        else:\n            batch_size = word_inputs.size(0)\n            word_in = torch.tanh(self.word2cnn(word_represents)).transpose(2, 1).contiguous()\n            feature_out = self.cnn(word_in).transpose(1, 2).contiguous()\n\n        feature_out = self.hidden2tag(feature_out)\n\n        if self.use_crf:\n            total_loss = self.crf.neg_log_likelihood_loss(feature_out, mask, batch_label)\n        else:\n            loss_function = nn.CrossEntropyLoss(ignore_index=0, reduction='sum')\n            feature_out = feature_out.contiguous().view(batch_size * seq_len, -1)\n            total_loss = loss_function(feature_out, batch_label.contiguous().view(batch_size * seq_len))\n\n        del feature_out, lstm_out\n        return total_loss\n\n    def forward(self, word_inputs, word_seq_lengths, char_inputs, batch_label, mask):\n        batch_size = word_inputs.size(0)\n        seq_len = word_inputs.size(1)\n        word_embeding = self.embeds(word_inputs)\n        word_list = [word_embeding]\n        if self.use_char:\n            char_features = self.char_feature(char_inputs).contiguous().view(batch_size, seq_len, -1)\n            word_list.append(char_features)\n        word_embeding = torch.cat(word_list, 2)\n        word_represents = self.drop(word_embeding)\n        if self.feature_extractor == 'lstm':\n            packed_words = pack_padded_sequence(word_represents, word_seq_lengths, True)\n            hidden = None\n            lstm_out, hidden = self.lstm(packed_words, hidden)\n            lstm_out, _ = pad_packed_sequence(lstm_out)\n            lstm_out = lstm_out.transpose(0, 1)\n            feature_out = self.drop(lstm_out)\n        else:\n            batch_size = word_inputs.size(0)\n            word_in = torch.tanh(self.word2cnn(word_represents)).transpose(2, 1).contiguous()\n            feature_out = self.cnn(word_in).transpose(1, 2).contiguous()\n\n        feature_out = self.hidden2tag(feature_out)\n\n        if self.use_crf:\n            scores, tag_seq = self.crf._viterbi_decode(feature_out, mask)\n        else:\n            feature_out = feature_out.contiguous().view(batch_size * seq_len, -1)\n            _, tag_seq = torch.max(feature_out, 1)\n            tag_seq = tag_seq.view(batch_size, seq_len)\n            tag_seq = mask.long() * tag_seq\n        return tag_seq\n","metadata":{"execution":{"iopub.status.busy":"2021-05-22T14:32:33.663241Z","iopub.execute_input":"2021-05-22T14:32:33.663570Z","iopub.status.idle":"2021-05-22T14:32:33.688558Z","shell.execute_reply.started":"2021-05-22T14:32:33.663541Z","shell.execute_reply":"2021-05-22T14:32:33.687569Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"model_path = '../input/coleridgelstmmodel/lstmFalseTrue'\nuse_gpu = torch.cuda.is_available()\nprint(f'Use GPU: {use_gpu}')\n\nword_vocab = WordVocabulary(True)\nlabel_vocab = LabelVocabulary()\nalphabet = Alphabet()\n\nword_embed_dim = 100\nword_hidden_dim = 100\nchar_embedding_dim = 50\nchar_hidden_dim = 50\ndropout = 0.5\nuse_char = False\nuse_crf = True\npretrain_word_embedding = None\nfeature_extractor = 'lstm'\nuse_gpu = True\n\nprint('Creating NER model...')\nmodel = NamedEntityRecog(word_vocab.size(), word_embed_dim, word_hidden_dim, alphabet.size(),\n                         char_embedding_dim, char_hidden_dim,\n                         feature_extractor, label_vocab.size(), dropout,\n                         pretrain_embed=pretrain_word_embedding, use_char = use_char, use_crf = use_crf,\n                         use_gpu=use_gpu)\n\nif use_gpu:\n    model = model.cuda()\n    \nprint('Loading model...')\nmodel.load_state_dict(torch.load(model_path))","metadata":{"execution":{"iopub.status.busy":"2021-05-22T14:32:36.700797Z","iopub.execute_input":"2021-05-22T14:32:36.701134Z","iopub.status.idle":"2021-05-22T14:32:45.346872Z","shell.execute_reply.started":"2021-05-22T14:32:36.701102Z","shell.execute_reply":"2021-05-22T14:32:45.346019Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Use GPU: True\nCreating NER model...\nbuild CRF...\nLoading model...\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"import re\nimport json\n\ndef load_test_example_by_name(name):\n    doc_path = os.path.join(test_data_dir, name + '.json')\n    with open(doc_path) as f:\n        data = json.load(f)\n    return data\n\ndef preprocess_tokenize_doc(doc_json):\n    doc_text = ' '.join([remove_punc(sec['text']) for sec in doc_json])\n    doc_text = make_single_whitespace(doc_text)\n    \n    doc_tokens = doc_text.split(' ')\n    return doc_tokens\n\n_RE_COMBINE_WHITESPACE = re.compile(r\"\\s+\")\ndef make_single_whitespace(text):\n    return _RE_COMBINE_WHITESPACE.sub(\" \", text).strip()\n\ndef remove_punc(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt))\n\n","metadata":{"execution":{"iopub.status.busy":"2021-05-22T14:46:57.898419Z","iopub.execute_input":"2021-05-22T14:46:57.898725Z","iopub.status.idle":"2021-05-22T14:46:57.911665Z","shell.execute_reply.started":"2021-05-22T14:46:57.898698Z","shell.execute_reply":"2021-05-22T14:46:57.910868Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def tokens_to_input(tokens):\n    text = tokens\n    \n    text_idx = []\n    seq_char_list = list()\n\n    for word in text:\n        text_idx.append(word_vocab.word_to_id(word))\n    text_tensor = torch.tensor(text_idx).long()\n\n    for word in text:\n        char_list = list(word)\n        char_id = list()\n        for char in char_list:\n            char_id.append(alphabet.char_to_id(char))\n        seq_char_list.append(char_id)\n    \n    return {'text': text_tensor, 'char': seq_char_list}\n\ndef predict(batch):\n    model.eval()\n    prediction = []\n    \n    batch_text, seq_length, word_perm_idx = batch['text']\n    char_inputs = batch['char']\n    char_inputs = char_inputs[word_perm_idx]\n    \n    char_dim = char_inputs.size(-1)\n    char_inputs = char_inputs.contiguous().view(-1, char_dim)\n    \n    if use_gpu:\n        batch_text = batch_text.cuda()\n        char_inputs = char_inputs.cuda()\n        \n    mask = get_mask(batch_text)\n    with torch.no_grad():\n        tag_seq = model(batch_text, seq_length, char_inputs, None, mask)\n    \n    for line_tensor, predicts_tensor in zip(batch_text, tag_seq):\n        for word_tensor, predict_tensor in zip(line_tensor, predicts_tensor):\n            if word_tensor.item() == 0:\n                break\n            line = (word_vocab.id_to_word(word_tensor.item()), label_vocab.id_to_label(predict_tensor.item()))\n            prediction.append(line)\n\n    return prediction","metadata":{"execution":{"iopub.status.busy":"2021-05-22T15:09:57.639941Z","iopub.execute_input":"2021-05-22T15:09:57.640320Z","iopub.status.idle":"2021-05-22T15:09:57.653090Z","shell.execute_reply.started":"2021-05-22T15:09:57.640284Z","shell.execute_reply":"2021-05-22T15:09:57.651955Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"# Padding\n\ndef my_collate(key, batch_tensor):\n    if key == 'char':\n        batch_tensor = pad_char(batch_tensor)\n        return batch_tensor\n    else:\n        word_seq_lengths = torch.LongTensor(list(map(len, batch_tensor)))\n        _, word_perm_idx = word_seq_lengths.sort(0, descending=True)\n        batch_tensor.sort(key=lambda x: len(x), reverse=True)\n        tensor_length = [len(sq) for sq in batch_tensor]\n        batch_tensor = pad_sequence(batch_tensor, batch_first=True, padding_value=0)\n        return batch_tensor, tensor_length, word_perm_idx\n\n\ndef my_collate_fn(batch):\n    return {key: my_collate(key, [d[key] for d in batch]) for key in batch[0]}\n\n\ndef pad_char(chars):\n    batch_size = len(chars)\n    max_seq_len = max(map(len, chars))\n    pad_chars = [chars[idx] + [[0]] * (max_seq_len - len(chars[idx])) for idx in range(len(chars))]\n    length_list = [list(map(len, pad_char)) for pad_char in pad_chars]\n    max_word_len = max(map(max, length_list))\n    char_seq_tensor = torch.zeros((batch_size, max_seq_len, max_word_len)).long()\n    char_seq_lengths = torch.LongTensor(length_list)\n    for idx, (seq, seqlen) in enumerate(zip(pad_chars, char_seq_lengths)):\n        for idy, (word, wordlen) in enumerate(zip(seq, seqlen)):\n            char_seq_tensor[idx, idy, :wordlen] = torch.LongTensor(word)\n\n    return char_seq_tensor","metadata":{"execution":{"iopub.status.busy":"2021-05-22T15:05:41.980165Z","iopub.execute_input":"2021-05-22T15:05:41.980534Z","iopub.status.idle":"2021-05-22T15:05:41.990817Z","shell.execute_reply.started":"2021-05-22T15:05:41.980505Z","shell.execute_reply":"2021-05-22T15:05:41.989580Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"## Create Submission","metadata":{}},{"cell_type":"code","source":"test_preds = []\nids = []\nfor index, row in sample_sub.iterrows():\n    test_id = row['Id']\n    ids.append(test_id)\n    \n    try:\n        # Get test document and preprocess\n        doc = load_test_example_by_name(test_id)\n        doc_tokens = preprocess_tokenize_doc(doc)\n\n        # LSTM expects lowercase tokens (doc_tokens_lower)\n        doc_tokens_lower = [t.lower() for t in doc_tokens]\n\n        proc_tokens = tokens_to_input(doc_tokens_lower)\n        proc_tokens = my_collate_fn([proc_tokens])\n        preds = predict(proc_tokens)\n\n        # Join prediction tokens\n        preds_string = \"\"\n\n        pred_prev = -1\n        token_i = 0\n\n        for token, pred in preds:\n            if pred == '1':\n\n                # Prediction for a new dataset begins\n                if (pred_prev != -1) and (token_i - pred_prev > 3):\n                    preds_string += '|'\n                    pred_prev = token_i\n\n                preds_string += token\n\n            token_i += 1\n\n        test_preds.append(preds_string)\n        \n    except Exception as e:\n        print(e)\n        test_preds.append(\"\")","metadata":{"execution":{"iopub.status.busy":"2021-05-22T15:23:18.695558Z","iopub.execute_input":"2021-05-22T15:23:18.695886Z","iopub.status.idle":"2021-05-22T15:23:30.383102Z","shell.execute_reply.started":"2021-05-22T15:23:18.695856Z","shell.execute_reply":"2021-05-22T15:23:30.382249Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stdout","text":"0\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:159: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729047590/work/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n","output_type":"stream"},{"name":"stdout","text":"1\n2\n3\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}