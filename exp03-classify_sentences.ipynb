{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd01b4c7016e99d31c2e7c892573dc93dbd4548eb0a0f5dca22fbf3a690830b4e66",
   "display_name": "Python 3.8.8 64-bit ('torch': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "e68e0ea63623fb9ac5793b248651fce59782b1ca0d072c96c440758c36acff31"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\ozano\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import random\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "#from fuzzywuzzy import fuzz\n",
    "\n",
    "train_example_paths = glob.glob('data/train/*.json')\n",
    "test_example_paths = glob.glob('data/test/*.json')\n",
    "\n",
    "train_example_names = [fn.split('.')[0] for fn in os.listdir('data/train')]\n",
    "test_example_names = [fn.split('.')[0] for fn in os.listdir('data/test')]\n",
    "\n",
    "metadata = pd.read_csv('data/train.csv')\n",
    "docIdx = train_example_names.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_example_by_name(name):\n",
    "    doc_path = os.path.join('data/train', name + '.json')\n",
    "    with open(doc_path) as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "source": [
    "## Create dataframe for tokens and targets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic text cleaning\n",
    "\n",
    "# Note that I removed lower from the original code here\n",
    "def text_cleaning(text):\n",
    "    text = re.sub(\"\\s+\",\" \", text) # remove extra spaces\n",
    "    text = ''.join([k for k in text if k not in string.punctuation]) # remove punctuation\n",
    "    text = re.sub('[^A-Za-z0-9]+', ' ', str(text)).strip() # remove unnecessary literals and Perform Case Normalization\n",
    "    return text\n",
    "\n",
    "def text_preprocess(text):\n",
    "    #text = text.replace('(', 'specialparstart').replace(')', 'specialparend')\n",
    "    text = re.sub(r'\\[[0-9]+]', 'specialreference', text)\n",
    "    # Remove dates\n",
    "    text = re.sub(r'(19|20)[0-9][0-9]', 'specialyear', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "##### STEP 1: Make a list of the known labels provided to us\n",
    "\n",
    "temp_1 = [text_cleaning(x) for x in metadata['dataset_label']]\n",
    "temp_2 = [text_cleaning(x) for x in metadata['dataset_title']]\n",
    "temp_3 = [text_cleaning(x) for x in metadata['cleaned_label']]\n",
    "\n",
    "existing_labels = temp_1 + temp_2 + temp_3\n",
    "existing_labels = [l.lower() for l in existing_labels]\n",
    "existing_labels = list(set(existing_labels))\n",
    "\n",
    "# Sort labels by length in descending order\n",
    "existing_labels = sorted(existing_labels, key = len, reverse= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sentences = []\n",
    "neg_sentences = []\n",
    "\n",
    "def process_doc(doc_id):\n",
    "    global count_BIO\n",
    "    global count_O\n",
    "\n",
    "    doc_json = load_train_example_by_name(doc_id)\n",
    "    doc_text = ' '.join([sec['text'] for sec in doc_json])\n",
    "\n",
    "    # Tokenize sentencewise\n",
    "    sentences = sent_tokenize(doc_text)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        clean_sentence = text_preprocess(sentence)\n",
    "        clean_sentence = text_cleaning(clean_sentence).lower()\n",
    "\n",
    "        has_label = False\n",
    "        for clean_label in existing_labels:\n",
    "            if clean_label in clean_sentence:\n",
    "                has_label = True\n",
    "\n",
    "                # Remove label from the text, or model will overfit\n",
    "                clean_sentence = clean_sentence.replace(clean_label, '')\n",
    "\n",
    "        if has_label:\n",
    "            pos_sentences.append(clean_sentence)\n",
    "        else:\n",
    "            neg_sentences.append(clean_sentence)\n",
    "\n",
    "#get_doc(docIdx[0])[0]"
   ]
  },
  {
   "source": [
    "## Create Dataset for All Documents"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 14316/14316 [05:01<00:00, 47.44it/s]pos size: 55891\n",
      "neg size: 4123030\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for doc_id in tqdm(docIdx):\n",
    "    process_doc(doc_id)\n",
    "\n",
    "print(f'pos size: {len(pos_sentences)}')\n",
    "print(f'neg size: {len(neg_sentences)}')"
   ]
  },
  {
   "source": [
    "## Save Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f'data/sentence_classification_data/pos.pkl', 'wb') as f:\n",
    "    pickle.dump(pos_sentences, f)\n",
    "\n",
    "with open(f'data/sentence_classification_data/neg.pkl', 'wb') as f:\n",
    "    pickle.dump(neg_sentences, f)\n",
    "\n",
    "print(f'pos size: {len(pos_sentences)}')\n",
    "print(f'neg size: {len(neg_sentences)}')"
   ]
  },
  {
   "source": [
    "## Load Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f'data/sentence_classification_data/pos.pkl', 'rb') as f:\n",
    "    pos_sentences = pickle.load(f)\n",
    "\n",
    "with open(f'data/sentence_classification_data/neg.pkl', 'rb') as f:\n",
    "    neg_sentences = pickle.load(f)\n",
    "\n",
    "print(f'pos size: {len(pos_sentences)}')\n",
    "print(f'neg size: {len(neg_sentences)}')"
   ]
  },
  {
   "source": [
    "## Create Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Creating TF-IDF vectorizer...\n",
      "Creating data arrays...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "print('Creating TF-IDF vectorizer...')\n",
    "all_sentences = pos_sentences + neg_sentences\n",
    "vectorizer = TfidfVectorizer().fit(all_sentences)\n",
    "\n",
    "print('Creating data arrays...')\n",
    "X = vectorizer.transform(all_sentences)\n",
    "y = np.zeros(len(all_sentences))\n",
    "y[len(pos_sentences):] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Splitting data...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print('Splitting data...')\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "data = {\n",
    "    'train_data': (X_train, y_train),\n",
    "    'val_data': (X_val, y_val)\n",
    "}\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Train Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlmodels import sklearn_model\n",
    "\n",
    "from mlmodels.search.hparameters import lr_params\n",
    "\n",
    "delete_file('trials_sklearn')\n",
    "\n",
    "model = sklearn_model.SklearnModel1(minimize_metric = False)\n",
    "\n",
    "res = model.search(data, crf_params.search_space, crf_params.search_fixed, num_iter = 25)\n",
    "best_hparams = res['best_params']\n",
    "best_hparams.update(crf_params.search_fixed)"
   ]
  }
 ]
}