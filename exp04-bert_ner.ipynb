{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd01b4c7016e99d31c2e7c892573dc93dbd4548eb0a0f5dca22fbf3a690830b4e66",
   "display_name": "Python 3.8.8 64-bit ('torch': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "aac00fb1da9c94ab374af15aaaf3bdbaafa792d2239349bc70bec9f747decd69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "https://www.analyticsvidhya.com/blog/2020/07/transfer-learning-for-nlp-fine-tuning-bert-for-text-classification/\n",
    "### Model from:\n",
    "\n",
    "https://github.com/allenai/scibert"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\ozano\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import random\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "import pickle\n",
    "\n",
    "train_example_names = [fn.split('.')[0] for fn in os.listdir('data/train')]\n",
    "test_example_names = [fn.split('.')[0] for fn in os.listdir('data/test')]\n",
    "\n",
    "metadata = pd.read_csv('data/train.csv')\n",
    "docIdx = train_example_names.copy()"
   ]
  },
  {
   "source": [
    "## Create dataframe for tokens and targets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "\n",
    "match_puncs_re = r\"([.,!?()\\-;\\[\\]+\\\\\\/@:<>#_{}&%'*=\" + r'\"' + r\"|])\"\n",
    "match_puncs_re = re.compile(match_puncs_re)\n",
    "\n",
    "def load_train_example_by_name(name):\n",
    "    doc_path = os.path.join('data/train', name + '.json')\n",
    "    with open(doc_path) as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def load_test_example_by_name(name):\n",
    "    doc_path = os.path.join('data/test', name + '.json')\n",
    "    with open(doc_path) as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def text_cleaning_for_bert(text):\n",
    "    # Keeps puncs, pads them with whitespaces\n",
    "\n",
    "    text = text.replace('^', ' ')\n",
    "    text = unidecode.unidecode(text)\n",
    "\n",
    "    # Remove websites\n",
    "    text = ' '.join(['specialwebsite' if 'http' in t or 'www' in t else t for t in text.split(' ') ])\n",
    "\n",
    "    text = match_puncs_re.sub(r' \\1 ', text)\n",
    "\n",
    "    # remove extra spaces\n",
    "    text = re.sub(\"\\s+\",\" \", text)\n",
    "\n",
    "    return text.lower()\n",
    "\n",
    "def text_cleaning_for_label(text):\n",
    "    text = text.replace('^', ' ')\n",
    "    text = unidecode.unidecode(text)\n",
    "\n",
    "    text = re.sub('[^A-Za-z0-9]+', ' ', str(text)).strip() # remove unnecessary literals\n",
    "\n",
    "    # Remove websites\n",
    "    text = ' '.join(['specialwebsite' if 'http' in t or 'www' in t else t for t in text.split(' ') ])\n",
    "\n",
    "    # remove extra spaces\n",
    "    text = re.sub(\"\\s+\",\" \", text)\n",
    "\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def text_cleaning(text):\n",
    "    text = re.sub('[^A-Za-z0-9]+', ' ', str(text)).strip() # remove unnecessary literals\n",
    "\n",
    "    # remove extra spaces\n",
    "    text = re.sub(\"\\s+\",\" \", text)\n",
    "\n",
    "    return text.lower()\n",
    "\n",
    "##### STEP 1: Make a list of the known labels provided to us\n",
    "\n",
    "temp_1 = [text_cleaning(x) for x in metadata['dataset_label']]\n",
    "temp_2 = [text_cleaning(x) for x in metadata['dataset_title']]\n",
    "temp_3 = [text_cleaning(x) for x in metadata['cleaned_label']]\n",
    "\n",
    "existing_labels = temp_1 + temp_2 + temp_3\n",
    "existing_labels = [l.lower() for l in existing_labels]\n",
    "existing_labels = list(set(existing_labels))\n",
    "existing_labels = existing_labels + ['programme for international student assessment', 'kindergarten cohort ecls', 'organization for economic cooperation and development', 'blsa']\n",
    "# Sort labels by length in descending order\n",
    "existing_labels = sorted(existing_labels, key = len, reverse= True)"
   ]
  },
  {
   "source": [
    "## Make sentences"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sentences = []\n",
    "neg_sentences = []\n",
    "\n",
    "def process_doc(doc_id):\n",
    "    doc_json = load_train_example_by_name(doc_id)\n",
    "    doc_text = ' '.join([sec['text'] for sec in doc_json])\n",
    "\n",
    "    # Tokenize sentencewise\n",
    "    sentences = sent_tokenize(doc_text)\n",
    "\n",
    "    adni_count = 0\n",
    "    for sentence in sentences:\n",
    "        clean_sentence = text_cleaning(sentence)\n",
    "\n",
    "        has_label = False\n",
    "        label_is_adni = False\n",
    "        for clean_label in existing_labels:\n",
    "            if clean_label in clean_sentence:\n",
    "                has_label = True\n",
    "\n",
    "                if 'adni' in clean_label or 'alzheimer' in clean_label:\n",
    "                    adni_count += 1\n",
    "                    label_is_adni = True\n",
    "\n",
    "                break\n",
    "\n",
    "        if has_label and (adni_count <= 2 or not label_is_adni):\n",
    "            pos_sentences.append(sentence)\n",
    "        else:\n",
    "            if random.uniform(0, 1) < 0.25:\n",
    "                neg_sentences.append(sentence)"
   ]
  },
  {
   "source": [
    "## Generate and Save Sentences"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 14316/14316 [03:52<00:00, 61.47it/s]\n",
      "pos size: 32509\n",
      "neg size: 1036186\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "pos_sentences = []\n",
    "neg_sentences = []\n",
    "\n",
    "for doc_id in tqdm(docIdx):\n",
    "    process_doc(doc_id)\n",
    "\n",
    "with open(f'data/bert_ner_sentences/pos.pkl', 'wb') as f:\n",
    "    pickle.dump(pos_sentences, f)\n",
    "\n",
    "with open(f'data/bert_ner_sentences/neg.pkl', 'wb') as f:\n",
    "    pickle.dump(neg_sentences, f)\n",
    "\n",
    "print(f'pos size: {len(pos_sentences)}')\n",
    "print(f'neg size: {len(neg_sentences)}')"
   ]
  },
  {
   "source": [
    "## Load Sentences"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pos size: 32509\nneg size: 1036186\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f'data/bert_ner_sentences/pos.pkl', 'rb') as f:\n",
    "    pos_sentences = pickle.load(f)\n",
    "\n",
    "with open(f'data/bert_ner_sentences/neg.pkl', 'rb') as f:\n",
    "    neg_sentences = pickle.load(f)\n",
    "\n",
    "print(f'pos size: {len(pos_sentences)}')\n",
    "print(f'neg size: {len(neg_sentences)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'pos_sentences' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-6314d4ab6894>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mneg_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msmall_sent_targets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m \u001b[0mprocess_pos_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_sentences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2472\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pos_sentences' is not defined"
     ]
    }
   ],
   "source": [
    "pos_sentences_processed = []\n",
    "neg_sentences_processed = []\n",
    "pos_labels = []\n",
    "neg_labels = []\n",
    "\n",
    "n_broken_sent = 0\n",
    "n_pos_no_label = 0\n",
    "\n",
    "def is_text_broken(tokens):\n",
    "    # Some texts are like 'p a dsdv a d a ds f b', remove them\n",
    "    if len(tokens) == 0:\n",
    "        return True\n",
    "\n",
    "    if len(tokens) < 50:\n",
    "        return False\n",
    "\n",
    "    one_char_token_ratio = len([l for l in tokens if len(l) == 1]) / len(tokens)\n",
    "    return one_char_token_ratio > 0.15\n",
    "\n",
    "def split_to_smaller_sent(tokens, s_size, overlap_size):\n",
    "    # output sentences will be s_size + overlap_size long\n",
    "    small_sents = []\n",
    "\n",
    "    if len(tokens) <= s_size:\n",
    "        return [tokens]\n",
    "\n",
    "    n_parts = len(tokens) // s_size\n",
    "    if len(tokens) % s_size != 0:\n",
    "        n_parts += 1\n",
    "\n",
    "    for i_part in range(n_parts):\n",
    "        start_i = i_part * s_size\n",
    "        if i_part > 0:\n",
    "            start_i -= overlap_size\n",
    "\n",
    "        end_i = min(len(tokens), (i_part + 1) * s_size)\n",
    "\n",
    "        small_sents.append(tokens[start_i: end_i])\n",
    "\n",
    "    return small_sents\n",
    "\n",
    "def join_tuple_tokens(tuples):\n",
    "    return ' '.join([t[1] for t in tuples])\n",
    "\n",
    "def get_index(lst, el):\n",
    "    try:\n",
    "        return lst.index(el)\n",
    "    except ValueError as e:\n",
    "        for i, lst_el in enumerate(lst):\n",
    "            if el in lst_el:\n",
    "                return i\n",
    "        \n",
    "    raise ValueError(f'Element {el} not found in {lst}')\n",
    "\n",
    "def process_pos_sentence(sentence):\n",
    "    global n_broken_sent\n",
    "    global last_doc_labels\n",
    "\n",
    "    bert_sentence = text_cleaning_for_bert(sentence)\n",
    "    label_sentence = text_cleaning_for_label(sentence)\n",
    "\n",
    "    if is_text_broken(label_sentence.split(' ')): # Can't use bert cleaning for this, because all punc.s are padded with spaces\n",
    "        n_broken_sent += 1\n",
    "        return\n",
    "    \n",
    "    bert_tokens = bert_sentence.split(' ')\n",
    "    ### STEP 1: Split into fixed sized sentences ###\n",
    "    for small_sentence_tokens in split_to_smaller_sent(bert_tokens, s_size = 125, overlap_size = 25):\n",
    "\n",
    "        small_bert_sentence = ' '.join(small_sentence_tokens)\n",
    "\n",
    "        # Need to remove punc.s and uppercase letters to find labels\n",
    "        small_label_sentence = text_cleaning_for_label(small_bert_sentence)\n",
    "\n",
    "        has_label = False\n",
    "        sent_labels = []\n",
    "        ### STEP 2: Match labels ###\n",
    "        # Check if contains labels\n",
    "        for clean_label in existing_labels:\n",
    "            if clean_label in small_label_sentence:\n",
    "                has_label = True\n",
    "\n",
    "                # Remove label from the text, to only match the largest label\n",
    "                small_label_sentence = small_label_sentence.replace(clean_label, '')\n",
    "                sent_labels.append(clean_label)\n",
    "\n",
    "        small_sent_targets = ['O' for _ in range(len(small_sentence_tokens))]\n",
    "\n",
    "        if has_label:\n",
    "            # Tokenize labels for matching\n",
    "            sent_label_tokens = [l.split(' ') for l in sent_labels]\n",
    "\n",
    "            # Get index, token tuples for clean tokens. Indices are for raw tokens\n",
    "            small_sent_tuples = [(i, token) for i, token in enumerate(small_sentence_tokens) if text_cleaning_for_label(token) != '']\n",
    "\n",
    "            ### STEP 3: Set corresponding targets for each label ###\n",
    "            # Target: (B, I, O), Label: adni\n",
    "            for l in sent_labels:\n",
    "                l_tokens = l.split(' ')\n",
    "                small_sent_joined = [join_tuple_tokens(small_sent_tuples[i: i + len(l_tokens)]) for i in range(len(small_sent_tuples) - len(l_tokens) + 1)]\n",
    "\n",
    "                label_start_i = get_index(small_sent_joined, l)\n",
    "                label_end_i = label_start_i + len(l_tokens) - 1\n",
    "\n",
    "                target_start_i = small_sent_tuples[label_start_i][0]\n",
    "                target_end_i = small_sent_tuples[label_end_i][0]\n",
    "\n",
    "                # Do not use the same tokens for multiple labels\n",
    "                small_sent_tuples = small_sent_tuples[:label_start_i] + small_sent_tuples[label_end_i:]\n",
    "\n",
    "                try:\n",
    "                    small_sent_targets[target_start_i] = 'B'\n",
    "                    if target_end_i - target_start_i > 0:\n",
    "                        for i in range(target_start_i+1, target_end_i+1):\n",
    "                            small_sent_targets[i] = 'I'\n",
    "\n",
    "                except Exception as e:\n",
    "                    print('DEBUG')\n",
    "                    print(small_sentence_tokens)\n",
    "                    print(len(small_sentence_tokens))\n",
    "                    print(len(small_sent_targets))\n",
    "                    print(target_start_i)\n",
    "                    print(small_sent_joined)\n",
    "                    print('DEBUG')\n",
    "                    raise e\n",
    "\n",
    "        ### STEP 4: Add sentence output to lists ###\n",
    "        if has_label:\n",
    "            pos_sentences_processed.append(small_sentence_tokens)\n",
    "            pos_labels.append(small_sent_targets)\n",
    "        \"\"\"else:\n",
    "            neg_sentences_processed.append(small_sentence_tokens)\n",
    "            neg_labels.append(small_sent_targets)\"\"\"\n",
    "\n",
    "def process_neg_sentence(sentence):\n",
    "    global n_broken_sent\n",
    "    \n",
    "    bert_sentence = text_cleaning_for_bert(sentence)\n",
    "    label_sentence = text_cleaning_for_label(sentence)\n",
    "\n",
    "    if is_text_broken(label_sentence.split(' ')): # Can't use bert cleaning for this, because all punc.s are padded with spaces\n",
    "        n_broken_sent += 1\n",
    "        return\n",
    "\n",
    "    bert_tokens = bert_sentence.split(' ')\n",
    "    \n",
    "    ### STEP 1: Split into fixed sized sentences ###\n",
    "    for small_sentence_tokens in split_to_smaller_sent(bert_tokens, s_size = 125, overlap_size = 25):\n",
    "        small_sent_targets = ['O' for _ in range(len(bert_tokens))]\n",
    "\n",
    "        neg_sentences_processed.append(small_sentence_tokens)\n",
    "        neg_labels.append(small_sent_targets)\n",
    "\n",
    "process_pos_sentence(pos_sentences[2472])"
   ]
  },
  {
   "source": [
    "## Create NER Dataset and Save"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 32509/32509 [00:08<00:00, 3897.80it/s]\n",
      "100%|██████████| 1036186/1036186 [00:57<00:00, 18128.76it/s]\n",
      "\n",
      "broken sentences: 8296\n",
      "n_pos_no_label: 0\n",
      "pos_proc size: 32235\n",
      "neg_proc size: 1032513\n"
     ]
    }
   ],
   "source": [
    "pos_sentences_processed = []\n",
    "neg_sentences_processed = []\n",
    "pos_labels = []\n",
    "neg_labels = []\n",
    "\n",
    "n_pos_no_label = 0\n",
    "n_broken_sent = 0\n",
    "\n",
    "for sent in tqdm(pos_sentences):\n",
    "    process_pos_sentence(sent)\n",
    "\n",
    "for sent in tqdm(neg_sentences):\n",
    "    process_neg_sentence(sent)\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(f'data/bert_ner_data/pos.pkl', 'wb') as f:\n",
    "    pickle.dump(pos_sentences_processed, f)\n",
    "\n",
    "with open(f'data/bert_ner_data/neg.pkl', 'wb') as f:\n",
    "    pickle.dump(neg_sentences_processed, f)\n",
    "\n",
    "with open(f'data/bert_ner_data/pos_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(pos_labels, f)\n",
    "\n",
    "with open(f'data/bert_ner_data/neg_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(neg_labels, f)\n",
    "\n",
    "\n",
    "print('')\n",
    "print(f'broken sentences: {n_broken_sent}')\n",
    "print(f'n_pos_no_label: {n_pos_no_label}')\n",
    "print(f'pos_proc size: {len(pos_sentences_processed)}')\n",
    "print(f'neg_proc size: {len(neg_sentences_processed)}')"
   ]
  },
  {
   "source": [
    "## Load NER Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pos size: 32235\nneg size: 1032513\npos label size: 32235\nneg label size: 1032513\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f'data/bert_ner_data/pos.pkl', 'rb') as f:\n",
    "    pos_sentences_processed = pickle.load(f)\n",
    "\n",
    "with open(f'data/bert_ner_data/neg.pkl', 'rb') as f:\n",
    "    neg_sentences_processed = pickle.load(f)\n",
    "\n",
    "with open(f'data/bert_ner_data/pos_labels.pkl', 'rb') as f:\n",
    "    pos_labels = pickle.load(f)\n",
    "\n",
    "with open(f'data/bert_ner_data/neg_labels.pkl', 'rb') as f:\n",
    "    neg_labels = pickle.load(f)\n",
    "\n",
    "print(f'pos size: {len(pos_sentences_processed)}')\n",
    "print(f'neg size: {len(neg_sentences_processed)}')\n",
    "print(f'pos label size: {len(pos_labels)}')\n",
    "print(f'neg label size: {len(neg_labels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "              token label\n",
       "0             based     O\n",
       "1                on     O\n",
       "2               the     O\n",
       "3              2003     O\n",
       "4            united     O\n",
       "5            states     O\n",
       "6        department     O\n",
       "7                of     O\n",
       "8       agriculture     O\n",
       "9             rural     B\n",
       "10                -     I\n",
       "11            urban     I\n",
       "12        continuum     I\n",
       "13            codes     I\n",
       "14                ,     O\n",
       "15               13     O\n",
       "16               10     O\n",
       "17        practices     O\n",
       "18             were     O\n",
       "19          located     O\n",
       "20               in     O\n",
       "21          smaller     O\n",
       "22      communities     O\n",
       "23           within     O\n",
       "24            urban     O\n",
       "25         counties     O\n",
       "26                ,     O\n",
       "27              and     O\n",
       "28              the     O\n",
       "29        remaining     O\n",
       "30               39     O\n",
       "31        practices     O\n",
       "32          located     O\n",
       "33               in     O\n",
       "34  nonmetropolitan     O\n",
       "35         counties     O\n",
       "36                .     O\n",
       "37                      O"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>token</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>based</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>on</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>the</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2003</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>united</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>states</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>department</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>of</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>agriculture</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>rural</td>\n      <td>B</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>-</td>\n      <td>I</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>urban</td>\n      <td>I</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>continuum</td>\n      <td>I</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>codes</td>\n      <td>I</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>,</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>13</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>10</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>practices</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>were</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>located</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>in</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>smaller</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>communities</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>within</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>urban</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>counties</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>,</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>and</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>the</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>remaining</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>39</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>practices</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>located</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>in</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>nonmetropolitan</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>counties</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>.</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td></td>\n      <td>O</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "i_ex = 1000\n",
    "pd.DataFrame({'token': pos_sentences_processed[i_ex], 'label': pos_labels[i_ex]})"
   ]
  },
  {
   "source": [
    "## Create Training Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Splitting data...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "sentences = pos_sentences_processed# + neg_sentences_processed\n",
    "labels = pos_labels# + neg_labels\n",
    "\n",
    "print('Splitting data...')\n",
    "train_sents, val_sents, train_labels, val_labels = train_test_split(sentences, labels, test_size=0.20, random_state=42)"
   ]
  },
  {
   "source": [
    "## Save Datasets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r'data\\ner\\coleridge'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Save train\n",
    "dst_path = os.path.join(data_dir, 'train.txt')\n",
    "lines = []\n",
    "lines = []\n",
    "docsize = 0\n",
    "for tokens, labels in zip(train_sents, train_labels):\n",
    "    for t, l in zip(tokens, labels):\n",
    "        if len(t) == 0:\n",
    "            continue\n",
    "        lines.append(f'{t} NN O {l}')\n",
    "        docsize += 1\n",
    "\n",
    "    lines.append('')\n",
    "\n",
    "lines = lines[:-1]\n",
    "lines = [f'-DOCSTART- ({docsize})', ''] + lines\n",
    "with open(dst_path, 'w') as f:\n",
    "    for line in lines:\n",
    "        f.write(line)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "lines = []\n",
    "docsize = 0\n",
    "for tokens, labels in zip(val_sents, val_labels):\n",
    "    for t, l in zip(tokens, labels):\n",
    "        if len(t) == 0:\n",
    "            continue\n",
    "        lines.append(f'{t} NN O {l}')\n",
    "        docsize += 1\n",
    "\n",
    "    lines.append('')\n",
    "\n",
    "lines = lines[:-1]\n",
    "lines = [f'-DOCSTART- ({docsize})', ''] + lines\n",
    "\n",
    "# Save dev\n",
    "dst_path = os.path.join(data_dir, 'dev.txt')\n",
    "with open(dst_path, 'w') as f:\n",
    "    for line in lines:\n",
    "        f.write(line)\n",
    "        f.write('\\n')\n",
    "\n",
    "test_path = os.path.join(data_dir, 'test.txt')\n",
    "with open(test_path, 'w') as f:\n",
    "    for line in lines:\n",
    "        f.write(line)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## Fine Tune Bert"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    " import os\n",
    " import math\n",
    " import random\n",
    " import csv\n",
    " import sys\n",
    " import numpy as np\n",
    " import pandas as pd\n",
    " from sklearn import metrics\n",
    " from sklearn.metrics import f1_score, precision_score, recall_score\n",
    " from sklearn.metrics import classification_report\n",
    " import statistics as stats\n",
    " from bert_sklearn import BertTokenClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Building sklearn token classifier...\n"
     ]
    }
   ],
   "source": [
    "model = BertTokenClassifier(bert_model='scibert-scivocab-uncased',\n",
    "                             max_seq_length=178, \n",
    "                             epochs=3,\n",
    "                             #gradient accumulation\n",
    "                             gradient_accumulation_steps=4,\n",
    "                             learning_rate=3e-5,\n",
    "                             train_batch_size=8,#batch size for training\n",
    "                             eval_batch_size=8, #batch size for evaluation\n",
    "                             validation_fraction=0., \n",
    "                             #ignore the tokens with label ‘O’                      \n",
    "                             ignore_label=['O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, y_train = flatten(train_sents), flatten(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\ozano\\.conda\\envs\\torch\\lib\\site-packages\\bert_sklearn\\utils.py:26: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(X)\n",
      "Loading scibert-scivocab-uncased model...\n",
      "Defaulting to linear classifier/regressor\n",
      "Loading Pytorch checkpoint\n",
      "train data size: 25788, validation data size: 0\n",
      "Training  :   0%|          | 3/12894 [00:12<9:56:21,  2.78s/it, loss=0.461] C:\\Users\\ozano\\.conda\\envs\\torch\\lib\\site-packages\\bert_sklearn\\model\\pytorch_pretrained\\optimization.py:275: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:1005.)\n",
      "  next_m.mul_(beta1).add_(1 - beta1, grad)\n",
      "Training  : 100%|██████████| 12894/12894 [35:47<00:00,  6.00it/s, loss=0.0086]\n",
      "Training  : 100%|██████████| 12894/12894 [37:20<00:00,  5.76it/s, loss=0.001]\n",
      "Training  : 100%|██████████| 12894/12894 [36:39<00:00,  5.86it/s, loss=0.000288]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "BertTokenClassifier(bert_model='scibert-scivocab-uncased', do_lower_case=True,\n",
       "                    gradient_accumulation_steps=4, ignore_label=['O'],\n",
       "                    label_list=array(['B', 'I', 'O'], dtype='<U1'),\n",
       "                    learning_rate=3e-05, max_seq_length=178, train_batch_size=8,\n",
       "                    validation_fraction=0.0)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "model.fit(train_sents, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to disk\n",
    "savefile='data/sklearn_bert.bin'\n",
    "model.save(savefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\ozano\\.conda\\envs\\torch\\lib\\site-packages\\bert_sklearn\\utils.py:26: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(X)\n",
      "Predicting: 100%|██████████| 806/806 [01:38<00:00,  8.19it/s]\n"
     ]
    }
   ],
   "source": [
    "val_preds = model.predict(val_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   pred        token\n",
       "0     O         this\n",
       "1     O         work\n",
       "2     O    describes\n",
       "3     O          the\n",
       "4     O   validation\n",
       "5     O           of\n",
       "6     O         this\n",
       "7     O    automated\n",
       "8     O  hippocampal\n",
       "9     O   volumetric\n",
       "10    O  measurement\n",
       "11    O      program\n",
       "12    O           on\n",
       "13    O           99\n",
       "14    O     subjects\n",
       "15    O         from\n",
       "16    O          the\n",
       "17    B         adni\n",
       "18    O        study\n",
       "19    O         with\n",
       "20    O     manually\n",
       "21    O    segmented\n",
       "22    O   hippocampi\n",
       "23    O            .\n",
       "24    O             "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pred</th>\n      <th>token</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>O</td>\n      <td>this</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>O</td>\n      <td>work</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>O</td>\n      <td>describes</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>O</td>\n      <td>the</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>O</td>\n      <td>validation</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>O</td>\n      <td>of</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>O</td>\n      <td>this</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>O</td>\n      <td>automated</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>O</td>\n      <td>hippocampal</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>O</td>\n      <td>volumetric</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>O</td>\n      <td>measurement</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>O</td>\n      <td>program</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>O</td>\n      <td>on</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>O</td>\n      <td>99</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>O</td>\n      <td>subjects</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>O</td>\n      <td>from</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>O</td>\n      <td>the</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>B</td>\n      <td>adni</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>O</td>\n      <td>study</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>O</td>\n      <td>with</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>O</td>\n      <td>manually</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>O</td>\n      <td>segmented</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>O</td>\n      <td>hippocampi</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>O</td>\n      <td>.</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>O</td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "ex_i = 0\n",
    "pd.DataFrame({'token': val_sents[ex_i], 'pred':val_preds[ex_i]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}