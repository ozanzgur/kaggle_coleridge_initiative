{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd01b4c7016e99d31c2e7c892573dc93dbd4548eb0a0f5dca22fbf3a690830b4e66",
   "display_name": "Python 3.8.8 64-bit ('torch': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from string_search import *\n",
    "\n",
    "N_GRAM_SIZE = 4\n",
    "\n",
    "data_dir = r'C:\\projects\\personal\\kaggle\\kaggle_coleridge_initiative\\string_search\\data'\n",
    "data_path = os.path.join(data_dir, 'gov_data.csv')\n",
    "\n",
    "def text_cleaning(text):\n",
    "    text = re.sub('[^A-Za-z]+', ' ', str(text)).strip() # remove unnecessary literals\n",
    "\n",
    "    # remove extra spaces\n",
    "    text = re.sub(\"\\s+\",\" \", text)\n",
    "\n",
    "    return text.lower().strip()\n",
    "\n",
    "    import string\n",
    "\n",
    "def is_name_ok(text):\n",
    "    if len([c for c in text if c.isalnum()]) < 4:\n",
    "        return False\n",
    "    if len(text.split(' ')) < 3:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def get_n_grams(s):\n",
    "    s = f' {s} '\n",
    "    n_grams = []\n",
    "    for i in range(len(s) - N_GRAM_SIZE + 1):\n",
    "        ngram_str = s[i: i + N_GRAM_SIZE]\n",
    "        #if ' ' not in ngram_str[1:]:\n",
    "        if not ' ' in ngram_str[1:3]:\n",
    "            n_grams.append(ngram_str)\n",
    "        \n",
    "    \n",
    "    return reduce_n_grams(n_grams)\n",
    "\n",
    "def reduce_n_grams(n_grams):\n",
    "    n_grams_single = []\n",
    "    for token in n_grams:\n",
    "        if token not in n_grams_single:\n",
    "            n_grams_single.append(token)\n",
    "    return n_grams_single\n",
    "\n",
    "##### STEP 1: Make a list of the known labels provided to us\n",
    "\n",
    "metadata = pd.read_csv(r'C:\\projects\\personal\\kaggle\\kaggle_coleridge_initiative\\data\\train.csv')\n",
    "temp_1 = [text_cleaning(x) for x in metadata['dataset_label']]\n",
    "temp_2 = [text_cleaning(x) for x in metadata['dataset_title']]\n",
    "temp_3 = [text_cleaning(x) for x in metadata['cleaned_label']]\n",
    "\n",
    "existing_labels = temp_1 + temp_2 + temp_3\n",
    "existing_labels = [l.lower() for l in existing_labels]\n",
    "existing_labels = list(set(existing_labels))\n",
    "# Sort labels by length in descending order\n",
    "existing_labels = sorted(existing_labels, key = len, reverse= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "292115\n",
      "139120\n",
      "137647\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(data_path)\n",
    "\n",
    "df = pd.concat([df, pd.DataFrame({'title': existing_labels})], ignore_index= True).reset_index(drop = True)\n",
    "#df['raw'] = df.title\n",
    "df['title'] = df.title.apply(text_cleaning)\n",
    "\n",
    "print(len(df))\n",
    "to_remove = ['data set', 'research reports', 'data science', 'council c', 'line p', 'point lay', 'l surf', 'analysis data', 'test results', 'annual report', 'ap index', 'cyber security', 'rural residents', 'financial information', 'invasive species', 'strategic plan', 'administrative data', 'supporting information', 'domestic violence']\n",
    "titles = list(df.title.unique())\n",
    "for t in to_remove:\n",
    "    titles.remove(t)\n",
    "\n",
    "df = pd.DataFrame({'title': titles})\n",
    "\n",
    "print(len(df))\n",
    "df = df.loc[df.title.apply(is_name_ok)]\n",
    "\n",
    "print(len(df))\n",
    "\n",
    "#df = df.iloc[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                               title\n",
       "0  department for the aging dfta geriatric mental...\n",
       "1  low altitude aerial imagery obtained with unma...\n",
       "2                           forestry planting spaces\n",
       "3  nys math test results by grade citywide by rac...\n",
       "4  high operational temperature mwir detectors wi..."
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>department for the aging dfta geriatric mental...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>low altitude aerial imagery obtained with unma...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>forestry planting spaces</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>nys math test results by grade citywide by rac...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>high operational temperature mwir detectors wi...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "source": [
    "## Get N-Grams"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ngram = pd.DataFrame()\n",
    "for col in df.columns:\n",
    "    df_ngram[col] = df[col].apply(lambda x: get_n_grams(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                               title\n",
       "0  [ dep, depa, epar, part, artm, rtme, tmen, men...\n",
       "1  [ low, low ,  alt, alti, ltit, titu, itud, tud...\n",
       "2  [ for, fore, ores, rest, estr, stry, try ,  pl...\n",
       "3  [ nys, nys ,  mat, math, ath ,  tes, test, est...\n",
       "4  [ hig, high, igh ,  ope, oper, pera, erat, rat..."
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[ dep, depa, epar, part, artm, rtme, tmen, men...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[ low, low ,  alt, alti, ltit, titu, itud, tud...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[ for, fore, ores, rest, estr, stry, try ,  pl...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[ nys, nys ,  mat, math, ath ,  tes, test, est...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[ hig, high, igh ,  ope, oper, pera, erat, rat...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "df_ngram.head()"
   ]
  },
  {
   "source": [
    "## Create Index"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_n_gram_length(x):\n",
    "    return max(1, len(x))\n",
    "\n",
    "name_index = create_ngram_index(df_ngram['title'].values)\n",
    "name_lens = df_ngram['title'].apply(get_n_gram_length).values"
   ]
  },
  {
   "source": [
    "## Search"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import random\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "import pickle\n",
    "\n",
    "train_example_names = [fn.split('.')[0] for fn in os.listdir(r'C:\\projects\\personal\\kaggle\\kaggle_coleridge_initiative\\data\\train')]\n",
    "\n",
    "docIdx = train_example_names.copy()\n",
    "\n",
    "def load_train_example_by_name(name):\n",
    "    doc_path = os.path.join(r'C:\\projects\\personal\\kaggle\\kaggle_coleridge_initiative\\data\\train', name + '.json')\n",
    "    with open(doc_path) as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def get_doc(doc_id):\n",
    "    doc_json = load_train_example_by_name(doc_id)\n",
    "    doc_text = ' '.join([sec['text'] for sec in doc_json])\n",
    "\n",
    "    # Tokenize sentencewise\n",
    "    sentences = sent_tokenize(doc_text)\n",
    "    sentences = [text_cleaning(s) for s in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\projects\\personal\\kaggle\\kaggle_coleridge_initiative\\data/bert_ner_data/pos.pkl', 'rb') as f:\n",
    "    pos_sentences_processed = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matches(input_n_grams, n_gram_index, n_gram_lengths):\n",
    "    input_n_gram_set = input_n_grams\n",
    "    input_len = len(input_n_gram_set)\n",
    "    matches = {}\n",
    "\n",
    "    for token in input_n_gram_set:\n",
    "        if token in n_gram_index:\n",
    "            token_matches = n_gram_index[token]\n",
    "\n",
    "            for token_match in token_matches:\n",
    "                if token_match in matches:\n",
    "                    # Increase matching token count of this entry\n",
    "                    matches[token_match] += 1\n",
    "                else:\n",
    "                    # Add new entry\n",
    "                    matches[token_match] = 1\n",
    "\n",
    "    #matches = {k: v / max(input_len, n_gram_lengths[k]) for k,v in matches.items()}\n",
    "    matches = {k: v / input_len for k,v in matches.items()}\n",
    "\n",
    "    return [(k, v) for k, v in matches.items()]\n",
    "\n",
    "\n",
    "def get_matches(input_n_grams, n_gram_index, n_gram_lengths):\n",
    "    input_n_gram_set = input_n_grams\n",
    "    input_len = len(input_n_gram_set)\n",
    "    matches = {}\n",
    "\n",
    "    for token in input_n_gram_set:\n",
    "        if token in n_gram_index:\n",
    "            token_matches = n_gram_index[token]\n",
    "\n",
    "            for token_match in token_matches:\n",
    "                if token_match in matches:\n",
    "                    # Increase matching token count of this entry\n",
    "                    matches[token_match] += 1\n",
    "                else:\n",
    "                    # Add new entry\n",
    "                    matches[token_match] = 1\n",
    "\n",
    "    #matches = {k: v / max(input_len, n_gram_lengths[k]) for k,v in matches.items()}\n",
    "    matches = {k: v / n_gram_lengths[k] for k,v in matches.items()}\n",
    "    #matches = {k: v / input_len for k,v in matches.items()}\n",
    "\n",
    "    return [(k, v) for k, v in matches.items()]\n",
    "\n",
    "def get_match_values(matches, entries):\n",
    "    return [(entries[i], score) for i, score in matches]\n",
    "\n",
    "def get_sentence_matches(s, name_index, name_lens, names):\n",
    "    input_ngrams = get_n_grams(s)\n",
    "    matches = get_matches(input_ngrams, name_index, name_lens)\n",
    "    matches = [(i, score) for i, score in matches if score >= MATCH_THRESHOLD]\n",
    "    matches = get_match_values(matches, names)\n",
    "    matches = [(text, score) for text, score in matches if f' {text.lower()} ' in f' {s} ']\n",
    "    return [text for text, _ in matches]\n",
    "\n",
    "MATCH_THRESHOLD = 0.9\n",
    "def get_doc_matches(sentences):\n",
    "    doc_matches = []\n",
    "    for s in tqdm(sentences):\n",
    "        matches = get_sentence_matches(s, name_index, name_lens, df.title.values)\n",
    "        doc_matches.extend(matches)\n",
    "\n",
    "    return list(set(doc_matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id = docIdx[26]\n",
    "sentences = get_doc(doc_id)\n",
    "matches = [l for l in df.title.values if l in ' '.join(sentences)]#matches = get_doc_matches(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['high schools',\n",
       " 'national education longitudinal study',\n",
       " 'education longitudinal study']"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "[s for s in sentences if 'domestic violence' in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = ['data set', 'research reports', 'data science', 'council c', 'line p', 'point lay', 'l surf', 'analysis data', 'test results', 'annual report', 'ap index', 'cyber security', 'rural residents', 'financial information', 'invasive species', 'strategic plan', 'administrative data', 'supporting information', 'domestic violence', 'high schools']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}